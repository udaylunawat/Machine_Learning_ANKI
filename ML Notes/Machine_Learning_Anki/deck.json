{
    "__type__": "Deck",
    "children": [
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "1355f1a4-3db5-11ea-a55a-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "150px-Visualisation_mode_median_mean.svg.png",
                "latex-1b8a906c01b64fdfa7519e7d42453d0795315a25.png",
                "latex-d66de9da43613c74d5cf21e0e071ea86a709666d.png"
            ],
            "name": "10. EDA",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Features of iris dataset",
                        "Sepal Length<div>Sepal Width</div><div>Petal Length</div><div>Petal Width</div>"
                    ],
                    "flags": 0,
                    "guid": "u?C7>q8L?W",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Features v/s class labels",
                        "Briefly, feature is input; label is output."
                    ],
                    "flags": 0,
                    "guid": "iH^o{df8Fb",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What are imbalanced datasets ?",
                        "Imbalance means that the number of data points available for different  classes are different<div><br></div><div><br>If there are two classes, then&nbsp;<b>balanced data</b>&nbsp;would mean 50% points for each of the class. For most machine learning techniques,&nbsp;<b>little imbalance is not a problem</b>. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "gt|m+TWfQK",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Equation for a line / linear surface",
                        "y = mx + c<br><br>m = gradient (slope) <br>c = y intercept (constant)<br><br>y intercept is the value of y at the point where the line crosses the y axis"
                    ],
                    "flags": 0,
                    "guid": "t>W{4l^#@9",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How can you Linearly seperate points or classes ?",
                        "By drawing a line / plane you can seperate all the classes."
                    ],
                    "flags": 0,
                    "guid": "CH_L08!B7&",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Pair plots ?",
                        "Plot pairwise/featurewise relationships in a dataset."
                    ],
                    "flags": 0,
                    "guid": "bx){NP3t1%",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Limitation of pair plots. And which are the techniques to overcome that limitation ?",
                        "With large no. of features, it's not feasible to understand pairplots.<div><br></div><div>For large no. of dimensions we use techniques like T-SNE and PCA for dimensionlaity reduction.</div>"
                    ],
                    "flags": 0,
                    "guid": "KyJ[[GS=Fr",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Mean, median and mode in PDF",
                        "<img src=\"150px-Visualisation_mode_median_mean.svg.png\"><br>"
                    ],
                    "flags": 0,
                    "guid": "g*fSe?Lu:Y",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "In ML amount of code is less than standard engineering job",
                        "Richness/Quality in analysis is more important that lines of code"
                    ],
                    "flags": 0,
                    "guid": "PmNXdlxuY@",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Iris flower species",
                        "Virginica<div>Versicolor</div><div>Setosa</div>"
                    ],
                    "flags": 0,
                    "guid": "lam*_]5/t8",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is a PDF ? How is it related to histogram ?",
                        "Probability density function.<div>It's a smoothed histogram.</div>"
                    ],
                    "flags": 0,
                    "guid": "zwp`UfZUW#",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How is a PDF smoothed ?",
                        "Using KDE (Kernel density estimator)"
                    ],
                    "flags": 0,
                    "guid": "z`w/=)>Zjg",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why are histograms also called as density plots ?",
                        "Because the height of the plot also represents the density of every region.&nbsp;"
                    ],
                    "flags": 0,
                    "guid": "u&45F}ra*;",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Univariate analysis.Univariate features in iris dataset",
                        "One variable analysis.<div><br></div><div>In case of the Iris dataset example, we used all features like SL, SW, PL, PW for univariate analysis.</div>"
                    ],
                    "flags": 0,
                    "guid": "JOo(B@6r;q",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Define CDF",
                        "Cumulative distribution function<div><br><br>Probability of a random variable upto a particular value<br><br></div><div>diff(CDF) --&gt; PDF</div><div>integration(pdf) --&gt; CDF</div>"
                    ],
                    "flags": 0,
                    "guid": "lDr91n%h.t",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Need of CDF with example from iris dataset",
                        "Using CDF's we can visually see what % of a type of flowers have  petal length of x or less<br><br>"
                    ],
                    "flags": 0,
                    "guid": "h>)(8^e^-*",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Effect of outliers on mean",
                        "The&nbsp;<b>mean</b>&nbsp;is more sensitive to the existence of&nbsp;<b>outliers</b>&nbsp;than the median or mode"
                    ],
                    "flags": 0,
                    "guid": "F$tD!Uu]/M",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Central Tendency, what does it do ?",
                        "A single value that reflects the center of the data distribution.<div><br></div><div>Although it does not provide information regarding the individual values in the dataset, it delivers a comprehensive summary of the whole dataset.&nbsp;</div><div><br></div><div>Measures of central tendency:- Mean, Median, Mode</div>"
                    ],
                    "flags": 0,
                    "guid": "bo$A=z~n^f",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Measures of central tendency",
                        "Mean, Median, Mode"
                    ],
                    "flags": 0,
                    "guid": "xLFMhYw69?",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is Standard deviation  ? Low / High sd",
                        "<div><b>Standard deviation</b>&nbsp;is a number used to&nbsp;<b>tell</b>&nbsp;how measurements for a group are spread out from the average (mean), or expected value. <br><br>A low&nbsp;<b>standard deviation</b>means that most of the numbers are close to the average. A high&nbsp;<b>standard deviation</b>&nbsp;means that the numbers are more spread out.<br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "e#M-pV}V$1",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Mean",
                        "Represents the sum of all values in a dataset divided by the total number of the values."
                    ],
                    "flags": 0,
                    "guid": "c5gm`oli_n",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<strong>Median</strong>",
                        "The middle value in a dataset that is arranged in ascending order (from the smallest value to the largest value). If a dataset contains an even number of values, the median of the dataset is the mean of the two middle values."
                    ],
                    "flags": 0,
                    "guid": "pQ/g(?C!uB",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<strong>Mode ? How many modes can be there in a dataset ?</strong>",
                        "Defines the most frequently occurring value in a dataset. In some cases, a dataset may contain multiple modes while some datasets may not have any mode at all."
                    ],
                    "flags": 0,
                    "guid": "QIP@9D>xMX",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Dispersion and what are it's other names.&nbsp;",
                        "In&nbsp;<b>statistics</b>, dispersion (also called variability, scatter, or&nbsp;<b>spread</b>) is the extent to which a distribution is stretched or squeezed.<br><div><br></div><div>Spread in simple english can be thought of as variance.</div>"
                    ],
                    "flags": 0,
                    "guid": "P7tCN4W4V,",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA",
                        "optional"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Standard deviation, sd - negative, 0 or positive",
                        "TLDR - Spread of the data distribution<br><br>Standard deviation measures the spread of a data distribution.&nbsp;<strong>The more spread out a data distribution is, the greater its standard deviation.</strong><div><strong><br></strong></div><div>Standard deviation cannot be negative. A standard deviation close to&nbsp;0 indicates that the data points tend to be close to the mean. The further the data points are from the mean, the greater the standard deviation.<strong><br></strong></div><div><br></div><div>Spread according to verma sir is another english word to describe standard deviation.</div>"
                    ],
                    "flags": 0,
                    "guid": "Co7}s{q#+*",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Variance and sd relationship in formulae (English interpretation as well)",
                        "<div><span>[$$] \\sqrt{Variance} =&nbsp; \\sqrt{ \\frac{ 1 }{ n } \\sum_{i=1}^{n} (x_{i} - \\mu)^</span><span>{2}</span><span>&nbsp;} = Standard Deviation [/$$]</span></div><div><span><br></span></div><div><span>Square root of the average squared distance of each point from the mean value.</span></div>"
                    ],
                    "flags": 0,
                    "guid": "J_3UX/6(cm",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Variance formula",
                        "[$$] Variance =&nbsp; \\frac{ 1 }{ n } \\sum_{i=1}^{n} (x_{i} - \\mu)^{2}&nbsp;[/$$]<div><br></div><div>Mathematically variance can be defined as the average of summation over each of the points, when you subtract each of it to the mean and square it.<br></div><div>OR</div><br><div>It is the average of all the squared distances of points from a mean "
                    ],
                    "flags": 0,
                    "guid": "mkW)+9agn`",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Intuition of variance",
                        "<div>Averaged squared distance of each point from the mean value.</div>"
                    ],
                    "flags": 0,
                    "guid": "q5YGqAy}@<",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why is domain knowledge important",
                        "TLDR<br><br>For Sanity checks<br>finding false readings<br>classifying errors and outliers<br><br><br><div>One has very little chance of solving a problem if one does not understand what one needs to solve.&nbsp;<br></div><div><br></div>It’s difficult to come up with project ideas in a domain that you don’t know much about. It can also be difficult to determine the type of data that may be helpful for a project.<div><br></div><div>Knowing the domain is useful not only for figuring out projects and how to approach them, but also for having rules of thumb for sanity checks on the data. Knowing how data is captured (is it hand-entered? Is it from machines that can give false readings for any number of reasons?) can help a data scientist with data cleaning and from going too far down the wrong path. It can also inform what true outliers are and which values might just be due to measurement error.<br></div><div><br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "beC)U?4sPz",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Problem with variance and standard deviation",
                        "Even one Outlier can corrupt the variance and standard dev.<br><br> Because it is based around mean and not median."
                    ],
                    "flags": 0,
                    "guid": "h5DbJ=S_.d",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Advantage of mean, variance and standard deviation",
                        "By just knowing the mean ,sd and variance, we can imagine the pdf (probability density function) and density plot of the features."
                    ],
                    "flags": 0,
                    "guid": "rH@oQ|<qoh",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Output variable is also known by these names",
                        "Dependent variable / Class / Class-label / response-label"
                    ],
                    "flags": 0,
                    "guid": "B%nW}Ue!}",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Advantage of median as compared to mean",
                        "Mean can be corrupted by outlier, whereas median isn't."
                    ],
                    "flags": 0,
                    "guid": "cHuwh~FLTL",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "When can median be corrupted ?",
                        "When more than 50% of points are corrupted."
                    ],
                    "flags": 0,
                    "guid": "I@7Q9sC}iX",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is EDA ? When is it done ? Why is it called exploratory ?",
                        "Exploratory data analysis is task of analysis of data using simple tools (such as plotting tools) using statistics and linear algebra and other techniques.<div><br></div><div>This is done before we create model for ML.</div><div><br></div><div>Exploratory because we don't know anything about the dataset when we start. We're trying to understand what the data is.</div>"
                    ],
                    "flags": 0,
                    "guid": "QHXVX$,[k2",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "MAD. How is it related to std-dev ?",
                        "Median absolute deviation<div><br></div><div>It is similar to standard deviation except it uses median instead of mean to avoid getting corrupted by outlers.</div>"
                    ],
                    "flags": 0,
                    "guid": "RaJ$svGLn*",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "EDA"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "1356dbf8-3db5-11ea-98f9-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "-uIalImQqxZ913FHkoQuUenkiFCDJOGl-2RpH_fmb2HZwk2xea9TUL3q3AdEN1x2FPaDi5k9N5A5TBlXiLJPMGnt1HXDY8u-uvk0JDj8POeF5FMROIe3Foz_WiQBYf9y9PKL.png",
                "-yXdlMMV1B3DrMtU16_dEdoX1TiSIHHqmENJ9sqqis2wi26_7Na6f7JVz9on5HkAfgHSKVaGDuK1SvLkigpx2xzaAjsS2ET1gnIgt-9G8WMGDOy69tEOPf2ovrcAMwo-kHF4.png",
                "1024px-Binomial_distribution_pmf.svg.png",
                "1moHhd-crJr5wLCdbPgSitAIZrHxYzoEHfZd46-Sq2eeazn-wpeyasY5LOsE87rKEe8mlJVdABBi9Eeu6G2soF1FR9gXchqaQtUqztsP8BdUKa6xJCe8CTZhwzvo7rZDnOg0.png",
                "3hUELI1ib0pa06scj_HPAyu4AJb9HaItXhCwrsDwhn82TkLDTX6ZxgdLQDwaLUZBPcsvCHsevhuuwAlBaBMBVmVp_6eywJzTVdrt02HIh2IYQ8P2Kr4UZDIpN8OlrW0nt_sY.png",
                "4VPm9lvzOWVys2NVQAxXd76k8WGmJnmAzGU9ntaclEGS6IqIL5OkvcDkRk9x2-IXE-Aj-YcjvsnDc3qfwTQCcn5G-vgYEw-oip1wRMncJ6rmH0bDe5JfJt1mAqYBzYO2wQb4.png",
                "5rMRZTOKMgXk3igRMSwloFVUaU6nNK3sS2JaMR8FEXM3PAHXgraD7jQqHh2kpPC27kVEVHyLZkVLf5kW04Nn-RSD6bozEwUuN_qhuvZ_Np5-TpAHHMfHBN2iUiTzQpd4BKfZ.png",
                "9AXcYXHCH5X2RaF8GsPXrelxh-J_ZnV6EvAQpifAqFAAyPsFIi7ft4S1eQU-sjAkksA8yPT8i1OGryEs-CykKjUmwPHSvq3139IDGNeIhc-1ugEAJWJOCLv5P3COk9nEWcqL.png",
                "DkchCr3JI3i3q6S3AhVeHUuqpVUzWFhXlVdvGYHoh4j0PmYYmEEihEX0yOHIkv5cwV8aURLP_l7LkcOuC6LW_DmZCPdEPUAVC00CYbMQ-lToFwIqNrHiOHjl3FNvXAvdW2GY.png",
                "DrZ_CUoPtHGtuyvy1rzKw5um95nVgc4AFjpjN0PGwiw4KElf8A1blPJcPyuhPDnC60zR4iHHI2cT4BOHSLhrvH4ax9crSHzhmtkEjlocJmfHbike0wEPpeASAPR499FJPVae.png",
                "E-nGmjv-sm2IeicqwRDAZQYcjz_gpPufJJMvE7Oa4oLxu9A6rsRnDWBiei3ZR7c7T5C7066LBiehdtlCAoiMaz0TZKyOkljLmB2Xg8ZVuvenQW0BiCVsrJnXUUUWI3cVonv9.png",
                "FIde-i1fkn7cg8-bIxXral6yjsw7WQ0dYCqi9zbXJkz1x55QUVoqujZNyun3sBAug2HhFTtevDvsUPj2aesgExglScd2hVMLEJhzxJeXf2B-JhHhK3rBm9e-niuQ5wUJ3kfh.png",
                "FdmcngL5bySvNyfDuzXfL5sRzrv9mB9au6EiG1zLAUpkOt0V7e0DA3SqDR4DOnA4uyQsfI-iLM_LZpiH4b29QR4fsLoFlyOLeEWlxpFhxxWqxSRHUwwX26vx5CTSNTNsYyjP.png",
                "GMRpFGyWs6qu_2PTZ5p_-LaZPq2hE-aC2VxqNS9a6YBtUqjcg_A-qkNRiJVhzChlINlmMsXuFcukTlnCJPE1WKSCjDIZXtTbMtUuUcclHF4_MdujpoZbWT3turzmA1OF282x.png",
                "GoBJx8DuOa0MP4-e8oeZIncuEy3ygR_EXBAiTWITG3vwQLD60xFplo9AbuEA4dQMIiwfBa_PUY9WmXisGx6cyj9KZ9StR4EqdfSevPOyq96JCDRyqQ4_vQjfHwdbX-jPAF31.png",
                "HQOD2YPr8yM_XOmbAlVLfrgITT9DdAWraMD0E3nEWZx7goeLPyT0IoDfa0uhzfIEDKvv9kWXtv_mcHwYCsfWAzoL8dvcOcrWRCGpnZ2PZGyuHqnBueIUBfRyODF1pACW13Ps.png",
                "HwPX8y0vClszm-Mr-9yVchRgLAmAKCn7E7iGvxU81swDgARDegkkyuQptDiFfT0w_mKzGS689ZeoVsvIxHSzYYvHbmJgd0S4nKVW8NY2R2fQvsT3M-5I8Imru47Grhp1xH3a.png",
                "KS_Example.png",
                "MwOl_kKYgs3Exycf5a8qNROUivYzKZ5QdAf5fC6YYjwxL4afwj__AxSRhbui20zsUm6_-H2Jlo0ZjiS70CR9b_pHLcuQU6o2frchv5O_Va2o7YgGh4dJuFkrasGjVoVTbHsG.png",
                "O-TmUASGxA77bBLwkPVo69QK5er5gL6XieSWxn8adg7AHb1akFPCPgms7kTp2wNZGYCiOAQg02nIEIjHeN0BYf7iwFGKcJlTJeHB4LX36_9-hZdxY5-i4FMNMf08OFqS06H9.png",
                "PpRapYBJBc2wumvUB1HMUcYeXxMojxfHTgrHkdTa_bAwGwi0mIMJa0eR0kRXjduRsfnUrrYE0plYORwsuqJ9TT6u4KJxuMxMuxFpFaQp3EpmZgbG6GT3O0UmA7Yah7Xxdlrc.png",
                "RWlQqEh7A3o5PH1G6NeDBRqsBZTfl4NgyHvSGv8dyoZ5WR3GZ4QUB4IQzX95N1oV55zNEd9owWqWhNlSoP-YI7xzD0eS1lROxZz78scMqMva6PQa0B8KUtAEou6If4D6ydSh.png",
                "SmAR47I_-XkpyIzz4OkO-CuKOzETX0gnA8NbFQ8kJDiR3uxCpnCausLOuutn4VH-wvdw-3Wm67UyoHX4_3WzE581mPLFJf4BD_RlwNQOgmucrVjj4fWdGzq3UjMyKjLXmq8O.png",
                "WTgBlo3xo7BOu4SJLG-NKXo2TER-CjvAF2SkLja93ueWyHneOisLgUzsRbRJDb1dDTGpxp1fxV0T43RgEzznPQYIaYFiVVwf-Sp0xGxO9JHOVVZ9uyVwcItxDYEFv8hRLjuZ.png",
                "Z9xYq4yNzBUve7Gk07a83JS4JuqWU4fvbSgnOZycktK0GKDi6wFuUUoiWGakVRs3pis6PtSkBBS2aWauCG-nq_O4VXqpQDSZQ10G3i5N9qNmr018joznFCPvlf8J6rbJjP94.png",
                "ZY8Ta1bf0ZfPpE_pYBfydM_no3LllgCzu1TBy4UI5PtPJ-IeltvGVHPC7i2IRGSJNLNxkOLz_UKtVMh2E3v6vRZoUqJmtJo9748D_fBip7HYguRSCjVu-jS6zWyY-qbwdwv0.png",
                "__0HL_5jICrK1DLxoxYGTBaWlHURy2pqm9YmY65r1AoJk0YbMhPJxMgfqZiCki0bgSJ2LkBMyyswiDt5VbLIDL1TK-XzXHwj3datDHK6DUsH2K92CxOONuLaSMgP2Ib25elh.png",
                "aCAKGMnT_59Y1_skV9wQzeDw8s0UCYe0VzsxWx64uwfY45FaYts8T2VvxtetejB9Ctkj7NVp8V1xUKiOXVt5q5oC_oQU4QAQCXwC4nTHfZ_-aqeZ4Bxef9PVn0ZHMzeNLUXZ.png",
                "bvXa8TVjVtlp3lrho3Ci5NHzSkfo59L0Ty2igkUKojPC4Yxw43Y1_jbThgz6n40EaoAPoa3W0_qZ-s-qrhdikE4hNIpJVa0g5gTOFesj3pah9dMnk_U1Ax_FDen35d7ep2dQ.png",
                "dPG-cq6g4Tt3P0_qX55Z4J52V2ZehnzckW0I37KgnsZ8flVgNl_Dxyl0u-IKVn91GDhmW4X0sn468Q6OqlQzw_E_sweB7D8XzL_-jjIcKUlP7AVBLauI9N916DowU1wgDAuw.png",
                "hP8cCXSa4Jq-Xw6IdemmH69VzjbmEb4OcHbhUL7KCVLYxzdbFOEP9L6mTSEMMU3juK4eLnR_rQPRG6yg7PKiNYwttf_2ghcLscK_V2okuJnmXUJd0MyAffROI-cgOfEZmvYF.png",
                "iBLpvXzQ6nX827oHVaop5QBZ794t72peFD_4IiMUgdul3RAkTDfWUFJ5WLpsZwDuTZFZBND0UQfg4HNcMJNPRyK-mBm76ZFV9U62uodWWlB-wqznPrqZwIY2t9KUTKRVZAOZ.png",
                "icgiey33_YvdSQO_aJ-5aDqI8-2LnZO7bSpIU6YKhprgQAlSpymtbawyI6sINl010WXKYZAL_TCiWtPDfWdW0kQN3cwGguFmc-ZZMmJkzox5kwBHcq0trMSNlkqvYvELFw6D.png",
                "jCO1g5dhkMFoAx80PGHg2AQG-K2Pu8x4p6nbJbuB1fY8D3IeFt4AS0XI5Op9lQxkLlRrbMxs96k1m7CVhLqtgrM7XsT6vu7HUTeCu30oKQ5XIZ0CO3VOZxO5LAZ546CvTVyb.png",
                "latex-0b5e3309f2a63153876c01437774f38df1b2caf1.png",
                "latex-15c23f9653895e812bfdc20de9aea6fe8f8eb0b2.png",
                "latex-1b96433b91f07ed079c5f833b950dbef558119a1.png",
                "latex-72430fcdb7e2e75864d4e437bc18e05944f422d9.png",
                "latex-9d32666cae43b50402d439d31e9a59ba80dde5a4.png",
                "latex-bcbbbd442226f5afd2bfa9a99675190df785b6b8.png",
                "muZarnFAVGktYRebYO2bObgRwZfnWhaTzSE90rlyk3JoTEMCHTn28WrhM6bqJ8CaIBUUxgeCTcagJauMtZn3EPJ05zUOxV37-p-R_bCHR1kjp6noW_GZ4qnT2eCJg3susC61.png",
                "nqp3S8f9tSoAa_Xr-uz1pwH1Jg019Qg4BLDYGqxnfrDzcJRRASOUL6Ub_M_ros4w-meUiGFTQKO0xxuXfVALUOUHrFRw2ZPCBKoOC_f9hhOtDCuG-zE1CZMti9WB-8KmHlt0.png",
                "paste-1d7e911742c56967e1051a254d74010f204a1117.jpg",
                "paste-4a97959a95c06938f902127d1b2bf31563b83a85.jpg",
                "paste-cab3e31bffd27baa6310b5559831306fd4d947f9.jpg",
                "paste-fe765f23724813199a798946ab64204b4daa840d.jpg",
                "r0T8F6UF6cYjwuT0HKpQhfEAIkZciUnGy7xJBpeDt5tX0_tXMItcYIunKgB4tXv57Oo1beY6ypMaDxiM1BbEVIEbZ2DlyXmrozEJRBBlTmcbjM_AqwgcRONGn-KLV3OQGrUz.png",
                "reI2gHC96fel5mM443zoJgr144JCPN1KA73ytqUAuz3ovC1xKjgFPS4JpiDSIUko4pSKnw4vUb5ljRYqa5PYbOGXMUuoPh6ScxhcvVGt0QiBnACwRDD1t3qgmKTKqdGNaMtx.png",
                "s0B2FIOEY1ooS1FzgC5w1G_9B4A_2R2RlIIO-q9IrzYcm6Tdi7blyISNLiMsp-ONiSwle7EfyM_B_C_OPVb_u98MWt1RwGCoxch5vHojDSNmKmHe-m6b5WO3cAW4Yb65pY4-.png",
                "sxfx3yotuy18yByvpyW1KvU1OzM4uzx8da96vKqdTdQvqz2rHcqiMOjRLybXg3VPLtfjUcKafY9vrlYYd8Y0Pk0EhLNodGEW0H6NxairzZC1N00cE8QVutkXP7y4ndupeuc4.png",
                "t8dYRrZPisYaP5i_JT4UDAm2GBRIOS2ft1l1n7aKzCjID-yCWKHFikDTJGhhfpxeVagitQZwY_aaAvQPuVmiYcogfctyXPGfiKftWqoVMwd9u88_WFm5KADUGlcWMD_qAJ2D.png",
                "tEwTqIVArI0oKJpVet7f2tcFn9SLJ5WNkUKvYAaVFC_aGLCeZmmRz0cOBU28A7n8B7llbiNq8Tji9cFN4fkjUMyFXywcZtQIUZgTaA8VwYdUzW6HQUhaLvH17_RCHzULerBk.png",
                "tFejGCnIC2DmYPnWKviGEQVBJnFA_0porhALbbw_fCOpxQ37zAMVZbrzE6DsbLonal2CWPeZ8-JP2-caNUfHUtnkoqGZRKS8R9rXLR0OPvYOFyBbzZFvOcUKcTi9FhHYR1CF.png",
                "u2HaIfgO9xCWVI17BdlskYpzD9TuEyGS9ys6yYHklKtRs9CuLDoYWaZpL7hMKJwbkZRjIWpu6YXHRvSMiSOsIL2kM-dlVAQvDI8PscCxzXGJYPPw94pve1_Q92g2NdJxK-2m.png",
                "uIoeJm9VR6rZrQ1_0zUcY1xk3613KfztwhYxT1Dsi0VgShmRhHJG3VaA2mj33WD81CVhMf5kSMCrcN9Te4XNM9B2dut-EFZpBhvXjdAA5UqbuEssZ1qvt_4-vLUW-iLhMFOB.png",
                "uoKGVAp3mNJGw5d71jVZutCg8OkbiNNGsV9U8Yv-10z1npUko3I9QXKo-wnp50UmMYwJ0ETiT74HYkE2SUDLY_NbwITmywsxM7MRKtBFic__Ijn7QQTfRdvHM4yk3HoNlEKs.png",
                "xFnGsh6cQqAzl_UAiLOGxssGwMp7j-F7XN-zK5p2ZIoDV3TE6v5L2um-qV49UOvL1mIGZI4kcwm4TZDl-UxpbBXQZHTftIiV_UFZJr4cUF8ZojV-M4QaemCi15_X5_0cZG0A.png",
                "xQiEXqfutC81KGdE0-1VnxgM172cO0vgeuHXKgFszdufLxi5iiYVN656H1oePcmfd9Uv7neZ2NiKeuxvX3r6Ip7XiJkB1Q8yKhWhhPnAWsKlmFFjBzCB05tLGds744vdt2Hd.png",
                "xyKL6uxDCrHgN0ZD2MwO3b1OLmX2Bk9u8fL6Wf91Y-tjqhPBcEhry_uGfy-XxFCMSUFhUuk8yucUlotwTpFAQHNmZrYoEidASiZDJbqRPSn8rucBC4UgxFeX65P89UNbqzlb.png"
            ],
            "name": "12. Probability and Statistics",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Random variable and 2 examples",
                        "A&nbsp;<strong><em>random variable</em></strong>, is a variable whose possible values are numerical outcomes of a&nbsp;<strong>random experiment</strong>. There are two types of random variables.<br>1.&nbsp;<strong>D<em>iscrete Random Variable</em></strong>&nbsp;is one which may take on only a countable number of distinct values such as 0,1,2,3,4,…….. Discrete random variables are usually (but not necessarily) counts.<br>2.&nbsp;<strong>C<em>ontinuous Random Variable</em></strong>&nbsp;is one which takes an infinite number of possible values. Continuous random variables are usually measurements.<br><div><br></div><div>Example of rolling a fair dice and tossing a coin.</div><br><div>source - https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/random-variables<br></div><div><br></div><div>https://towardsdatascience.com/basic-probability-theory-and-statistics-3105ab637213<br></div>"
                    ],
                    "flags": 0,
                    "guid": "M#Jh>RAz,[",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How can you import MAD fucntion ?",
                        "from statsmodels import robust<div>robust.mad()</div>"
                    ],
                    "flags": 0,
                    "guid": "uJLMomYjB@",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Advantage of boxplot over histograms and CDF.",
                        "Histograms don't show where the percentile values are. We can use CDF for that.&nbsp;<div><br></div><div>But Box plots gives us percentiles more clearly.</div>"
                    ],
                    "flags": 0,
                    "guid": "vS{ROZa?J)",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Permutation definition",
                        "Each of several possible ways in which a number of things can be ordered or arranged.<div><br></div><div>permutations, or&nbsp;<strong>all possible ways</strong>&nbsp;of doing something.<br></div><div><br></div><div>ABC and BAC are different in terms of permutation</div>"
                    ],
                    "flags": 0,
                    "guid": "fpzxe#F,Da",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Permutation formulae and it's interpretation",
                        "<div><div>permutations, or&nbsp;<strong>all possible ways</strong>&nbsp;of doing something.<br></div><div><br></div><div>[$$]P(n,r) = _{n} P_{r} = \\frac{n!}{(n-r)!} - permutation[/$$]<br></div><div><br></div><div>And this is the fancy permutation formula: You have&nbsp;<strong>n</strong>&nbsp;items and want to find the number of ways <b>r</b>&nbsp;items can be ordered.</div><div><br></div><div>Examples:</div><div><br></div><div>Ex 1 :- Let's say we've <b><u>5 chairs and 5 people</u></b> :-&nbsp;</div><div><br></div><div>[$$]<span>P(5,5) =</span><span>&nbsp;</span><span>\\frac{</span><span>5!}{(5-5)!} = 5! = 120</span><span>[/$$]</span></div><div><span><br></span></div><div><span>We've 120 ways to sit them.</span></div><div><span><br></span></div><div><span><br></span></div><div><span><br></span></div><div><b><u>Eg 2 :- 5 people with 3 chairs</u></b> :-</div><br></div><div>[$$]P(5,3) =&nbsp;\\frac{5!}{(5-3)!} = 60[/$$]<br></div><div><br></div><div><br></div><div>Eg 3 :- How many ways can we award a 1st, 2nd and 3rd place prize among <b><u>eight</u>&nbsp;</b>contestants? (Gold / Silver / Bronze)<br></div><div><br></div><div><br></div>[$$]P(8,3) =&nbsp;\\frac{8!}{(8-3)!} = 336[/$$]<br><div><br></div><div><br></div><div>Source:-&nbsp;https://www.khanacademy.org/math/precalculus/prob-comb/combinatorics-precalc/v/permutation-formula</div>"
                    ],
                    "flags": 0,
                    "guid": "o|iz3G00}J",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Combinations formulae, it's intuition and example.",
                        "<div>Combinations</div><div><br></div><div>[$$]C(n,r) = _{n} C_{r} = \\frac{n!}{r!(n-r)!} = Combinations[/$$]</div><div><br><br>How many ways do we have of choosing r out of n ? <br></div><div><b><u>Examples</u></b></div><div><b><u><br></u></b></div><div><b><u>Ex 1. How many ways do we have of choosing 3 people out of 6 ?</u></b></div><div><b><u>(Here ABC and BAC is same.)</u></b></div><div><b><u><br></u></b></div><div><b><u><br></u></b></div><div><div>[$$]C(6,3) =&nbsp;\\frac{6!}{3!(6-3)!} = \\frac{720}{36} = 20[/$$]</div><div><br></div><div>We've 20 ways of choosing 3 people out of 6.</div></div>"
                    ],
                    "flags": 0,
                    "guid": "d|zl/a5Q<_",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Permutation v/s combination with examples",
                        "Permutation is all possible ways of doing something. <br>The number of combinations is the number of ways in which we can select a group of objects from a set.<br><br><div>So, in Mathematics we use more&nbsp;<i>precise</i>&nbsp;language:</div>When the order doesn't matter, it is a&nbsp;<b>Combination</b>.When the order&nbsp;<b>does</b>&nbsp;matter it is a&nbsp;<b>Permutation</b>.<div><br></div><div><strong>Permutations are for lists (order matters) and combinations are for groups (order doesn’t matter).</strong><br></div><br><br><div><i style=\"\"><b>\"</b>My fruit salad is a <b>combination </b>of apples, grapes and bananas<b>\"</b></i>&nbsp;We don't care what order the fruits are in, they could also be \"bananas, grapes and apples\" or \"grapes, apples and bananas\", its the same fruit salad.</div><div><br></div><div><br></div><div><b><i>\"The combination to the safe is 472\"</i></b>. Now we&nbsp;<b>do</b>&nbsp;care about the order. \"724\" won't work, nor will \"247\". It has to be exactly&nbsp;<b>4-7-2</b>. (<b>Permutation</b>)</div><div><br></div><div><br></div><br>So a combination lock should actually be called a permutation lock.<br><br><br><div>Combinations are just permutations where the order is not taken into account. So the number of permutations will always be greater than the number of combinations. Using the definition of permutations, we can get the formula for combinations.&nbsp;<br></div><br><br><div>In English we use the word \"combination\" loosely, without thinking if the&nbsp;<b>order</b>&nbsp;of things is important. In other words:</div><div><br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "gHn@sNI|Tx",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Combinations definition",
                        "Number of ways in which we can select a group of points/objects from a set."
                    ],
                    "flags": 0,
                    "guid": "Qn?RB`Lbfa",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Sigma square",
                        "variance"
                    ],
                    "flags": 0,
                    "guid": "im?B7qV~-x",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Descriptive statistics",
                        "A descriptive statistic is a summary statistic that quantitatively describes or summarizes features of a collection of information.<div><b><br></b></div><div><b>Descriptive statistics</b>&nbsp;are broken down into measures of central tendency and measures of variability (spread).</div><div><br></div><div>Some measures that are commonly used to describe a data set are measures of&nbsp;central tendency&nbsp;and measures of variability or&nbsp;dispersion. Measures of central tendency include the&nbsp;mean,&nbsp;median&nbsp;and&nbsp;mode, while measures of variability include the&nbsp;standard deviation&nbsp;(or&nbsp;variance), the minimum and maximum values of the variables,&nbsp;kurtosis&nbsp;and&nbsp;skewness.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "i]Lp%K;LDX",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Statistics",
                        "optional"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Inferential statistics",
                        "Also called as inductive statistics.&nbsp;Statistical inference<span>&nbsp;is the process of using&nbsp;data analysis&nbsp;to deduce properties of an underlying&nbsp;probability distribution.</span><span>&nbsp;Inferential statistical analysis infers properties of a&nbsp;population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is&nbsp;sampled&nbsp;from a larger population.</span><div>Inferential statistics can be contrasted with&nbsp;descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.</div><div><br></div><div>Statistical inference makes propositions about a population, using data drawn from the population with some form of&nbsp;sampling. Given a hypothesis about a population, for which we wish to draw inferences, statistical inference consists of (first)&nbsp;selecting&nbsp;a&nbsp;statistical model&nbsp;of the process that generates the data and (second) deducing propositions from the model.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "l(I;<;FA*,",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Statistics",
                        "optional"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Descriptive statistics and inferential statistics",
                        "Descriptive statistics is distinguished from&nbsp;inferential statistics&nbsp;(or inductive statistics), in that descriptive statistics aims to summarize a&nbsp;sample, rather than use the data to learn about the&nbsp;population&nbsp;that the sample of data is thought to represent. This generally means that descriptive statistics, unlike inferential statistics, is not developed on the basis of&nbsp;probability theory, and are frequently&nbsp;nonparametric statistics."
                    ],
                    "flags": 0,
                    "guid": "px4iEl*y43",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Sampling theory, what is sampling and types of sampling",
                        "<b>Sampling theory</b>&nbsp;is a study of relationships existing between a population and samples drawn from the population.&nbsp;<div><br></div><div><b>sampling</b>&nbsp;is the selection of a subset (a&nbsp;statistical sample) of individuals from within a&nbsp;statistical population&nbsp;to estimate characteristics of the whole population.&nbsp;<br></div><div><br></div><div>The main&nbsp;<b>types</b>&nbsp;of probability&nbsp;<b>sampling</b>&nbsp;methods are simple random&nbsp;<b>sampling</b>, stratified&nbsp;<b>sampling</b>, cluster&nbsp;<b>sampling</b>, multistage&nbsp;<b>sampling</b>, and systematic random<b>sampling</b>.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "L}1K$%HcGV",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Probability theory",
                        "Probability is quantified as a number between 0 and 1, where, loosely speaking, 0 indicates impossibility and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur.<br><br><div>Probability is the measure of the likelihood that an event will occur in a Random Experiment. <br><br><b>Probability theory</b>&nbsp;is the branch of&nbsp;mathematics&nbsp;concerned with&nbsp;probability.&nbsp;<div><br></div><div>Central subjects in probability theory include discrete and continuous&nbsp;random variables,&nbsp;probability distributions, and&nbsp;stochastic processes<br></div><div><br></div><br>Example<br>A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (“heads” and “tails”) are both equally probable; the probability of “heads” equals the probability of “tails”; and since no other outcomes are possible, the probability of either “heads” or “tails” is 1/2 (which could also be written as 0.5 or 50%).<br></div>"
                    ],
                    "flags": 0,
                    "guid": "gS;1cV`N}=",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Probability distribution",
                        "a&nbsp;<b>probability distribution</b>&nbsp;is a mathematical&nbsp;function&nbsp;that provides the probabilities of occurrence of different possible outcomes in an&nbsp;experiment. In more technical terms, the probability distribution is a description of a&nbsp;random&nbsp;phenomenon in terms of the&nbsp;probabilities&nbsp;of&nbsp;events. For instance, if the&nbsp;random variable&nbsp;X&nbsp;is used to denote the outcome of a coin toss (\"the experiment\"), then the probability distribution of&nbsp;X&nbsp;would take the value 0.5 for&nbsp;<i>X</i>&nbsp;= heads, and 0.5 for&nbsp;<i>X</i>&nbsp;= tails(assuming the coin is fair)."
                    ],
                    "flags": 0,
                    "guid": "wn!loI~P?J",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Probability distribution classes",
                        "Probability distributions are generally divided into two classes. A&nbsp;<b>discrete probability distribution</b>(applicable to the scenarios where the set of possible outcomes is&nbsp;discrete, such as a coin toss or a roll of dice) can be encoded by a discrete list of the probabilities of the outcomes, known as a&nbsp;probability mass function. <br><br>On the other hand, a&nbsp;<b>continuous probability distribution</b>&nbsp;(applicable to the scenarios where the set of possible outcomes can take on values in a continuous range (e.g. real numbers), such as the temperature on a given day) is typically described by&nbsp;probability density functions&nbsp;(with the probability of any individual outcome actually being 0).<div><br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "Nt:I/^oMrW",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Univariate",
                        "A probability distribution whose sample space is one-dimensional (for example real numbers, list of labels, ordered labels or binary) is called&nbsp;univariate"
                    ],
                    "flags": 0,
                    "guid": "x_iA;E[_gM",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Multivariate",
                        "A distribution whose sample space is a&nbsp;vector space&nbsp;of dimension 2 or more is called&nbsp;multivariate."
                    ],
                    "flags": 0,
                    "guid": "wHZz/XfnNm",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Univariate and multivariate distributions",
                        "A univariate distribution gives the probabilities of a single&nbsp;random variable&nbsp;taking on various alternative values; a multivariate distribution (a&nbsp;joint probability distribution) gives the probabilities of a&nbsp;random vector&nbsp;– a list of two or more random variables – taking on various combinations of values."
                    ],
                    "flags": 0,
                    "guid": "yq^E?oD,L<",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Important Univariate probability distributions",
                        "Important and commonly encountered univariate probability distributions include the&nbsp;binomial distribution, the&nbsp;hypergeometric distribution, and the&nbsp;normal distribution."
                    ],
                    "flags": 0,
                    "guid": "e{T(}~A^.v",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Random Variable and it's types",
                        "A&nbsp;<strong><em>random variable</em></strong>, is a variable whose possible values are numerical outcomes of a&nbsp;<strong>random experiment</strong>. There are two types of random variables.<br>1.&nbsp;<strong>D<em>iscrete Random Variable</em></strong>&nbsp;is one which may take on only a countable number of distinct values such as 0,1,2,3,4,…….. Discrete random variables are usually (but not necessarily) counts.<br>2.&nbsp;<strong>C<em>ontinuous Random Variable</em></strong>&nbsp;is one which takes an infinite number of possible values. Continuous random variables are usually measurements."
                    ],
                    "flags": 0,
                    "guid": "MlEBroOX62",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Examples/types of statistical dispersion<br>",
                        "Common examples of measures of&nbsp;<b>statistical</b>&nbsp;dispersion are the variance, standard deviation, and interquartile range.<div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "JRShSha0r^",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Basic Idea of Probability",
                        "<b><img src=\"t8dYRrZPisYaP5i_JT4UDAm2GBRIOS2ft1l1n7aKzCjID-yCWKHFikDTJGhhfpxeVagitQZwY_aaAvQPuVmiYcogfctyXPGfiKftWqoVMwd9u88_WFm5KADUGlcWMD_qAJ2D.png\"></b><br><div><div style=\"\">Suppose, we’ve a new flower xq in the dataset and we want to identify it’s type.We can see that it’s lying in the intersection so we will give a percentage mentioned above i.e 80% chance it’s versicolor.</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "kKe$qH1Xx9",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is a Random Variable?",
                        "A random Variable&nbsp;<div style=\"display: inline !important;\">X, is a variable whose possible values are numerical outcomes of a random phenomenon.</div><div><br></div><div>Ex: Rolling of a dice</div><div><br></div><div>X = {1,2,3,4,5,6}<br><br></div>"
                    ],
                    "flags": 0,
                    "guid": "OVQHS:zbra",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<div style=\"\">Types of Random variables</div>",
                        "<b><img src=\"iBLpvXzQ6nX827oHVaop5QBZ794t72peFD_4IiMUgdul3RAkTDfWUFJ5WLpsZwDuTZFZBND0UQfg4HNcMJNPRyK-mBm76ZFV9U62uodWWlB-wqznPrqZwIY2t9KUTKRVZAOZ.png\"></b><div>Discrete Random variable : X is discrete r.v which can take value only from from a distinct set of points( ex: 4 not 4.5)&nbsp;<b><br></b></div><div><br></div><div>Continous Random Variable : Y can take any real value (Ex: Height of people)</div>"
                    ],
                    "flags": 0,
                    "guid": "wO-x-Xk}^_",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Population and Sample",
                        "<div style=\"\">Suppose you want to take the average height of all the people in the world of 7B people. Theoretically you cant do that so you take a random sample of 1000 people based on the proportion they are distributed in 7B otherwise you might take just 1000 Indians or Americans which can do loss. As the sample increases&nbsp;<span>(for eg: 1M from 1000 ) </span><span>we get closer to the accurate answer</span></div><br>"
                    ],
                    "flags": 0,
                    "guid": "H![pKLtBDx",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Gaussian Distribution",
                        "<b><img src=\"5rMRZTOKMgXk3igRMSwloFVUaU6nNK3sS2JaMR8FEXM3PAHXgraD7jQqHh2kpPC27kVEVHyLZkVLf5kW04Nn-RSD6bozEwUuN_qhuvZ_Np5-TpAHHMfHBN2iUiTzQpd4BKfZ.png\"></b><div><div style=\"\">The bell-shaped curve is the PDF (Probability Distribution function) of Gaussian distributed random variable X.</div><div style=\"\">It can be used to find the distribution of weights, heights etc<br></div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "CK^aUG}DMn",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What are the parameters needed for drawing Gaussian&nbsp;",
                        "If our Continuous random variable follows Gaussian distribution then just by knowing it's mean and standard deviation we can draw it's distribution"
                    ],
                    "flags": 0,
                    "guid": "yr0<+$iVJ.",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<div style=\"\">Gaussian Mathematics formula</div><br>",
                        "<b><img src=\"WTgBlo3xo7BOu4SJLG-NKXo2TER-CjvAF2SkLja93ueWyHneOisLgUzsRbRJDb1dDTGpxp1fxV0T43RgEzznPQYIaYFiVVwf-Sp0xGxO9JHOVVZ9uyVwcItxDYEFv8hRLjuZ.png\"></b>"
                    ],
                    "flags": 0,
                    "guid": "ciAnS#B`md",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Take-Aways from formula of Gaussian",
                        "Just remember that<div>P(x) = y = exp (-x<sup>2</sup>) as x increases , (-x<sup>2</sup>) decreases gence the bell curve</div><div><br></div><div><b><img src=\"__0HL_5jICrK1DLxoxYGTBaWlHURy2pqm9YmY65r1AoJk0YbMhPJxMgfqZiCki0bgSJ2LkBMyyswiDt5VbLIDL1TK-XzXHwj3datDHK6DUsH2K92CxOONuLaSMgP2Ib25elh.png\"></b><br></div>"
                    ],
                    "flags": 0,
                    "guid": "MFCrMLDG~s",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Skewness&nbsp;",
                        "<b><img src=\"dPG-cq6g4Tt3P0_qX55Z4J52V2ZehnzckW0I37KgnsZ8flVgNl_Dxyl0u-IKVn91GDhmW4X0sn468Q6OqlQzw_E_sweB7D8XzL_-jjIcKUlP7AVBLauI9N916DowU1wgDAuw.png\"></b><div>Skewness tells us about how far our distribution is from symmetric distribution<b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "OwXzvmc=s0",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Kurtosis",
                        "<b><img src=\"E-nGmjv-sm2IeicqwRDAZQYcjz_gpPufJJMvE7Oa4oLxu9A6rsRnDWBiei3ZR7c7T5C7066LBiehdtlCAoiMaz0TZKyOkljLmB2Xg8ZVuvenQW0BiCVsrJnXUUUWI3cVonv9.png\"></b><div>It measures how peaked your distribution is. So if Kurtosis (X) = 2 it's peak will be greater than normal distribution and if negative it'll be lower.</div>"
                    ],
                    "flags": 0,
                    "guid": "q=+btX|t.}",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Standard Normal Variate",
                        "<div style=\"\">If you’ve a random variable Z such that its mean = 0 and variance = 1 then it is SNV</div><br>"
                    ],
                    "flags": 0,
                    "guid": "z]MP3$Q@}P",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Standardization of Random Variable",
                        "<b><img src=\"-yXdlMMV1B3DrMtU16_dEdoX1TiSIHHqmENJ9sqqis2wi26_7Na6f7JVz9on5HkAfgHSKVaGDuK1SvLkigpx2xzaAjsS2ET1gnIgt-9G8WMGDOy69tEOPf2ovrcAMwo-kHF4.png\"></b><div>We substract the mean and divide it by standard deviation for Standardization</div>"
                    ],
                    "flags": 0,
                    "guid": "m4s=!;#u#|",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How Kernel Density Estimation is applied",
                        "<img src=\"FIde-i1fkn7cg8-bIxXral6yjsw7WQ0dYCqi9zbXJkz1x55QUVoqujZNyun3sBAug2HhFTtevDvsUPj2aesgExglScd2hVMLEJhzxJeXf2B-JhHhK3rBm9e-niuQ5wUJ3kfh.png\"><div>Kernel density estimation we make graph in red(kernel) from the sample points and then we sum up all the heights as seen from x in the above diagram<b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "lG8SrBT!ac",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Bandwidth in KDE estimation",
                        "<img src=\"paste-cab3e31bffd27baa6310b5559831306fd4d947f9.jpg\"><div><div style=\"\">If bandwidth is low we get smoothed (red)&nbsp; if too large then too smoothed (green)</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "qWTHANkGZD",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Sampling Distribution in CLT",
                        "<b><img src=\"O-TmUASGxA77bBLwkPVo69QK5er5gL6XieSWxn8adg7AHb1akFPCPgms7kTp2wNZGYCiOAQg02nIEIjHeN0BYf7iwFGKcJlTJeHB4LX36_9-hZdxY5-i4FMNMf08OFqS06H9.png\"></b><div><div style=\"\">1 ) Suppose we’ve a population of X&nbsp; (disb. Of incomes)(not necessarily Gaussian) and we take out ‘m’ samples and their sample means x of each sample S.</div><br></div><div>2)&nbsp;<div style=\"display: inline !important;\">If we get sample means of each sample the distb of sample means is x<span style=\"font-size: 16.6667px;\"><sub>i</sub></span>&nbsp;and disb of x is called sampling disb of sample mean</div></div><br>"
                    ],
                    "flags": 0,
                    "guid": "M]2zdP=tf",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Central Limit Theorem",
                        "<b><img src=\"u2HaIfgO9xCWVI17BdlskYpzD9TuEyGS9ys6yYHklKtRs9CuLDoYWaZpL7hMKJwbkZRjIWpu6YXHRvSMiSOsIL2kM-dlVAQvDI8PscCxzXGJYPPw94pve1_Q92g2NdJxK-2m.png\"></b><div>CLT says that sample means x<sub>i</sub>&nbsp;is Gaussian Distribution (μ, σ<sup>2</sup>/n) if X (population) : finite&nbsp;μ &amp;&nbsp;σ<sup>2</sup></div>"
                    ],
                    "flags": 0,
                    "guid": "e%GalJ/s,N",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "CLT explanation",
                        "<div style=\"\">We take out 30 samples from population X: μ,&nbsp;σ<sup>2</sup>&nbsp; and get the means and plot them we get a <b>Gaussian disb of sample means</b> with mean ≈ population mean&nbsp;μ and variance σ<sup>2</sup>/n. Which can help us estimate the mean and variance of whole population X .Here ,with&nbsp; 30k we estimated mean of 7B population</div><br>"
                    ],
                    "flags": 0,
                    "guid": "Kp*tr{ZSK]",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to test if Random Variable X is normally distributed or not?",
                        "QQ Plot &amp; Statistical testing&nbsp;"
                    ],
                    "flags": 0,
                    "guid": "vxo7_|T}}R",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Steps of making QQ plots",
                        "1 ) Sort Random Variable X and compute their percentiles<div><br></div><div>2 )&nbsp;Create a r.v. Y which is std normal disb and calculate percentiles</div><div><br></div><div>3 ) Plot X and Y percentiles and if their points lie on a straight line rhen they have similar distribution because if not a straight line then their distribution is small. Since Y is normally distributed we can infer that X is normally distributed</div><div><img src=\"paste-1d7e911742c56967e1051a254d74010f204a1117.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "g/BJEf)YuL",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How / Where distributions are used?",
                        "<b><img src=\"HwPX8y0vClszm-Mr-9yVchRgLAmAKCn7E7iGvxU81swDgARDegkkyuQptDiFfT0w_mKzGS689ZeoVsvIxHSzYYvHbmJgd0S4nKVW8NY2R2fQvsT3M-5I8Imru47Grhp1xH3a.png\"></b>"
                    ],
                    "flags": 0,
                    "guid": "v$aSvsYr[C",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<div style=\"\">We get all the answers from the 68-95-99.7 rule if we know that our r.v is Gaussian distribution&nbsp; but what if it isn’t?</div><br>",
                        "We use Chebyshev's Inequality"
                    ],
                    "flags": 0,
                    "guid": "f]G6Tp`g*2",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability",
                        "Statistics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Chebyshev's Inequality",
                        "<div>P (X ≥ μ + kσ or&nbsp;X&nbsp;≤&nbsp;μ - kσ&nbsp;)&nbsp;≤ 1/ k<sup>2</sup></div><div><b><img src=\"nqp3S8f9tSoAa_Xr-uz1pwH1Jg019Qg4BLDYGqxnfrDzcJRRASOUL6Ub_M_ros4w-meUiGFTQKO0xxuXfVALUOUHrFRw2ZPCBKoOC_f9hhOtDCuG-zE1CZMti9WB-8KmHlt0.png\"></b><br></div><div>The green region is what this Probability is implying (example below will clear all)<br></div><div><br></div><div>OR</div><div><b><br></b></div><div>P ( μ - kσ&nbsp;≤ X&nbsp;≤&nbsp;μ + kσ&nbsp;) &gt; 1 - 1/k<sup>2</sup><b><br></b></div><div><b><img src=\"reI2gHC96fel5mM443zoJgr144JCPN1KA73ytqUAuz3ovC1xKjgFPS4JpiDSIUko4pSKnw4vUb5ljRYqa5PYbOGXMUuoPh6ScxhcvVGt0QiBnACwRDD1t3qgmKTKqdGNaMtx.png\"></b><sup><br></sup></div><div>The probablity of yellow region is greater than 1/k<sup>2</sup> because the probability of the green region is less than 1/k<sup>2</sup><b><br></b></div><div><sup><br></sup></div><div><span style=\"font-size: 16.6667px;\">TL;DR -&nbsp;</span>P ( μ - kσ&nbsp;≤ X&nbsp;≤&nbsp;μ + kσ&nbsp;) &gt; 1 - 1/k<sup>2</sup></div>"
                    ],
                    "flags": 0,
                    "guid": "cD/4TkOe.(",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Types of Uniform Distribution",
                        "1 ) Discrete Uniform Distribution<div><br></div><div>2 ) Continuous Uniform Distribution</div>"
                    ],
                    "flags": 0,
                    "guid": "vOV9R6I^-G",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Probability mass function",
                        "<img src=\"r0T8F6UF6cYjwuT0HKpQhfEAIkZciUnGy7xJBpeDt5tX0_tXMItcYIunKgB4tXv57Oo1beY6ypMaDxiM1BbEVIEbZ2DlyXmrozEJRBBlTmcbjM_AqwgcRONGn-KLV3OQGrUz.png\"><div><div>The height of the PDF/PMF gives probability of that r.v. As we can see that in the distribution every r.v has the same probability / equiprobable. Where parameters are ‘a’ and ‘b’</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "KzXx8lK{O-",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "PDF of Continuous Uniform Distribution",
                        "<b><img src=\"GoBJx8DuOa0MP4-e8oeZIncuEy3ygR_EXBAiTWITG3vwQLD60xFplo9AbuEA4dQMIiwfBa_PUY9WmXisGx6cyj9KZ9StR4EqdfSevPOyq96JCDRyqQ4_vQjfHwdbX-jPAF31.png\"></b><div>The probability of any r.v X between a and b is same which is 1/(b-a) because the area under the curve is 1 so height will be 1/(b-a) since width is (b-a) (area = breadth * height)<b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "cl|z2E!g}@",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Bernoulli Distribution",
                        "<img src=\"paste-fe765f23724813199a798946ab64204b4daa840d.jpg\"><br><div style=\"\">Bernoulli can only have two outcomes with p and q = (1 - p) equalling 1 in total.&nbsp;Suppose if our r.v X is Coin toss then it's Bernoulli (p = 0.5)</div>"
                    ],
                    "flags": 0,
                    "guid": "gi3}Mj(%BA",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "PMF of Bernoulli Distribution",
                        "<b><img src=\"HQOD2YPr8yM_XOmbAlVLfrgITT9DdAWraMD0E3nEWZx7goeLPyT0IoDfa0uhzfIEDKvv9kWXtv_mcHwYCsfWAzoL8dvcOcrWRCGpnZ2PZGyuHqnBueIUBfRyODF1pACW13Ps.png\">&nbsp;</b><div>Since Bernoulli has only 2 outcomes at k = 0 and 1 i.e 'q' and 'p' respectively as seen</div><div>Just make sure their total sum is 1</div>"
                    ],
                    "flags": 0,
                    "guid": "M>LQnAa&XO",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Binomial Distribution",
                        "We've a fair coin toss so it's probability (P) = 0.5.&nbsp;<div><br></div><div>Y = Number of times getting heads when we toss it n times (=10)</div><div><br></div><div>Y is represented as Y ~ Binomial (n, P)</div><div><br></div><div><div style=\"\">For the above problem. It can be seen that the values can be between Y&nbsp;ε {0,1,2,3….10} for the above question</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "zP8B87XMU-",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Parameters of Binomial Distribution",
                        "n - Number of trials<div><br></div><div>p&nbsp;ε [0,1] - Success probabilty in each trial</div>"
                    ],
                    "flags": 0,
                    "guid": "NQkqcN?Mc`",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "PMF of Binomial Distribution",
                        "<div>PMF - (<sup>n</sup>C<sub>r&nbsp;</sub>p<sup>k&nbsp;</sup>(1 - p)<sup>n-k</sup>&nbsp;) where n - Number of trials and k - Number of Successful trials</div><div><br></div><div><br></div><img src=\"1024px-Binomial_distribution_pmf.svg.png\">"
                    ],
                    "flags": 0,
                    "guid": "H%w;SLq4O3",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Log- Normal distribution",
                        "A random variable X is said to be log-normally distributed if log(X) is normally distributed."
                    ],
                    "flags": 0,
                    "guid": "B4=d9m3iw",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "PDF of Log Normal",
                        "<b><img src=\"muZarnFAVGktYRebYO2bObgRwZfnWhaTzSE90rlyk3JoTEMCHTn28WrhM6bqJ8CaIBUUxgeCTcagJauMtZn3EPJ05zUOxV37-p-R_bCHR1kjp6noW_GZ4qnT2eCJg3susC61.png\"></b><div><div style=\"\">As it (blue) can be seen it is largely skewed at the right side. This is log-normally distribution&nbsp;</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "bSP&lya7Vz",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Examples&nbsp; of Log-Normal",
                        "<img src=\"aCAKGMnT_59Y1_skV9wQzeDw8s0UCYe0VzsxWx64uwfY45FaYts8T2VvxtetejB9Ctkj7NVp8V1xUKiOXVt5q5oC_oQU4QAQCXwC4nTHfZ_-aqeZ4Bxef9PVn0ZHMzeNLUXZ.png\"><div><div style=\"\">This is log-distribution of comments in a post. Most people comment short but some people comment long.</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "O;|)_IVo;4",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<div style=\"\">How to know X is log normal distribution?</div><br>",
                        "We take the log of values X and then plot a QQ plot of Y = (log(X)) if it’s Gaussian then X is log normal"
                    ],
                    "flags": 0,
                    "guid": "PWcd#k9XdS",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Power Law distribution",
                        "<b><img src=\"jCO1g5dhkMFoAx80PGHg2AQG-K2Pu8x4p6nbJbuB1fY8D3IeFt4AS0XI5Op9lQxkLlRrbMxs96k1m7CVhLqtgrM7XsT6vu7HUTeCu30oKQ5XIZ0CO3VOZxO5LAZ546CvTVyb.png\"></b><div>In the Power-law distribution , 80% of the mass/density lie in 20%. Whenever a distribution follows Power-law it follows Pareto distribution . It's also known as 80-20 rule<b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "s5wd>>LTBr",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Pareto Distribution: PDF and Parameters",
                        "<b><img src=\"MwOl_kKYgs3Exycf5a8qNROUivYzKZ5QdAf5fC6YYjwxL4afwj__AxSRhbui20zsUm6_-H2Jlo0ZjiS70CR9b_pHLcuQU6o2frchv5O_Va2o7YgGh4dJuFkrasGjVoVTbHsG.png\"></b><div><div style=\"\">Parameters :- x<sub style=\"\">m</sub> &gt;0 &amp; α &gt; 0 take these parameters as mean(x<sub style=\"\">m</sub>) and variance (σ)</div><div><br></div>In the above image x<sub style=\"\">m</sub>=1 and as&nbsp;α&nbsp;↑ tail-fatness&nbsp;↓<br></div>"
                    ],
                    "flags": 0,
                    "guid": "Qso+&a:t`v",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Dirac Delta function in Pareto Distribution",
                        "<img src=\"paste-4a97959a95c06938f902127d1b2bf31563b83a85.jpg\"><div><div style=\"\">If α = ∞ then it’s just 1 peak as drawn above . It is called Dirac- delta function</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "vCW(<cD{v_",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to check if our Continuous Random Variable is Power law?&nbsp;",
                        "<div style=\"\">If x (input) and y(Probability) then take out the log of both x &amp; y and if straight line then Power Law .</div><div style=\"\">QQ plots can also be used to check if it follows PAreto Distribution</div><div style=\"\"><br></div><b><img src=\"xyKL6uxDCrHgN0ZD2MwO3b1OLmX2Bk9u8fL6Wf91Y-tjqhPBcEhry_uGfy-XxFCMSUFhUuk8yucUlotwTpFAQHNmZrYoEidASiZDJbqRPSn8rucBC4UgxFeX65P89UNbqzlb.png\"></b><div><b><br></b></div><div><b><img src=\"sxfx3yotuy18yByvpyW1KvU1OzM4uzx8da96vKqdTdQvqz2rHcqiMOjRLybXg3VPLtfjUcKafY9vrlYYd8Y0Pk0EhLNodGEW0H6NxairzZC1N00cE8QVutkXP7y4ndupeuc4.png\"></b><b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "CM0fYR**/I",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How will we convert if X is a power-law/ pareto distribution to Y (Gaussian) ?",
                        "By Box-Cox transform"
                    ],
                    "flags": 0,
                    "guid": "q|qry}U:!f",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How do we Box-Cox transform",
                        "<b><img src=\"-uIalImQqxZ913FHkoQuUenkiFCDJOGl-2RpH_fmb2HZwk2xea9TUL3q3AdEN1x2FPaDi5k9N5A5TBlXiLJPMGnt1HXDY8u-uvk0JDj8POeF5FMROIe3Foz_WiQBYf9y9PKL.png\"></b><div><div style=\"\">1 ) We will try to run a function called box-cox to our r.v X and get lambda (λ)</div><div style=\"\"><br></div><div style=\"\">2 ) We get the above y (Gaussian) with the formula and then our power-law distribution will get converted to Gaussian distribution.</div><div><br></div>Note - if λ = 0 then X -&gt; log normal<br></div>"
                    ],
                    "flags": 0,
                    "guid": "w,o>tizWS~",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Correlation applications",
                        "<div style=\"\">Suppose we’ve X : heights, Y: Weights then if we need to find the relation between them like if&nbsp;</div>X ↑ then Y&nbsp;↑ or X&nbsp;↓ then Y&nbsp;↓ the answer to this is some correlations like Co-variance, Pearson co-relation coefficient or Spearman co-relation coefficient"
                    ],
                    "flags": 0,
                    "guid": "J+NW:]@@4m",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Types of Co-relations",
                        "1 ) Co-variance<div><br></div><div>2 ) Pearson Corelation coefficient</div><div><br></div><div>3 ) Spearman co-relation coefficient</div>"
                    ],
                    "flags": 0,
                    "guid": "A4r0KYD&P*",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Co- Variance",
                        "<b><img src=\"uoKGVAp3mNJGw5d71jVZutCg8OkbiNNGsV9U8Yv-10z1npUko3I9QXKo-wnp50UmMYwJ0ETiT74HYkE2SUDLY_NbwITmywsxM7MRKtBFic__Ijn7QQTfRdvHM4yk3HoNlEKs.png\"></b><div><div style=\"\">Looking at the formula it can be clearly seen that variance(X) is covariance of X with itself</div><div style=\"\">By getting the covariance if&nbsp;</div><div style=\"\"><br></div><div style=\"\">Cov(X, Y) = +ve then X ↑, Y&nbsp;↑</div><div style=\"\"><br></div>Cov(X, Y) = -ve then X ↑, Y&nbsp;↓<br></div>"
                    ],
                    "flags": 0,
                    "guid": "gE<Krz{0gM",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Geomrtric Intuition of Co-variance",
                        "<div style=\"font-weight: bold;\"><img src=\"1moHhd-crJr5wLCdbPgSitAIZrHxYzoEHfZd46-Sq2eeazn-wpeyasY5LOsE87rKEe8mlJVdABBi9Eeu6G2soF1FR9gXchqaQtUqztsP8BdUKa6xJCe8CTZhwzvo7rZDnOg0.png\"></div><div style=\"\"><div style=\"\">In the 1st img ,(x<sub>i&nbsp;</sub>- μ<sub>x</sub>) * (y<sub>i&nbsp;</sub>- μ<sub>y</sub>) for every x and y is positive then Cov(X, Y) = +ve then as X&nbsp;↑ , Y&nbsp;↑</div></div><br><br><div><b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "y0,M5T;=c(",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Problem with Variance",
                        "<b><img src=\"xQiEXqfutC81KGdE0-1VnxgM172cO0vgeuHXKgFszdufLxi5iiYVN656H1oePcmfd9Uv7neZ2NiKeuxvX3r6Ip7XiJkB1Q8yKhWhhPnAWsKlmFFjBzCB05tLGds744vdt2Hd.png\"></b><div><div style=\"\">If we change from X (cm), Y(kg) to X(ft) , Y(lbs) then covariance wont be equal. Even when the dataset is the same as just it has calculated with different metric as mentioned.</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "AVu/pcx`=V",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Peaeson Correlation Coefficient calculation",
                        "Cov(X,Y)&nbsp; divided by product of std.dev&nbsp;σ<sub>x</sub>,&nbsp;σ<sub>y&nbsp;</sub>&nbsp;<div><br></div><div>PCC = Cov(X, Y) /&nbsp;<span>σ</span><sub>x </sub>*<span>&nbsp;σ</span><sub>y</sub></div>"
                    ],
                    "flags": 0,
                    "guid": "P$;VSN![i)",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How is Pearson Correlation Coefficient different then Co-Variance?",
                        "<div style=\"\">In covariance, we got the idea that if both X and Y then Cov(X, Y) is +ve but we’ve no idea how +ve/ How strongly co-related.&nbsp;<div style=\"display: inline !important;\">PCC gives a good idea about that.</div></div><br><br>"
                    ],
                    "flags": 0,
                    "guid": "j[Zr)wK4J6",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How Pearson Correlation coefficient tells us about correlation between features?",
                        "<b><img src=\"icgiey33_YvdSQO_aJ-5aDqI8-2LnZO7bSpIU6YKhprgQAlSpymtbawyI6sINl010WXKYZAL_TCiWtPDfWdW0kQN3cwGguFmc-ZZMmJkzox5kwBHcq0trMSNlkqvYvELFw6D.png\"></b><div>The PCC (ρ) is always between -1 and 1.&nbsp;</div><div><br></div><div>If&nbsp;ρ = 1 then it perfectly fits the line and X and Y are increasing and it indicates that perfect positive correlation.&nbsp;</div><div><br></div><div>If -1 then decreasing and perfect negative correlation.</div><div><br></div><div>If it’s not fitting then is between -1 and 1 as seen above then we can infer that points are correlated but not that strongly.<b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "B)A.KBZoUh",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Example of Pearson Correlation Coefficient ?",
                        "<b><img src=\"hP8cCXSa4Jq-Xw6IdemmH69VzjbmEb4OcHbhUL7KCVLYxzdbFOEP9L6mTSEMMU3juK4eLnR_rQPRG6yg7PKiNYwttf_2ghcLscK_V2okuJnmXUJd0MyAffROI-cgOfEZmvYF.png\"></b><div><div style=\"\">In the first row as line doesn't fit perfectly the ρ decreases.</div><div style=\"\"><br></div><div style=\"\">Second row: All are perfectly fitting. Note that it slope doesnt matter here.</div><div><br></div>Third row: PCC(ρ) doesn't work for Non-linear combos.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "olgH<N*pLK",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Disadvantage of Pearson Correlation",
                        "1 )If our data <b>isn't linear</b> then it'll not be 1 even if the data is monotonically increasing<span>/Non decreasing</span><div><br><div><div><b><img src=\"s0B2FIOEY1ooS1FzgC5w1G_9B4A_2R2RlIIO-q9IrzYcm6Tdi7blyISNLiMsp-ONiSwle7EfyM_B_C_OPVb_u98MWt1RwGCoxch5vHojDSNmKmHe-m6b5WO3cAW4Yb65pY4-.png\"></b><br></div></div><div>If X2&gt;X1 ; Y2&nbsp;≥ Y1 monotonically non decreasing.&nbsp;</div><div><br></div><div>If X2 &gt; X1; Y2 &gt; Y1 monotonically increasing. In the&nbsp; non decreasing image, Spearman co = 1 but Pearson co = 0.88 since not linear.<b><br></b></div><div><br></div><div>2 ) It is also sensitive to outliers</div></div><div><br></div><div><b><img src=\"tEwTqIVArI0oKJpVet7f2tcFn9SLJ5WNkUKvYAaVFC_aGLCeZmmRz0cOBU28A7n8B7llbiNq8Tji9cFN4fkjUMyFXywcZtQIUZgTaA8VwYdUzW6HQUhaLvH17_RCHzULerBk.png\"></b><br></div>"
                    ],
                    "flags": 0,
                    "guid": "I.|!/1lQE!",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Spearman Rank Correlation coefficient",
                        "<b><img src=\"PpRapYBJBc2wumvUB1HMUcYeXxMojxfHTgrHkdTa_bAwGwi0mIMJa0eR0kRXjduRsfnUrrYE0plYORwsuqJ9TT6u4KJxuMxMuxFpFaQp3EpmZgbG6GT3O0UmA7Yah7Xxdlrc.png\"></b><div><div style=\"\">In Spearman rank, we dont use X and Y instead we sort them ascendingly and give them ranks as seen above and then r =&nbsp;ρ<sub>rx, ry</sub>&nbsp; where rx and ry are ranks of X and Y.&nbsp;</div><div style=\"\"><br></div><div style=\"\">Spearman doesn't care about the figure following linear relationship like Pearson. It only cares about the ranks, so if both X and Y are ↑ then r = 1 whether <b>it’s linear or not</b>. If X ↑, Y&nbsp;↓ then r = -1</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "E]iDd[iv%)",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Advantage of Spearman Correlation",
                        "<div>1 ) Even when data isn't linear but monotonically increasing then it's Spearman Correlation is 1</div><b><img src=\"s0B2FIOEY1ooS1FzgC5w1G_9B4A_2R2RlIIO-q9IrzYcm6Tdi7blyISNLiMsp-ONiSwle7EfyM_B_C_OPVb_u98MWt1RwGCoxch5vHojDSNmKmHe-m6b5WO3cAW4Yb65pY4-.png\"></b><div><b><br></b></div><div>2 ) Less sensitive to outliers</div><div><b><img src=\"tEwTqIVArI0oKJpVet7f2tcFn9SLJ5WNkUKvYAaVFC_aGLCeZmmRz0cOBU28A7n8B7llbiNq8Tji9cFN4fkjUMyFXywcZtQIUZgTaA8VwYdUzW6HQUhaLvH17_RCHzULerBk.png\"></b><b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "B#BWzT`1Af",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Correlation V/S Causation",
                        "<b><img src=\"SmAR47I_-XkpyIzz4OkO-CuKOzETX0gnA8NbFQ8kJDiR3uxCpnCausLOuutn4VH-wvdw-3Wm67UyoHX4_3WzE581mPLFJf4BD_RlwNQOgmucrVjj4fWdGzq3UjMyKjLXmq8O.png\"></b><div><div style=\"\">Data of Chocolate consumption(X) v/s Nobel Laureates per 10M population(Y). As X&nbsp;↑ ; Y&nbsp;↑ but that doesnt imply that&nbsp; X is causing Y or vice versa.&nbsp;</div><div style=\"\"><br></div><div style=\"\">Causation is advanced statistics topics in which you learn about causal models which tells us what is causing what </div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "F$w&);-8:V",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Correlation usage example",
                        "<b><img src=\"uIoeJm9VR6rZrQ1_0zUcY1xk3613KfztwhYxT1Dsi0VgShmRhHJG3VaA2mj33WD81CVhMf5kSMCrcN9Te4XNM9B2dut-EFZpBhvXjdAA5UqbuEssZ1qvt_4-vLUW-iLhMFOB.png\"></b><div>In an e-commerce , if they find that time-spent is +vely correlated money spent as seen then they’ll try to design website to increase the time spending(Strategy).<b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "E:K-x!_{16",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Confidence Interval",
                        "<div><div style=\"\"><b><img src=\"Z9xYq4yNzBUve7Gk07a83JS4JuqWU4fvbSgnOZycktK0GKDi6wFuUUoiWGakVRs3pis6PtSkBBS2aWauCG-nq_O4VXqpQDSZQ10G3i5N9qNmr018joznFCPvlf8J6rbJjP94.png\"></b><br></div><div style=\"\"><br></div><div style=\"\">x (sample mean)&nbsp; ≈&nbsp;<span>μ</span><span>&nbsp;</span><span>( pop mean) which is giving us point estimate about how close to our population mean and as n (number of sample) increases we get close to u (as n&nbsp;</span><span>↑</span><span>, (x&nbsp;</span><span>→&nbsp;</span><span>μ</span><span>)</span></div><div style=\"\">But there’s one better way to estimate this called confidence Interval</div><br></div><b><div><b><br></b></div><img src=\"4VPm9lvzOWVys2NVQAxXd76k8WGmJnmAzGU9ntaclEGS6IqIL5OkvcDkRk9x2-IXE-Aj-YcjvsnDc3qfwTQCcn5G-vgYEw-oip1wRMncJ6rmH0bDe5JfJt1mAqYBzYO2wQb4.png\"></b><div><div style=\"\">Instead of getting avg we say that there’s 95% probability (confidence) that our (population mean)lies in this interval above . When we give statements like this we call it Confidence Interval.</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "p3G0S77.{[",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "CONFIDENCE INTERVAL GIVEN THE UNDERLYING DISTRIBUTION",
                        "<div style=\"\">If we are supposed to find the C.I in a Gaussian distribution then we can find it by saying 95% of data(confidence C) lies between (μ - 2σ,μ + 2σ) since our distribution is Gaussian and we by using 68-95-99.7 rule</div><div style=\"\"><br></div><div style=\"\">What if we need to calculate the interval of 90%?</div><b><img src=\"xFnGsh6cQqAzl_UAiLOGxssGwMp7j-F7XN-zK5p2ZIoDV3TE6v5L2um-qV49UOvL1mIGZI4kcwm4TZDl-UxpbBXQZHTftIiV_UFZJr4cUF8ZojV-M4QaemCi15_X5_0cZG0A.png\"></b><br><div><div style=\"\">Our C is 90% and the remaining 10% is distributed across ((1-C)/2) then we find the interval by finding the lower bound and upper bound x’ and x’’ respectively by looking at the N-disb table (Table for normal distribution)</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "M>LS(.#6lt",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<div style=\"\">If we are given an random variable with std.dev then how will we calculate it’s C.I with 95% confidence?</div>",
                        "<b><img src=\"DkchCr3JI3i3q6S3AhVeHUuqpVUzWFhXlVdvGYHoh4j0PmYYmEEihEX0yOHIkv5cwV8aURLP_l7LkcOuC6LW_DmZCPdEPUAVC00CYbMQ-lToFwIqNrHiOHjl3FNvXAvdW2GY.png\"></b><div><div style=\"\">We use Central Limit Theorem(if the , finite) to convert it into Gaussian and then estimate by using the std devs to estimate 95% of data.</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "c2Yq|{!|_m",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "If we are given an random variable without std.dev then how will we calculate it’s C.I with 95% confidence?",
                        "<div style=\"\">We use student’s t-disb for it. It says x(sample-mean) t(n-1) where t- t-disb and n-1 is degree of freedom .&nbsp;</div><div style=\"font-weight: bold;\"><img src=\"bvXa8TVjVtlp3lrho3Ci5NHzSkfo59L0Ty2igkUKojPC4Yxw43Y1_jbThgz6n40EaoAPoa3W0_qZ-s-qrhdikE4hNIpJVa0g5gTOFesj3pah9dMnk_U1Ax_FDen35d7ep2dQ.png\"></div><div style=\"\">As v (degree of freedom) increases the peakedness increases and with that PDF we can do all the math .&nbsp;</div><br>"
                    ],
                    "flags": 0,
                    "guid": "x15bHeyqK/",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Confidence Interval using Bootstrapping<div>Task -<b>&nbsp;Estimate 95% C.I for median of X</b></div>",
                        "1 )&nbsp;<div style=\"display: inline !important;\">We are getting sample from X and then from sample S of size 'n' we are generating random sample of size ‘m’ ≤ n . Some values can get repeated it is called sampling with repetition.</div><br><br><div>2 ) From sample S we are generating k samples (here k = 100) (bootstrap-sample) &amp; then get median of that samples</div><div><b><img src=\"RWlQqEh7A3o5PH1G6NeDBRqsBZTfl4NgyHvSGv8dyoZ5WR3GZ4QUB4IQzX95N1oV55zNEd9owWqWhNlSoP-YI7xzD0eS1lROxZz78scMqMva6PQa0B8KUtAEou6If4D6ydSh.png\"></b><br></div><div><b><br></b></div><div>3 )Now after getting medians (m<sub>1</sub>,m<sub>2</sub>....m<sub>1000</sub>) of the bootstrap samples we sort them and after that we get the value of m'<sub>25</sub> and m'<sub>975</sub> since there are 950 values between them which is 95% percent values as k = 1000 where k is number of samples generated and [m'<span style=\"font-size: 16.6667px;\"><sub>25</sub></span>,&nbsp;m'<span style=\"font-size: 16.6667px;\"><sub>975</sub></span>] is our Confidence Interval</div><div><br></div><div><div style=\"\">If we want to get C.I of std-devs, percentile , etc then do the steps just like above.This is a non-parametric technique where we dont make any assumptions about the disb</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "oe!eHw46N}",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Steps of Hypothesis Testing",
                        "<div style=\"font-weight: bold;\"><b><div style=\"display: inline !important;\"><img src=\"ZY8Ta1bf0ZfPpE_pYBfydM_no3LllgCzu1TBy4UI5PtPJ-IeltvGVHPC7i2IRGSJNLNxkOLz_UKtVMh2E3v6vRZoUqJmtJo9748D_fBip7HYguRSCjVu-jS6zWyY-qbwdwv0.png\"></div></b><br></div><div style=\"\">We want an absolute answer to the above question. We want to quantify this and there’s a method called hypothesis testing to do that. Let’s see the steps<b><img src=\"3hUELI1ib0pa06scj_HPAyu4AJb9HaItXhCwrsDwhn82TkLDTX6ZxgdLQDwaLUZBPcsvCHsevhuuwAlBaBMBVmVp_6eywJzTVdrt02HIh2IYQ8P2Kr4UZDIpN8OlrW0nt_sY.png\"></b></div><div style=\"\">1) We choose a test statistic to see if there’s a value and the obvious answer here is (μ<sub style=\"\">2</sub>-μ<sub style=\"\">1</sub>)&nbsp;</div><div style=\"\"><br></div><div style=\"\">2) Null hypothesis(Ho) : It says no difference in the means.&nbsp;</div><div style=\"\">Alternative hypothesis (H1) says that there’s difference in means&nbsp;</div><div style=\"\">So we work with proof by contradiction which means we assume our null-hypothesis is true but if it’s incorrect then it’s proved that the hypothesis is incorrect.</div><div style=\"\"><br></div><div style=\"\">3) P-value<b><img src=\"DrZ_CUoPtHGtuyvy1rzKw5um95nVgc4AFjpjN0PGwiw4KElf8A1blPJcPyuhPDnC60zR4iHHI2cT4BOHSLhrvH4ax9crSHzhmtkEjlocJmfHbike0wEPpeASAPR499FJPVae.png\"></b></div><div style=\"\">P-value: Probability of observing&nbsp;<span>(μ</span><sub>2</sub><span>-μ</span><sub>1</sub><span>)&nbsp;</span><span>&nbsp;= 10 if null hypothesis is true. We assume that Ho is true&nbsp;</span></div><div style=\"\"><span><br></span></div><div style=\"\">If p-value is high let’s say 0.9 then we accept our null hyp(Ho)</div>If p-value is low let’s say 0.05 then we reject Ho or accept H1<div><br></div><div>TL;DR -</div><div><br></div><div>1 ) We get a problem like if there is a difference in heights in students of 2 different classes</div><div><br></div><div>2)&nbsp; We choose a test statistic which is also simple&nbsp;<span>&nbsp;</span><span>(μ</span><sub>2</sub><span>-μ</span><sub>1</sub><span>) for the above example</span></div><div><span><br></span></div><div><span>3 ) We make a hypothesis or basic assumptions. We work by proof by Contradiction i.e if one hypothesis is correct then the other hypothesis is incorrect</span></div><div><span><br></span></div><div><div style=\"\">Null hypothesis(Ho) : It says no difference in the means.&nbsp;</div><div style=\"\">Alternative hypothesis (H1) says that there’s difference in means&nbsp;</div><br></div><div>4 ) We calculate p-value which is nothing but Probability of observing(test-statistic) if our null hypothesis is true</div><div><div style=\"font-weight: bold;\"><span style=\"font-weight: normal;\"><br></span></div><div style=\"font-weight: bold;\"><span style=\"font-weight: normal;\">If p-value is high let’s say 0.9 then we accept our null hyp(Ho)</span></div><div style=\"\">If p-value is low let’s say 0.05 then we reject Ho or accept H1</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "FR`C8K2_K;",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Hypothesis testing with Coin Toss intuition<div>Ex : Given a coin determine if it's biased towards Heads or not</div>",
                        "If we are saying coin is biased it means that P(Heads) &gt; 0.5 since can have only two values&nbsp; heads and tails<div><br></div><div>1 ) Experimentation :&nbsp;<div style=\"display: inline !important;\">So we design an experiment saying that flip a coins and count no. of&nbsp; heads = X . This X is a random variable called Test-statistic. We flip the coin 5 times and get heads every time. Let’s ask a simple question after this experimentation.</div></div><br><div>2 )&nbsp;<div style=\"display: inline !important;\">Chance of X = 5 given our assumption ( P(X=5 | coin is not biased towards heads) = P(obs/Ho)</div></div><div><div style=\"display: inline !important;\"><b><br></b></div></div><div><div style=\"display: inline !important;\"><b><img src=\"GMRpFGyWs6qu_2PTZ5p_-LaZPq2hE-aC2VxqNS9a6YBtUqjcg_A-qkNRiJVhzChlINlmMsXuFcukTlnCJPE1WKSCjDIZXtTbMtUuUcclHF4_MdujpoZbWT3turzmA1OF282x.png\"></b></div></div><div style=\"\">Our assumption is mostly null-hypothesis because it’s kinda obvious because normally the coins are not biased.Let’s learn how to compute it</div><div style=\"\"><br></div><div style=\"\">3 )&nbsp;<div style=\"display: inline !important;\">Probability of getting head when a coin is fair is ½. We want to know the P(X=5) given coin is fair. So it’ll be ½*½*½..½ five times because coin is flipped five times. Which is 3% so we reject our null hypothesis .Note that we are using very simple ideas for our null-hypothesis (Ho).</div></div><br><br>"
                    ],
                    "flags": 0,
                    "guid": "y!.IXT+9{`",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Necessary Steps for Hypothesis testing&nbsp;",
                        "1) Design of the expt i.e sample size. Expt should be done carefully because&nbsp; if it is wrong then our whole hypothesis testing is wrong.&nbsp;<div>Ex : If we flip our coin only 3 times and we get heads in all then&nbsp;</div><div><br></div><div>P(X = 5 | Ho - Coin not biased) = 12.5 % which means we've to accept our coin even when our coin is still biased</div><div><br></div><div>2 ) P(obs|Ho) should be easy and feasible</div><div><br></div><div>3) Design X (test statistic) properly</div>"
                    ],
                    "flags": 0,
                    "guid": "dp!]UFZORl",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Calculating P-value",
                        "<b><img src=\"tFejGCnIC2DmYPnWKviGEQVBJnFA_0porhALbbw_fCOpxQ37zAMVZbrzE6DsbLonal2CWPeZ8-JP2-caNUfHUtnkoqGZRKS8R9rXLR0OPvYOFyBbzZFvOcUKcTi9FhHYR1CF.png\"></b><div>We’ve two classes Cl1 &amp; Cl2 (heights) of size 50 and calculate (μ<sub>1&nbsp;</sub>- μ<sub>2</sub>) = Δ and then take them and randomly jumble them and calculate&nbsp;<span>(μ</span><sub>1&nbsp;</sub><span>- μ</span><sub>2</sub><span>)</span><span>&nbsp;</span><span>=&nbsp;</span><span>δ</span><span>&nbsp;for 10k times. After that we sort the&nbsp;</span><span>δ</span><span>’s and see where our&nbsp;</span><span>&nbsp;</span><span>Δ</span><span>&nbsp;is fitting&nbsp;</span></div><div><b><img src=\"FdmcngL5bySvNyfDuzXfL5sRzrv9mB9au6EiG1zLAUpkOt0V7e0DA3SqDR4DOnA4uyQsfI-iLM_LZpiH4b29QR4fsLoFlyOLeEWlxpFhxxWqxSRHUwwX26vx5CTSNTNsYyjP.png\"></b><span><br></span></div><div><div style=\"\">If there are 5% of data above let’s say&nbsp;Δ =10cm then p-value = 0.05 and since it’s very small we will reject our Ho(no diff in data) . If p-value = 0.2 then we accept our Ho as p-value &gt; 5%</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "yO=q|uB:2B",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "K-S test",
                        "<div style=\"font-weight: bold;\"><img src=\"KS_Example.png\"><br></div><div style=\"\">Let’s say our test data X1 &amp; X2 which has size ‘n’ and ‘m’ respectively . Now we want to know if they follow the same distribution ? Our null hypothesis is that our X1 and X2 follows same disb</div>D<sub>n,m</sub> is our test statistic. D<sub>n,m</sub>&nbsp;is the max-gap between X1 and X2 as shown in the diag<div><b><img src=\"9AXcYXHCH5X2RaF8GsPXrelxh-J_ZnV6EvAQpifAqFAAyPsFIi7ft4S1eQU-sjAkksA8yPT8i1OGryEs-CykKjUmwPHSvq3139IDGNeIhc-1ugEAJWJOCLv5P3COk9nEWcqL.png\"></b><br></div><div>We’ve a condition for D<sub>n,m&nbsp;</sub>mentioned above and if it satisfies that condition then our null-hypothesis is rejected.&nbsp; For n = 1000,m=5000, the whole value is 0.047 and D<sub>n,m&nbsp;</sub>&gt;0.047(since D<sub>n,m</sub>&nbsp;≈&nbsp;<span>0.2) so null-hyp i.e they follow the same disb is rejected at&nbsp;</span><span>α</span><span>/sig.level =0.05. If n=50 and&nbsp;</span></div><div><span>m = 30 then Dn,m &lt; c(</span><span>α)</span><span>√(n+m)/nm .</span><span>So we accept our null-hyp at 5% sig-value/p-value</span></div>"
                    ],
                    "flags": 0,
                    "guid": "EsW![;qA!Y",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Probability"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "1358fedb-3db5-11ea-967b-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "2iIMIBhgXGgQqmX317sP6WW6ppiPJUEkFDkb4uRNEAqEC98cqGutUXSs1v01_WqwyQEIbcFXmjAz2AYjRHObRlffusREv_oIEon-2leKlkRbCn40FX6eGP3t8l5A-M95S7Tx.png",
                "46uXJ7W18ALd9_NvyTkKZxTzyxi8t24Fk8w55qrXpD-ofjMV7w58-dtcczn7WwjR0JuR6fGt4mEIn8iIM1zvmaXpcpSVJnaYbMSIAk4TEieekIGoeK6lW-5mN2EzIJq8PPXH.png",
                "DwcHLxgFI3f1n3T6fXA4xKxFGuKwFb3sf3kt2mULHXnjVA3U8AHKfxRwU3HKSnuPvc5sTr49y0qvgurwB6uzY6tX9aipUUpoDm2I6TiORzFg1w9DnH-7n8XvPBZUxog52OHX.png",
                "PXFOtHH3DKfbqef4i0aSRq2PwVb0ZxkJyTugo9yZ-pvTKvo10IgbpJTcbXWqeepc9tScDZeXEByAyVvby6jkNY6RLbDnkCz3O7ASaP4fSQ99KF8GF72TGTqVfEVWmftd8kHN.png",
                "f2kYzs7.jpg",
                "h6KMlMAqxqYL5NLWQa5i9vkq5QoGwFuHnOahPsdgvYnVG3cfdluxp1uosYuxVj-AebpR3k5uYHfuZkylkrXhaBqMeWgKP3CFWdwzUZsCY9MCKnV3Kk-ftT_dCmzOjhDiKqid.png",
                "jrIifCESKusUJbpzmkBFvXuUNFrChW4nyKJYdlIw-xGC_LzR6NBpkjGMeTonsY40TMJnFprBAD40aMJFMjM_oOpqM5eJ437pF0uqe3Oo4vexOT6NxhGAp-kM48zUuFY6KRKv.png",
                "o2kgw.jpg",
                "paste-0e7d2f3bd937c90042f7cddb3a55cbce0d7f0ba4.jpg",
                "paste-25378426f9f15ce16f0754a73469ee7ee8eda311.jpg",
                "paste-3676982e988f6a789c0893eed2327039560fed6b.jpg",
                "paste-51dfe6e3d13ce329fd3cf8a4b27e497653e7e8a7.jpg",
                "paste-bb87904edd333f16742da11d1515816f6f849a1f.jpg",
                "paste-d1a2eb34c46f8a8105c0839330fc8bdb2a8c8f79.jpg",
                "paste-feb696715bc8981f40a647e611674a5c2b8d117f.jpg",
                "xpKmRe--cI6bycwyffSK8rWWawbFF9pwcMydWI0rBx315zBT5ThVWgaSEhQlyNgL1_Qv4MlaZ5GYAR_V-baPjiPw8QOpxUE5ku5igoJ28Xrgpef_nkf1n4klTNFbCyAMpfq3.png"
            ],
            "name": "11. Linear Algebra",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Sparse matrix",
                        "matrix that consists mostly of zeros"
                    ],
                    "flags": 0,
                    "guid": "gpJKzCbpo9",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why Linear Algebra is used in Machine Learning?",
                        "<b><img src=\"46uXJ7W18ALd9_NvyTkKZxTzyxi8t24Fk8w55qrXpD-ofjMV7w58-dtcczn7WwjR0JuR6fGt4mEIn8iIM1zvmaXpcpSVJnaYbMSIAk4TEieekIGoeK6lW-5mN2EzIJq8PPXH.png\"></b><div><div style=\"\">As we can see the line or the plane separates the data points perfectly w.r.t 2-D and 3-D but what if we have 10 dimensions or 1000 dimensions.&nbsp;</div><div style=\"\"> Linear Algebra provides us the necessary tools to operate in higher dimensions</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "MQca0_RL(V",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is a Point/ Vector",
                        "<b><img src=\"DwcHLxgFI3f1n3T6fXA4xKxFGuKwFb3sf3kt2mULHXnjVA3U8AHKfxRwU3HKSnuPvc5sTr49y0qvgurwB6uzY6tX9aipUUpoDm2I6TiORzFg1w9DnH-7n8XvPBZUxog52OHX.png\"></b><div>If we want to represent a point in n-dimensions then we'll represent it by it's co-ordinates</div><div>of that point in n-dimensions .Ex :&nbsp;<div style=\"display: inline !important;\">x = [2,3,4,1,6…..n]</div></div><br>"
                    ],
                    "flags": 0,
                    "guid": "D&|*w?FZQt",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Distance of a point from origin?",
                        "<b><img src=\"PXFOtHH3DKfbqef4i0aSRq2PwVb0ZxkJyTugo9yZ-pvTKvo10IgbpJTcbXWqeepc9tScDZeXEByAyVvby6jkNY6RLbDnkCz3O7ASaP4fSQ99KF8GF72TGTqVfEVWmftd8kHN.png\"></b>"
                    ],
                    "flags": 0,
                    "guid": "loE~9]QzJD",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<b><div>Distance between two points</div></b><br>",
                        "<b><div><img src=\"h6KMlMAqxqYL5NLWQa5i9vkq5QoGwFuHnOahPsdgvYnVG3cfdluxp1uosYuxVj-AebpR3k5uYHfuZkylkrXhaBqMeWgKP3CFWdwzUZsCY9MCKnV3Kk-ftT_dCmzOjhDiKqid.png\"></div></b>"
                    ],
                    "flags": 0,
                    "guid": "nu;a:d+KdD",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Dot Product (Formula)",
                        "<img src=\"paste-feb696715bc8981f40a647e611674a5c2b8d117f.jpg\"><b><br></b><div><br></div><div>OR</div><div><br></div><div>a.b = a<sub>1</sub>b<sub>1&nbsp;</sub>+ a<sub>2</sub>b<sub>2</sub>&nbsp;where (a<sub>1</sub>,a<span style=\"font-size: 16.6667px;\"><sub>2</sub></span>) and (b<sub>1</sub><span>,b</span><sub>2</sub><span>) are co-ordinates&nbsp; on x-axis and y-axis respectively</span></div><div><span><br></span></div><div><span>The same formula can be applied in multiple dimensional vectors as well</span></div>"
                    ],
                    "flags": 0,
                    "guid": "dE:w=wKc9a",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Dot Product Intuition",
                        "<img src=\"o2kgw.jpg\"><div>They can be decomposed into horizontal and vertical components&nbsp;a=a<sub>x</sub>i+a<sub>y</sub>j and&nbsp;b=b<sub>x</sub>i+b<sub>y</sub>j<br></div><div><img src=\"paste-bb87904edd333f16742da11d1515816f6f849a1f.jpg\">&nbsp;</div><div><img src=\"paste-25378426f9f15ce16f0754a73469ee7ee8eda311.jpg\"><br></div><div>Same concept for multiple dimensional vectors but yes for dot product calculation column of 1st vector should be equal to rows of 2nd vector</div>"
                    ],
                    "flags": 0,
                    "guid": "RdGW97m&$r",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Projection of a vector",
                        "<img src=\"paste-0e7d2f3bd937c90042f7cddb3a55cbce0d7f0ba4.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "Mi;_U4pgg@",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is a Unit Vector ? What is it's magnitude",
                        "<b><img src=\"xpKmRe--cI6bycwyffSK8rWWawbFF9pwcMydWI0rBx315zBT5ThVWgaSEhQlyNgL1_Qv4MlaZ5GYAR_V-baPjiPw8QOpxUE5ku5igoJ28Xrgpef_nkf1n4klTNFbCyAMpfq3.png\"></b>"
                    ],
                    "flags": 0,
                    "guid": "tu/NLlxO6a",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Equation of a line&nbsp;",
                        "y = mx + b where 'm' is a slope and 'b' is an intercept<div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "EAXnAy4}t?",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "General Equation of a line",
                        "Line: ax + by + c = 0<div><br></div><div>Plane:&nbsp;<div style=\"display: inline !important;\"><b>&nbsp;</b>ax + by + cz + d = 0<b>&nbsp;</b></div></div><br>"
                    ],
                    "flags": 0,
                    "guid": "I./bk[4Z]H",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Equation for N-d vectors hyperplane",
                        "<img src=\"paste-3676982e988f6a789c0893eed2327039560fed6b.jpg\"><div>Where 'w' are normals and w<sub>0</sub>&nbsp;is the same as 'c' intercept in equation of line</div>"
                    ],
                    "flags": 0,
                    "guid": "zHM0sb|UQq",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why is a.b = a<sup>T</sup>b",
                        "Since every vector is considered as a column vector taking dot product of them is not possible<div><br></div><div>Ex : If a &amp; b are both n-dimensional their shape will be&nbsp; (n, 1) since column vectors. So if we want a.b then we'll do a<sup>T</sup>b since a<sup>T</sup>&nbsp;shape is (1,n) and b shape is&nbsp;(n,1) .</div><div><br></div><div>So it satisfies the property for&nbsp; dot product that column of the 1st matrix/vector should be same as rows of 2nd vector.</div><div><br></div><div><img src=\"paste-51dfe6e3d13ce329fd3cf8a4b27e497653e7e8a7.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "M%=Gfz!=YJ",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Equation of a plane passing through origin",
                        "W<sup>T</sup>x = 0 where W is the vector of normal"
                    ],
                    "flags": 0,
                    "guid": "n}Axn[X)h>",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<div style=\"\">What is the significance of W in W<sup>T</sup>x = 0?</div><br>",
                        "<div style=\"\">We know that W<sup>T</sup>x = 0 =&gt; ||W|| .||x|| cos(theta) therefore theta is 90 which leads us to know that W is vector perpendicular to the plane passing through origin</div><br><div><b><img src=\"jrIifCESKusUJbpzmkBFvXuUNFrChW4nyKJYdlIw-xGC_LzR6NBpkjGMeTonsY40TMJnFprBAD40aMJFMjM_oOpqM5eJ437pF0uqe3Oo4vexOT6NxhGAp-kM48zUuFY6KRKv.png\"></b><br></div><div><div style=\"\">It also means that W is perpendicular to any vector in the plane which makes their dot product = 0 as well </div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "I8FXFI|F,2",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<div style=\"\">Distance of a point from a plane</div><br>",
                        "<img src=\"f2kYzs7.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "uwDLGR8HJ2",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What does distance of a point from a plane tells us",
                        "<img src=\"paste-d1a2eb34c46f8a8105c0839330fc8bdb2a8c8f79.jpg\"><div><div style=\"\"><div>The angle between W and P vector is less than 90 it is +ve because cos is +ve between 0 and 90.</div>The angle between W and P’ vector is more than 90 it is -ve because cos is -ve between 90 and 180.&nbsp;</div><div style=\"\">It can also be inferred that the distance points lying in half space of P will be positive and negative for P’. It can be used in <b>classification.</b><br></div></div>"
                    ],
                    "flags": 0,
                    "guid": "Q<]3q+Xx7O",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Point in a circle hyperplane",
                        "<b><img src=\"2iIMIBhgXGgQqmX317sP6WW6ppiPJUEkFDkb4uRNEAqEC98cqGutUXSs1v01_WqwyQEIbcFXmjAz2AYjRHObRlffusREv_oIEon-2leKlkRbCn40FX6eGP3t8l5A-M95S7Tx.png\"></b><div>This concept can also be used for classification of points . Same concept can be used for other hyper-planes such as Ellipse, Hyper Cube, Square&nbsp;</div>"
                    ],
                    "flags": 0,
                    "guid": "FHT20Kgi1a",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Linear_Algebra"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "13599b2e-3db5-11ea-a138-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "latex-add3cf280cc9296af1c57918e45aef38d2a55edf.png",
                "latex-dc468100ee39ad9ddac2c0eee0464aa3ecaba64e.png",
                "latex-ec0a564ea37d0f61614405f2e712502f8a104adf.png",
                "nlp-flow.png"
            ],
            "name": "18. NLP (Amazon Food Reviews)",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Word vectors. Why ?",
                        "Convert word into vectors.<div><br></div><br>Machines need input as numbers<br><div>We convert our text into vectors so we could use Linear<br>Algebra for sentiment analysis and various other NLP tasks.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "G([6*i}.`t",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why is it called a Bag of words ?",
                        "It is called a “<em>bag</em>” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
                    ],
                    "flags": 0,
                    "guid": "s:H{{P*x/,",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Bag of words",
                        "In this model, a text (such as a sentence or a document) is represented as the&nbsp;bag (multiset)&nbsp;of its words, disregarding grammar and even word order but keeping&nbsp;multiplicity.<div><br></div><div>In this approach, we use the tokenized words for each observation and find out the frequency of each token.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "yZ+rO!1&)~",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "NLP Flow 5 steps",
                        "<img src=\"nlp-flow.png\">"
                    ],
                    "flags": 0,
                    "guid": "d>7mEmtjZX",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "IDF ? It's basic intuition",
                        "<div>Inverse document frequency</div><div><br></div><br><div>If a word wi occurs more frequently in the corpus then it's IDF will be low.</div><div><br></div><div>And similarly if a word is rare, it's IDF will be higher.</div><div><br></div><div>So IDF is the rarity of the word in the whole corpus.</div>"
                    ],
                    "flags": 0,
                    "guid": "xVv$%hg?`?",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "TF ? It's intuition and formulae",
                        "<b>Term frequency</b>&nbsp;indicates the significance of a particular&nbsp;<b>term</b>&nbsp;within the overall document.<div><br></div><div>Let's say we have documents/reviews by name r1, r2, r3 ...... rn.</div><div>These are texts and contain words wi.</div><div><br></div><div>TF(wi,rij) [$$]\\rightarrow[/$$] Term frequency of word i in any document rj.</div><div><br></div><div><br></div><div>[$$]<span>TF(wi,rij)\\, = \\,&nbsp;</span><span>\\frac{\\#</span><span>&nbsp;</span><span>\\,of</span><span>&nbsp;</span><span>\\, times</span><span>&nbsp;</span><span>\\,</span><span>&nbsp;word</span><span>&nbsp;</span><span>\\,</span><span>&nbsp;occuring</span><span>&nbsp;</span><span>\\,</span><span>&nbsp;in</span><span>&nbsp;</span><span>\\,</span><span>&nbsp;rj</span><span>}{Total</span><span>&nbsp;</span><span>\\,</span><span>&nbsp;no.</span><span>&nbsp;</span><span>\\,</span><span>&nbsp;of</span><span>&nbsp;</span><span>\\,</span><span>&nbsp;words</span><span>&nbsp;</span><span>\\,</span><span>&nbsp;in</span><span>&nbsp;</span><span>\\,</span><span>&nbsp;rj</span><span>}[/$$]</span></div><div><br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "o^#oa,?~mp",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "IDF ? Formulae",
                        "Inverse document frequency<div><br></div><div>[$$]IDF(Wi, \\, Dc) \\, = \\, log \\, (\\frac{N}{ni})[/$$]<br></div><div><br></div><div>Where <b>N</b> is <span># of docs</span></div><div>and<b> ni</b> is&nbsp;# of docs which contain Wi</div>"
                    ],
                    "flags": 0,
                    "guid": "QR[6ecGH,Z",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "TFIDF intuition",
                        "TF = How often (% of times) the word occurs in a document (review in case of Amazon food review dataset)<div><br></div><div>IDF = Rarity of the word in the whole corpus. If the word rarely occurs in the corpus it's IDF wil be greater.</div><div><br></div><div>TF(wi, rj) * IDF(wi, Dc)</div><div><br></div><div>Using TF-IDF, for a word wj in review rj</div><div><br></div><div>the corresponding vector cell would be&nbsp;<span>TF(wi, rj) * IDF(wi, Dc)</span></div><div><span><br></span></div><div>TFIDF =&nbsp;More weightage is given to words that occur more in the document and also to the words that are rare across the corpus Dc.<span><br></span></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "lKP^nbp+De",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "TF-IDF formulae",
                        "<div>TF(wi, rj) * IDF(wi, Dc)</div><div><br></div><div>Using TF-IDF, for a word wj in review rj</div><div><br></div><div>the corresponding vector cell would be&nbsp;TF(wi, rj) * IDF(wi, Dc)</div><div><br></div><div>TFIDF =&nbsp;More weightage is given to words that occur more in the document and also to the words that are rare across the corpus Dc.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "J(_Qh{|zv)",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Limitation of TF-IDF",
                        "TF-IDF doesn't consider the semantic meaning of words.<div><br></div><div>Eg - Even though <b>tasty </b>and <b>delicious </b>mean the same, TF-IDF counts them as different dimensions.</div><div>cheap - affordable</div>"
                    ],
                    "flags": 0,
                    "guid": "HT`nc,)Qku",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "NLP"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "135a108f-3db5-11ea-b09d-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "1_CNXFGBitdDADDSPK840Vaw.png",
                "220px-CRISP-DM_Process_Diagram.png",
                "220px-Monotonicity_example1.png",
                "220px-Monotonicity_example2.png",
                "Annotation 2019-07-19 171814.png",
                "Annotation 2019-07-19 172414.png",
                "latex-1ac4b3fc5751cdabc0ad5f8247f9ff45cfdc7b6b.png",
                "latex-613de38fa0272bdb621782e63f4f595f353f0e7d.png",
                "latex-c84d7fdf5a9942375842a79f5f624bebbfbc09a6.png",
                "latex-f1681cb7b3c46f3c2ade61c1fde33b13c1eb4b54.png",
                "xset-symbols.png.pagespeed.ic.PtCtCWo4yn.png"
            ],
            "name": "Basics",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Log / Logarithms",
                        "[$$]Exponents = 3^{4} = 81[/$$]<br><div>[$$]Logarithms = log_{3}81 =4 [/$$]</div><div><br></div><div><br></div><div>Example. [$$]log_{5}125 = x[/$$]</div><div><br></div><div>[$$]5^{x} = 125[/$$]</div><div><br></div><div>What's x ?</div><div><br></div><div><br></div><div>Logarithms can be used to exponents given the number and result.</div><div>Kaafi tatti answer i know, but don't try to delve too much.</div>"
                    ],
                    "flags": 0,
                    "guid": "J9Z8a`QCpG",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Monotonic functions. it's two type",
                        "Monotonically increasing<div>A&nbsp;<strong>monotonically increasing function</strong>&nbsp;is one that increases as&nbsp;xx&nbsp;does for all real&nbsp;xx.&nbsp;<br></div><div><br></div><div><img src=\"220px-Monotonicity_example1.png\"><br></div><div><br></div><div>Monotonically decreasing</div><div>A&nbsp;<strong>monotonically decreasing function</strong>, on the other hand, is one that decreases as&nbsp;xx&nbsp;increases for all real&nbsp;xx.&nbsp;<br></div><div><br></div><div><img src=\"220px-Monotonicity_example2.png\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "gz>=+$PRj}",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Is log monotonically increasing or decreasing ?",
                        "It's a monotonically increasing function"
                    ],
                    "flags": 0,
                    "guid": "tvZ}&S&bx6",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<b>A/B testing</b><br>",
                        "<div><div>A/B tests, also known as split tests, allow you to compare 2 versions of something to learn which is more effective. Simply put, do your users like version A or version B?</div><div>The concept is similar to the scientific method. If you want to find out what happens when you change one thing, you have to create a situation where only that one thing changes.</div></div><div><br></div>A statistical way of comparing two (or more) techniques, typically an incumbent against a new rival. A/B testing aims to determine not only which technique performs better but also to understand whether the difference is statistically significant. A/B testing usually considers only two techniques using one measurement, but it can be applied to any finite number of techniques and measures."
                    ],
                    "flags": 0,
                    "guid": "c|2s5IjPbh",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Machine Learning",
                        "Andriy Burkov - Machine learning can also be defined as the process of solving a practical problem by 1) gathering a dataset, and 2) algorithmically building a statistical model based on that dataset. That statistical model is assumed to be used somehow to solve the practical problem.<div><br></div><div>OR</div><div><br></div><div>Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.&nbsp;<strong>Machine learning focuses on the development of computer programs</strong>&nbsp;that can access data and use it learn for themselves.<br></div><div><br></div><div>Src -&nbsp;https://www.expertsystem.com/machine-learning-definition/<br><br><br>Tom Mitchell 1997 - A machine is said to learn from experience&nbsp;<b>E</b>&nbsp;with respect to some class of tasks&nbsp;<b>T</b>&nbsp;and performance measure&nbsp;<b>P</b>&nbsp;if its performance at tasks in&nbsp;<b>T</b>, as measured by&nbsp;<b>P</b>, improves with experience&nbsp;<b>E</b>.&nbsp;<br></div>"
                    ],
                    "flags": 0,
                    "guid": "uhZ0&9!,V4",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Types of learning",
                        "supervised<div>semi-supervised</div><div>unsupervised</div><div>reinforcement</div>"
                    ],
                    "flags": 0,
                    "guid": "l%D~Suszr]",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Supervised Learning intuition and example",
                        "This is where we give an input and the correct output to the machine learning algorithm - think of the machine learning algorithm as a student and us as the teacher. Initially the student doesn't know much so is randomly guessing the answer, but then we tell them the correct answer, so the student goes back and learns, then we repeat. The difference with machine learning algorithms and us humans is that it needs a LOT of examples to learn from."
                    ],
                    "flags": 0,
                    "guid": "v%aN!KN^4L",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<strong>Unsupervised learning intuition and example</strong>",
                        "<strong>Unsupervised learning</strong>&nbsp;- this is when we just give some data to the model and tell it to go make sense of the structure in the data (cluster similar items together). One example is when we sort groceries - we're not told which item goes where, but we tend to put all the fruit together, and the vegetables separately, with milk and dairy in another&nbsp;<em>cluster</em>."
                    ],
                    "flags": 0,
                    "guid": "kkBb84#HWA",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<strong>Reinforcement learning intuition and example</strong>",
                        "<strong>Reinforcement learning</strong>&nbsp;- this is where we give the algorithm a reward based on whether it has done the right action or not - think about this like training a dog - every time it sits when told to, or performs a trick, we give it a biscuit or some other reward. Eventually the dog (algorithm) does what we tell it to do, because that's how it will get the reward."
                    ],
                    "flags": 0,
                    "guid": "cxX)ZShK]!",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Extensive read on methods of Machine Learning",
                        "<div>Machine Learning is a set of methods which enables the computer to take decisions or infer conclusions without us guiding it.</div><div>“So if we don’t guide the computer, how does it learn ?”</div><div>Just like a human, a computer can learn from three sources. One is Observing what others did in similar situations. The other is observing a situation and trying to come up with best possible logic on the spot to decide/conclude . The third is learning from previous mistakes/success . These three methods correspond to three branches of Machine learning, Supervised, Unsupervised and Reinforcement learning respectively. In Supervised Learning, a computer can seeing house attributes and prizes of 200 houses, come up with prize of 201th house if its attributes are known. Or, it can tell what word in a sentence is name of a city, given it is shown example sentences which may or may not contain names of cities and every occurrence of a city name is tagged in these examples.</div><div>Unsupervised is where we ask the computer to take decisions based on raw data attributes and a set of measurable quantities. Some examples would include asking a computer to come up with localities in a dataset where Lat-Long of house is given. It would use Lat Long to find distances and form localities of house. We can also ask it to come up with a shortened version of a blog post, based on word occurrence in the post. Note that no decisions made by others are given to the computer. As one can imagine, these methods might not be exactly close to human subjectivity, as unlike Supervised learning model, which try to emulate human inference, these models would make decisions based on a few Mathematical quantities we ask them to.</div><div>The third type of learning is Reinforcement Learning. This is a method in which computer starts with making random decisions, and then learns based on errors it makes and successes it encounters as it goes. A recent discovery was an algorithm which could play many different arcade games after learning the correct/wrong moves. These algorithms would start by making lot of failures in the beginning and then get better as they go.<br><br><br>Source -https://medium.com/@muktabh/so-what-is-machine-learning-eli5-d1ee8e6cb2d1</div>"
                    ],
                    "flags": 0,
                    "guid": "d?Q/@/4yIS",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Reinforcement Learning examples",
                        "Elon Musk&nbsp;creating an&nbsp;AI bot&nbsp;to beat the world’s best Dota 2 players or Google’s DeepMind AlphaGo Zero&nbsp;project. Both were built using reinforcement learning algorithms. It’s insane to think that artificial systems can not only learn how to play our games better than we can but actually teach us better ways of playing them ourselves.&nbsp;<br><br>Reinforcement learning follows a basic idea of teaching a system how to minimize punishment and maximize reward. It’s somewhat like supervised learning with the idea of penalizing the machine for getting the wrong answer. Essentially, the agent learns through trial and error in order to get an answer. The difference is that we don’t know what the right answer looks like. There’s no set input/output combination for reinforcement learning. The algorithm only knows how to achieve a goal based on how you train it and what you set as rewards and punishments.<br>"
                    ],
                    "flags": 0,
                    "guid": "iNga)G41EH",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Industries affected by Machine Learning",
                        "<div>All of them.&nbsp;</div><div>Seriously. The amount of data in the world is growing at an exponential pace and right now, it seems like machine learning is the most efficient way to parse all of it (besides potentially using quantum computers.) It’s what machine learning was built for:&nbsp;<strong>Input -&gt; Analysis -&gt; Output.&nbsp;</strong>There’s literally an entire field dedicated to the processing and extracting knowledge from data (conveniently called data science.)</div><div><br></div><div><img src=\"1_CNXFGBitdDADDSPK840Vaw.png\"><br></div><div><br></div><div>It seems plausible that in the near future&nbsp;<strong>AI will be like electricity.</strong>&nbsp;Every business in every industry will be using it within the next 10 years or they’ll fall behind real fast. It’s like how today you don’t ask corporations if they use electricity or not; they just have to.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "hAq5U%^2`2",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Really short explanation of ML and it's methods",
                        "Machine learning at its core is the taking an input, analyzing it and producing a useful conclusion or decision<div><br></div><div>Supervised learning uses labeled data and trains an algorithm to match an expected output given a certain input.</div><div><br></div><div>Unsupervised learning works with problems we don’t already know the answer to by clustering and sorting inputs based on certain patterns and trends.</div><div><br></div><div>Reinforcement learning doesn’t really aim for a traditional output per se since it works off of solely an incentive system where the goal is to minimize negative penalties and maximize positive reinforcement</div>"
                    ],
                    "flags": 0,
                    "guid": "k]/x-<NY3w",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Supervised learning explained mathematically",
                        "<img src=\"Annotation 2019-07-19 171814.png\">"
                    ],
                    "flags": 0,
                    "guid": "w.@d&efciU",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Goal of a supervised learning algorithm",
                        "<div>The goal of a<span>supervised learning algorithm</span><span>is to use the dataset to produce a&nbsp;</span><span>model&nbsp;</span><span>that takes a feature vector</span><span>x</span><span>as input and outputs information that allows deducing the label</span><span>for this feature vector. For instance, the model created using the dataset of people could&nbsp;</span><span>take as input a feature vector describing a person and output a probability that the person&nbsp;</span><span>has cancer.</span></div>"
                    ],
                    "flags": 0,
                    "guid": "n?/vt|}sk^",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Unsupervised learning exaplained mathematically",
                        "<img src=\"Annotation 2019-07-19 172414.png\"><div><br></div><div><div>In</div><div>unsupervised learning</div><div>, the dataset is a collection of</div><div>unlabeled examples</div><div>{</div><div>x</div><div>i</div><div>}</div><div>N</div><div>i</div><div>=1</div><div>.</div><div>Again,</div><div>x</div><div>is a feature vector, and the goal of an</div><div>unsupervised learning algorithm</div><div>is</div><div>to create a</div><div>model</div><div>that takes a feature vector</div><div>x</div><div>as input and either transforms it into</div><div>another vector or into a value that can be used to solve a practical problem.&nbsp;</div></div>"
                    ],
                    "flags": 0,
                    "guid": "mWxmOlj!BL",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Unsupervised learning use cases wiith regards to machine learning techniques",
                        "For example, in <b>clustering</b>, the model returns the id of the cluster for each feature vector in the dataset. In <b>dimensionality reduction</b>, the output of the model is a feature vector that has fewer features than the input x; in <b>outlier detection</b>, the output is a real number that indicates how x is dierent from a “typical” example in the dataset.<br>"
                    ],
                    "flags": 0,
                    "guid": "c5TA1OFj>E",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is Analytics ?",
                        "<div>TLDR - Use of computer skills, mathematics, statistics, descriptive techniques and predictive models.<br>Gain valuable knowledge<br>Used for action recommendation, decision making.</div><div><br></div><div>Best&nbsp; definition - \"<span>Data&nbsp;</span><b>analytics</b><span>&nbsp;initiatives can help businesses increase revenues, improve operational efficiency, optimize marketing campaigns and customer service efforts, respond more quickly to emerging market trends and gain a competitive edge over rivals -- all with the&nbsp;</span><b>ultimate goal</b><span>&nbsp;of boosting business performance.</span><span>\"</span></div><div><br></div>Data&nbsp;<b>analytics</b>&nbsp;is a multidisciplinary field. There is extensive use of computer skills, mathematics and statistics, the use of descriptive techniques and predictive models to gain valuable knowledge from data.. The insights from data are&nbsp;<b>used</b>&nbsp;to recommend action or to guide decision making rooted in business context.<br><br><b>Analytics</b>&nbsp;often involves studying past historical data to research potential trends, to analyze the effects of certain decisions or events, or to evaluate the performance of a given tool or scenario. The goal of&nbsp;<b>analytics</b>&nbsp;is to improve the business by gaining knowledge which can be used to make improvements or changes.<br><div><br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "ErSfz},FTa",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Popular analytics tools",
                        "R Programming.<div>Tableau Public</div><div>Python (Popular libraries - Numpy, pandas, matpolotlib, seaborn)</div><div>SAS</div><div>ApacheSpark<br>Google Analytics</div><div>Excel</div><div>RapidMiner</div><div>KNIME</div><div>QlikView<br>Splunk</div>"
                    ],
                    "flags": 0,
                    "guid": "jX#dws-Z=n",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Role of Data scientist",
                        "1) Selecting features, building and optimizing classifiers using machine learning techniques<div>2) Data mining using state-of-the-art methods</div><div>3) Extending company’s data with third party sources of information when needed</div><div>4) Enhancing data collection procedures to include information that is relevant for building analytic systems</div><div>5) Processing, cleansing, and verifying the integrity of data used for analysis</div><div>6) Doing ad-hoc analysis and presenting results in a clear manner</div><div>7) Creating automated anomaly detection systems and constant tracking of its performance</div>"
                    ],
                    "flags": 0,
                    "guid": "kpo!o$L]@?",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "CRISP - DM",
                        "<div>It is the most popular Data science and Analytics methodology.&nbsp;<b>Cross-industry standard process for data mining</b><span>, known as&nbsp;</span>CRISP-DM<span>,</span><span>&nbsp;is an&nbsp;open standard&nbsp;process model that describes common approaches used by&nbsp;data mining&nbsp;experts. It is the most widely-used&nbsp;analytics&nbsp;model.</span></div><div>In 2015,&nbsp;IBM&nbsp;released a new methodology called&nbsp;<i>Analytics Solutions Unified Method&nbsp;for Data Mining/Predictive Analytics</i>&nbsp;(also known as ASUM-DM) which refines and extends CRISP-DM.</div><div><br></div><img src=\"220px-CRISP-DM_Process_Diagram.png\"><div><br></div><div><div>CRISP-DM breaks the process of&nbsp;data mining&nbsp;into six major phases<span style=\"font-size: 16.6667px;\">.</span></div>Business Understanding</div><div>Data Understanding</div><div>Data Preparation</div><div>Modeling</div><div>Evaluation</div><div>Deployment</div><div><br><div>The sequence of the phases is not strict and moving back and forth between different phases as it is always required. The arrows in the process diagram indicate the most important and frequent dependencies between phases. The outer circle in the diagram symbolizes the cyclic nature of data mining itself. A data mining process continues after a solution has been deployed. The lessons learned during the process can trigger new, often more focused business questions, and subsequent data mining processes will benefit from the experiences of previous ones.</div></div><div><br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "Jaa=$%[XF%",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Analytics / Data science methodologies",
                        "CRISP-DM methodology<br>SEMMA methodology<br><br>The main difference between CRISM–DM and SEMMA is that SEMMA focuses on the modeling aspect, whereas CRISP-DM gives more importance to stages of the cycle prior to modeling such as understanding the business problem to be solved, understanding and preprocessing the data to be used as input, for example, machine learning algorithms.<br>"
                    ],
                    "flags": 0,
                    "guid": "e&O}r&d`V}",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics",
                        "optional"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Set theory symbols",
                        "<img src=\"xset-symbols.png.pagespeed.ic.PtCtCWo4yn.png\">"
                    ],
                    "flags": 0,
                    "guid": "Eqr|*f2hd:",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Training intuition",
                        "Estimating parameters is called training the algorithm"
                    ],
                    "flags": 0,
                    "guid": "CioziFP#Xv",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Testing Intuition",
                        "Evaluating a method is called testing an algorithm"
                    ],
                    "flags": 0,
                    "guid": "h>Zi]=ilN|",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why is it a bad idea to use same data for training and testing ?",
                        "We need to know how the method will work on data it wasn't trained on."
                    ],
                    "flags": 0,
                    "guid": "q%4,1ygWiR",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Which problem does Cross validation solves ?",
                        "How to figure out which block is best for training and testing.<br><br>Rather than worrying about which block to use CV uses them all, one at a tme and summarizes the results at the end."
                    ],
                    "flags": 0,
                    "guid": "F7:`Q5*}l:",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What's the hardest part in Machine Learning ? What is often misunderstood as the hardest part ?",
                        "Programming is misunderstood as the hardest part in ML which it is not.<br>Hardest part is understanding the model and see how it works in different cases.<br><br>Given a real world problem we have to find out which case it falls into and how we should change it.<br><br>For Eg- when your model is undefitting, what do you do ? (Naive bayes example)<br>You decrease the alpha&nbsp;<div><br></div><div>And when it's overfitting, we'll increase the alpha</div>"
                    ],
                    "flags": 0,
                    "guid": "s%1=5n%l$!",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What's most important to handle to make a good model<br>",
                        "1) Choosing a good algo<div>2) Fight between underfitting and overfitting</div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "tY>YX_QEj0",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is random state &amp; what is random in train_test_split ?",
                        "random_state&nbsp;as the name suggests, is used for initializing the internal random number generator, which will decide the splitting of data into train and test indices&nbsp;"
                    ],
                    "flags": 0,
                    "guid": "RjYnKR0!}K",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Fit, transform, fit_transform and predict",
                        "<p style=\"margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; clear: both;\">Fitting finds the internal parameters of a model that will be used to transform data. Transforming applies the parameters to data. You may fit a model to one set of data, and then transform it on a completely different set.</p><p style=\"margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; clear: both;\">For example, you fit a linear model to data to get a slope and intercept. Then you use those parameters to transform (i.e., map) new or existing values of&nbsp;<span style=\"padding: 1px 5px; border-style: initial; border-color: initial; border-image: initial; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit;\">x</span>&nbsp;to&nbsp;<span style=\"padding: 1px 5px; border-style: initial; border-color: initial; border-image: initial; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit;\">y</span>.</p><p style=\"margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; clear: both;\"><span style=\"padding: 1px 5px; border-style: initial; border-color: initial; border-image: initial; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit;\">fit_transform</span>&nbsp;is just doing both steps to the same data.</p><p style=\"margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; clear: both;\"><br></p><p style=\"margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding: 0px; border: 0px; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; vertical-align: baseline; clear: both;\"><font color=\"#242729\" face=\"Consolas, Menlo, Monaco, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace, sans-serif\"><span style=\"font-size: 13px; padding: 1px 5px; border-style: initial; border-color: initial; border-image: initial; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; background-color: rgb(239, 240, 241);\">predict(X)</span></font>&nbsp;applies the learned model to&nbsp;<font color=\"#242729\" face=\"Consolas, Menlo, Monaco, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace, sans-serif\"><span style=\"font-size: 13px; padding: 1px 5px; border-style: initial; border-color: initial; border-image: initial; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit; background-color: rgb(239, 240, 241);\">X</span></font>, and returns&nbsp;<span style=\"padding: 1px 5px; border-style: initial; border-color: initial; border-image: initial; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-stretch: inherit; line-height: inherit;\">y_pred</span><br></p>"
                    ],
                    "flags": 0,
                    "guid": "h97Fx6j7&&",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Basics"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Sampling with and without replacement",
                        "If we put the number back&nbsp;<b>in the</b>&nbsp;bowl, it may be selected more than once; if we put it aside, it can selected only one time. When a population element can be selected more than one time, we are&nbsp;<b>sampling</b>&nbsp;with&nbsp;<b>replacement</b>&nbsp;. When a population element can be selected only one time, we are&nbsp;<b>sampling without replacement</b>."
                    ],
                    "flags": 0,
                    "guid": "pZj<Sw;Rq~",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": []
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "135ad395-3db5-11ea-8dc5-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "132.PNG",
                "1pOtBHai4jFd-ujaNXPilRg.png",
                "CM.PNG",
                "Capture.PNG",
                "ROC curve.PNG",
                "aa.PNG",
                "accuracy drawback 1.PNG",
                "accuracy drawback.PNG",
                "accureacy drawback 2.PNG",
                "auc.PNG",
                "hm.PNG",
                "latex-2b1809b336686cdefb02eb8689472b3d8a0b1bbb.png",
                "latex-f53aa027ed4721b3d30418c2f7a3041ca435215b.png",
                "t2.PNG"
            ],
            "name": "22. Performance Measurement",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Accuracy. CM formulae",
                        "<div>Accuracy -&nbsp;[$]\\frac{\\# \\ of \\&nbsp;<span>correctly \\ classified \\&nbsp;</span><span>points}{Total \\ \\# \\ of \\ points}</span><span>[/$]</span></div><br><br><div>Accuracy - measures the percentage of the time you correctly classify samples: (true positive + true negative) / all samples.</div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "uRTv/LrM;;",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Precision",
                        "<div>Precision - measures the percentage of the predicted members that were correctly classified: true positives / (true positives + false positives)</div><div><br><br>[$] Precision = \\frac{True positives} {Total predicted positives}[/$]<br><br><br></div> or <div> Out of all the things we recalled (TP + FP)  How many did we correctly recalled (TP) </div> "
                    ],
                    "flags": 0,
                    "guid": "DMlGbCHeUe",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Recall",
                        "<div>Recall - measures the percentage of true members that were correctly classified by the algorithm: true positives / (true positives + false negative)&nbsp;</div><div> OR<br></div><div> Of all the correct events (TP + FN) how much we correctly recalled (TP) "
                    ],
                    "flags": 0,
                    "guid": "qIEw*7~i,0",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "F1 score",
                        "<div>F1 score balances precision and recall</div><div>Or<br></div><div>Harmonic mean of precision and recall i.e  2 * (P *R) / (P + R) "
                    ],
                    "flags": 0,
                    "guid": "g>fcvNX#WR",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How AUC determines performance of a classification model ?",
                        "<div>An excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. In fact it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means model has no class separation capacity whatsoever.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "es%v_Ub>o0",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Gini, formulae and purpose",
                        "<div>Gini - a scale and centered version of AUC. That means rather than having a 0.5 score for a bad model, it makes that a bad model scores 0 and a good model scores 1.</div><div><br></div><div>The&nbsp;<strong>Gini Coefficient</strong>&nbsp;is 2*AUC – 1</div><div><br></div><div>Its purpose is to normalize the AUC so that a random classifier scores 0, and a perfect classifier scores 1. The range of possible Gini coefficient scores is [-1, 1].&nbsp;</div><div><br></div><div>If you search for “Gini Coefficient” on Google, you will find a closely related concept from economics that measures wealth inequality within a country.<br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "Ky3b|.`Rze",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Log-loss",
                        "TLDR - Penalises for incorrect classifications, Minimising log-loss is equivalent to maximising accuracy<br><br>Log-loss - similar to accuracy but increases the penalty for incorrect classifications that are \"further\" away from their true class. For log-loss, lower values are better.<div><br></div><div><div>Log Loss quantifies the accuracy of a classifier by penalising false classifications. Minimising the Log Loss is basically equivalent to maximising the accuracy of the classifier, but there is a subtle twist which we’ll get to in a moment.</div><div>In order to calculate Log Loss the classifier must assign a probability to each class rather than simply yielding the most likely class.&nbsp;</div></div>"
                    ],
                    "flags": 0,
                    "guid": "dU9=dnyWq3",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Scores - 7",
                        "Accuracy<br>Precision<div>Recall</div><div>F1 Score</div><div>AUC score</div><div>Gini</div><div>Log-loss</div>"
                    ],
                    "flags": 0,
                    "guid": "AGY?!Q|p_[",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to analyse confusion matrix",
                        "In a CxC matrix,<br>Priciple elements (diagonal elements) should be large, if our model is sensible.<div><br></div><div>Off diagonal elements should be small.</div><div><br></div><div>While analysing a confusion matrix, focus on these elements first</div>"
                    ],
                    "flags": 0,
                    "guid": "J7Z9e(.-x5",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "True positives",
                        "<div>Our prediction is right (True) and our predicted value is positive i.e. 1.<br></div><div><br></div><div># of points such that yi = 1 and yi_pred is also 1.</div>"
                    ],
                    "flags": 0,
                    "guid": "el{]d.-oiU",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "True negatives",
                        "<div><div>Our prediction is right (True) and our predicted value is negative i.e. 0.<span><br></span></div><div><span><br></span></div><div><span># of points such that yi = 0 and yi_pred is also 0</span><br></div></div>"
                    ],
                    "flags": 0,
                    "guid": "z]vq0)iGQM",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "False positives",
                        "<div>Our prediction is wrong (False) and our predicted value is positive i.e. 1.<br></div><div><br></div><div># of points such that yi = 0 and yi_pred is 1</div>"
                    ],
                    "flags": 0,
                    "guid": "g%B^@7:9A)",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "False negatives",
                        "<div><div><div><span>Our prediction is wrong (False) and our predicted value is negative i.e. o.</span><br></div></div></div><div><br></div># of points such that yi = 1 and yi_pred = 0"
                    ],
                    "flags": 0,
                    "guid": "cH(V]dIF/!",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to understand TP, FP, FN, TN ?",
                        "Let's take TP as an example.<br><br>True Positive<div><br></div><div>The first term (True)&nbsp;<span>tells us whether we are correct.</span></div><div><br></div><div>The 2nd term (Positive)&nbsp;<span>tells us what is the predicted label.</span></div>"
                    ],
                    "flags": 0,
                    "guid": "Mz0P+6`^L",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Total positives in CM",
                        "Sum of actual positive column (TP + FN).<div>It is denoted by P.</div>"
                    ],
                    "flags": 0,
                    "guid": "m^O-7]FOUz",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Total negatives in Confusion Matrix",
                        "Sum of actual negative columns (TN+FP).<div>It is denoted by N.</div>"
                    ],
                    "flags": 0,
                    "guid": "vtYtSZo3`^",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "True Postitive rate (TPR). Should it be high or low  ?",
                        "TPR = TP/P<div><br></div><div>TP = True Positives</div><div>P = Total no. of positives</div><div><br></div><div>High TPR is a sign of a good model</div><div><br></div><div><img src=\"CM.PNG\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "P[L#TfaAn4",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "True Negative rate (TNR)",
                        "TNR = TN/N<div><br></div><div>TN = True Negatives</div><div>N = Total no. of negatives</div><div><br></div><div>High TNR is a sign of a good model<br></div><div><br></div><div><img src=\"CM.PNG\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "Olck!XVbR/",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "False positive rate (FPR)",
                        "FPR = FP / N<div><br></div><div><br></div><div>Low FPR is a sign of a good model<br></div><div><br></div><div><img src=\"CM.PNG\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "cUUFaR)+?E",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "False Negative Rate (FNR)",
                        "FNR = FN / P<div><br></div><div>Low FNR is a sign of a good model<br></div><div><br></div><div><img src=\"CM.PNG\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "t7!wYRy+2(",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why are CM performance metrics considered better than accuracy ?",
                        "Using Confusion matrix and it's performance metrics i.e. TPR, TNR, FPR and FNR, we can deduce a model's performance even<b> if the dataset is highly imbalanced.</b><div><b><br></b></div><div>Whereas using just accuracy we can't do the same.</div>"
                    ],
                    "flags": 0,
                    "guid": "b]iA[+|%AW",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Which CM metric (out of 4) is more important ?",
                        "This is a very <b><i><u>domain specific</u></i></b> question.<div><br></div><div>For a cancer diagnosis scenario,</div><div>we need the TPR to be <b><u>extremely</u>&nbsp;</b>high (cancer detection rate),</div><div>FNR to be 0 or as <b>extremely </b>low as possible. (Cases where patient has cancer and the system says it doesn't have cancer).</div><div><br></div><div>Now the twist here is, it's OK for the FPR to be high here, because if a patient is diagnosed with cancer he'll go through more powerful tests to make sure that he has cancer or not.</div><div><br></div><div>Because the most important thing is make sure <b>not to miss</b> a cancerous patient. So, in this case the most important case is to reduce the FNR.</div>"
                    ],
                    "flags": 0,
                    "guid": "h.RQjY`ycb",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "CM",
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Problems with accuracy",
                        "<div>TLDR - Dumb model problem, cannot use probability scores</div><div><br></div><div><br></div><div><br></div>1) Case of imbalanced data<div><br></div><div>A dumb model that classifies all pts as -ve will get 90% accuracy on a dataset with 90% negative points.</div><div><br></div><div><img src=\"accuracy drawback 1.PNG\"><br></div><div><br></div><div><b><i>Never use accuracy as a measure in imbalanced datasets.</i></b></div><div><b><i><br></i></b></div><div><b><i>2) Accuracy cannot use probability scores</i></b></div><div><b><i><br></i></b></div><div><img src=\"accureacy drawback 2.PNG\"><b><i><br></i></b></div><div><br></div><div><img src=\"accuracy drawback.PNG\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "x#cjdRl{7d",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is precision&nbsp; ?",
                        "Out of all the points the model predicted to be positive(TP+FP),<br><br> what % are actually positive ?(TP)<div><br></div><div>Precision = TP / (TP+FP)</div>"
                    ],
                    "flags": 0,
                    "guid": "g>OyyW~x52",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What's common between precision and recall ?",
                        "In Information retrieval, Precision&nbsp; and recall only care about the positive class. It's all about how well you're doing on th positive class.<div><br></div><div>Eg- from millions of documents, i want to get the 10 most relevant documents to my query. I don't care about the millions of irrelevant documents. Only the relevant ones.</div>"
                    ],
                    "flags": 0,
                    "guid": "d*g:*-~v)[",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is recall ?",
                        "Recall is another name for TPR (True Postitive rate).<div>Of all the points that are actually +ve(class 1), how many points does the model detect to be +ve(class 1)<br><div><br></div><div>TPR = TP / P</div></div>"
                    ],
                    "flags": 0,
                    "guid": "hPC=u@1Yc%",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Precision v/s recall (difficult)",
                        "<img src=\"1pOtBHai4jFd-ujaNXPilRg.png\"><div><br></div><div><br><br><br>Often, we think that precision and recall both indicate accuracy of the model. While that is somewhat true, there is a deeper, distinct meaning of each of these terms. <br><br>Precision means the percentage of your results which are relevant. On the other hand,&nbsp;recall refers to the percentage of total relevant results correctly classified by your algorithm.<br></div><div><br></div><div><br></div><div><br></div><div>If still unclear, read this amazing article -&nbsp;https://towardsdatascience.com/precision-vs-recall-386cf9f89488</div><div><br></div><div>Here's a snip from the article -&nbsp;</div><div><span><br></span></div><div><span>if Jack had let’s say ten such instances in reality, and he narrated twenty instances to finally spell out the ten correct instances, then his recall will be a 100%, but his precision will only be 50%.</span></div>"
                    ],
                    "flags": 0,
                    "guid": "kW1h1cG.zU",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "F1-score",
                        "F1-score combines precision and recall.<div><br></div><div>F1-score = 2 * (pr*re / pr+re)</div><div><br></div><div>This formulae comes from the harmonic mean.</div>"
                    ],
                    "flags": 0,
                    "guid": "p}d-n>xyk<",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "F1- score formulae",
                        "F1-score = 2 * (pr*re / pr+re)<div><br></div><div>This formulae comes from the harmonic mean.</div>"
                    ],
                    "flags": 0,
                    "guid": "Q<+M3?G/E`",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Harmonic mean",
                        "<div>Harmonic mean says that we can reqrite F1 score as&nbsp;<br><br>F1 = 2 / (1/recall + 1/pr)</div><div><br></div><div>Inverse of avg of inverse of precision and recall.</div><div><br></div><div><img src=\"hm.PNG\"></div>"
                    ],
                    "flags": 0,
                    "guid": "q=`&(v^&9>",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement",
                        "optional"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why F1 score is used as a primar measure in Kaggle contests and why verma sir prefers Pr &amp; recall over F1 score ?",
                        "Because F1-score is high when precision and recall both are high.<br><br>But F1-score is not interpretable whereas precision and recall are much more interpretable."
                    ],
                    "flags": 0,
                    "guid": "Q9>PE.5QU+",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement",
                        "optional"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "ROC curve is acronym for ? Where is it useful ?",
                        "Receiver Operating Characteristic curve<div><br></div><div>Only useful in binary classification &amp; not in multiclass classification.</div><div><br></div><div><b>Extra info</b> - This curve was designed by electrical and radio engineers during 2nd world war to predict how well their missiles were working. That's why the use of receiver seems something that an electronic engineer would use in the naming scheme.</div>"
                    ],
                    "flags": 0,
                    "guid": "AF86;2}Fkr",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Steps to make an ROC curve",
                        "<div><b><u>Steps</u></b>&nbsp;-</div><div><br></div><div>1) The prediction score in y-hat is calculated and sorted in decreasing order.</div><div><br></div><div><img src=\"ROC curve.PNG\"><br></div><div><br></div><div>2)Select all thresholds one by one and calculate their FPR and TPR.</div><div><br></div><div><img src=\"t2.PNG\"><br></div><div><br></div><div>3) Draw plot with x = TPR, y = FPR</div><div><br></div><div><img src=\"Capture.PNG\"><br></div><div><br></div><div>When we connect all the points, we get a curve. That's the ROC curve</div><div>We'll get a line like this if the model is sensible.</div>"
                    ],
                    "flags": 0,
                    "guid": "gr).Hll6*!",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "AUC ? Where is it used ?",
                        "Area under curve is the area under the ROC curve.<div><br></div><div>The model is good if AUC is high.</div><div><br></div><div>It is used only for binary classification</div>"
                    ],
                    "flags": 0,
                    "guid": "JZN{2Vv[Lh",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "3 Properties of AUC",
                        "TLDR<br>1)Impact by imbalanced data / dumb model<br>2) Ordering - rank based<br>3) Binary classification, 0.5 means a random model<br>4) High and low AUC score impact<br><br>1) AUC can be high even for a dumb/simple model. (AUC is impacted by imbalanced data)<div><br></div><div>2) AUC is not depended on y-hat scores but only ordering. So, it is rank based.</div><div><br></div><div><img src=\"aa.PNG\"><br></div><div><br></div><div>Here AUC for model 1 and model 2 will be exactly same</div><div><br></div><div>3) AUC is used a lot for binary classification. In a <b><u>random</u>&nbsp;</b>binary classification model, will look like a straight line. So, AUC for a random model will be 0.5</div><div><br></div><div><img src=\"auc.PNG\"><br></div><div><br></div><div>4) When AUC score of model is less than 0.5 just swap the model output score. Eg a model having AUC score of 0.2, juat swap the model's output from 0 to 1 and 1 to 0. The AUC score will become 0.8.</div><div><br></div><div><img src=\"132.PNG\"><br></div><div><br></div><div>There can be cases where model is terrible even with good AUC scores.</div>"
                    ],
                    "flags": 0,
                    "guid": "dFt}iZ>FaH",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "precision vs accuracy",
                        "Measurements that are close to the known value are said to be accurate, whereas measurements that are close to each other are said to be&nbsp;precise."
                    ],
                    "flags": 0,
                    "guid": "Rv&|5",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What to choose between recall and precision  ?",
                        "<br>It depends on what kind of falses you can tolerate?<br><br>Can you tolerate “false negatives”, which means telling ill people they are healthy?<br><br>Can you tolerate “false positives”, meaning you’d have to tell healthy people they are ill?<br><br>In the first case people can die. In the second one, people will be worried and will take some extra tests to learn that they are healthy. Therefore “false negatives” are not tolerable here.<br><br>Recall tells us the prediction accuracy among only actual positives. It means how correct our prediction is among ill people. That matters in that case. That is why we have to minimize false negatives which means we are trying to maximize recall. It can cost us lower accuracies, which is still sufficient. <br><br><br><br>Recall is considerable but in which cases we can go for precision then?<br><br>When “false positives” can not be tolerated, precision should be favoured. A model for spam detection serves as a great example for this.<br><br>Can you tolerate “false negatives”? Which means you mark a “spam” mail as “not spam” and person will see a spam mail in his/her inbox.<br><br>Can you tolerate “false positives”? Which means you mark a “non-spam” mail as “spam” and person won’t see this real mail in his/her inbox.<br><br>Of course, we can’t tolerate the second one. Since precision is the performance indicator about positive predictions, in such cases we try to maximize precision by decreasing number of false positives. It would also cost us a lower accuracy but it might be worthy."
                    ],
                    "flags": 0,
                    "guid": "b&g*3",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "AUC and ROC",
                        "AUC - ROC curve is a performance measurement for classification problem at various thresholds settings.<b> ROC is a probability curve and AUC represents degree or measure of separability.</b>&nbsp;It tells how much model is capable of distinguishing between classes.&nbsp;Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease."
                    ],
                    "flags": 0,
                    "guid": "F3BI|?i2N0",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Precision v/s recall gf example",
                        "https://www.quora.com/What-is-the-best-way-to-understand-the-terms-precision-and-recall/answer/Supasate-Choochaisri-1?ch=3&share=3b8ad0ff&srid=RgnG"
                    ],
                    "flags": 0,
                    "guid": ":46@P",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Performance_measurement"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "135b96e1-3db5-11ea-b5bf-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "0CPs5nJE8TQ9l28QtjR5RwMEezfjVIKQLViW9Rv38oRG0gY29RGZUKgG7LF-DiLXIJ-SvErbYZfjT30xzoEpnhgpJBeNPZC_h2PtLrnRUJ-x2bw4G2gytks1tcS58KSmoHCW.png",
                "1_wSdP7fDeQtWXli0GV2tpsJut3hZc_snWzYLL4N4QLYT-yGFH-QcAf141UmmBoTn9VcAB82n5-udT-j3nDgKvt4MtVY4jMz7l-luF5or_Gme-U0GeA3vH_v-wYw-uIeJTs5.png",
                "2kSmbrq2u_YgV1cKKKwVE11ZP2OlF5HH4fAPErSKHT-MLsaQginzkjqmuIsX7hkmXY6hP1S9EQ4ILTK8Ecd2hjXN-bzcthBrj9CaAzA_Vnrci_QwmyY1eKpeUMZWh9Ty7wwU.png",
                "4qvFbLpwJUeEw71CGCNCfK0u1yB0602En-Yvh6JA5i3UpEZs6ojYRa_4ha2vFPIYqcdojc4n1QlmxdoNK4wK7hn6VA9J90egNxitp6naQsTPhUfzUzeuh0bIyyPoiFI43oKc.png",
                "8Q1qGVmmosmCpInJBi9uN1oKYVuCZ5MUY_uy3ALoyU7nHPhnUTZnQfTIAQnnBbR7DszWSVMWWhkX4XtmioLEgBxyHbngKdV44KdlE6wVTPHOkKEvaF5P5cBWvdQoh1p4VtLq.png",
                "8y7J8XLiwGqqpywc__Hb3aud5lOPMvY1wom884sDBfZpt7Zpoad4whJCXE-K1DMriluQ-ejaCxAehv0zZ_lA4KUb2DCDcdDELpTTtIiRFq11skUd5_kouAly2Me1oTkUNSvJ.png",
                "AgBg52SZXWLHnVwe0_fZL8RWilM9skGECPzhAC5rnHid5Ij2u1NPRtWHqCd_JuNdmBo_2miql5C77e_8YPc39-sytPptjw5S2hxUsKcfoGqJlos97pPud6J9ukfB3ldY8PiY.png",
                "KOd-vtHctJA02c-jtyx6xtmkRLP60eWpzuD5524i7eQTOM990tJCJWs8L6fQlOEheDa-Fz97ufGeXSii5BPVuPLsf2Albu_HuL8gSyP_w7hs9Pkm73yFy-1NzxJdrLlCuIcS.png",
                "Lp9vNfNa9CPQkUViS___rDhAR7TgqCPyylrwyDJ_sRuG18ui5msrjHHB7S9sX0rH09jOekp7yWkZR6sZm8_XvVe8qWYcZozzSIsM1ao6UNwLpEaIIa0rSZcSdY7cKnnXdydH.png",
                "PdMUg5ZxViIvR8HCziqZWLC97G3gWzsnbpmqHoezAxiA0cQ0SSpbAVfzY44rfVrvKlm153gjuCrFXZVjLfCzMIa0P4Rfpgi0ku7rWMOJW66N-AQLi2tjF1RyLJd_Hymxqksu.png",
                "WTApg_YdgOV-eBK-n2rvi6OLVqSCPO-b6dGYMb8M2N_csM9d-w6ZPIeMJ7ZSfwHcC7DNIwsSfhUoUtVuQerEQnUhpdAdmRCV840ks0sjpIj54rC_Zk1agInJvnaUMQ4PA9iw.png",
                "g6_dctqfNF-lGoEwFNeO1xEDrjsKyc7PO77q2_cbuJDcfEZgozzPHerbXvkoaxF4nX1l_n3vXY5vYnLzjALzHAEw-QgnlyJ4dGNI7dqDXSwRVcHJzRPBh65PizJWN-hLoJgV.png",
                "kgzmQ.png",
                "lOjVKPw--jSzYxx6KeRbIyxs6q6iq1lpfk6I5HtNCU94u49r72O7FXh6PBKOQi1-IED6j_vmsLUEmGcMBKYtvzreNQ_ERiOCYZyFyfi679jZeU14EnldDlO1WZ9pi6L2hVfn.png",
                "latex-0015cb4bdbfb0d08126ea6c61c4369ead6ede22d.png",
                "latex-080420b9e2dc9c6479827c55a0b0a3992d62ec82.png",
                "latex-4f072ef39f169ff4ec0903e854448e7380543246.png",
                "latex-7286c6d2eda4151aa3c9c4b25ea64610e413c618.png",
                "latex-76c32e7a3942a8d1957259a397219654f4740bd5.png",
                "latex-7bb4277e7e5bc2254c200f32fa102bbdb1640014.png",
                "latex-970116220cd3536f1adb9229b82e1030686dba22.png",
                "latex-a35952e9ec1c7eba8f91732edadf5f667166f25e.png",
                "latex-a5d2f4e6bef98618d77e45aeb5a83f4e9e426d16.png",
                "latex-add3cf280cc9296af1c57918e45aef38d2a55edf.png",
                "latex-b746efacbbe13ecd6771ab8d475f0a935362d54f.png",
                "latex-b820cf5c1a979ed6ad7616b03685284d84318f1d.png",
                "latex-bb7e78692b7d57f609fc67f9ebd01d6324c46a57.png",
                "latex-c11ae77a6ce806d50906720dbf28471413091379.png",
                "latex-d8d5bed60492f68d897782fef28854141761c738.png",
                "latex-ecfb65dd90273aee56bd9eeb5136571debe854e1.png",
                "latex-f9d87f3778213a037720915e66147a3be61b95ea.png",
                "mEt_hLcF8nqs6mYMZ_XcPxDGXvFmqDQor801Ny1rz4O_MG026eLbg5EX7eW_mNAbmIusxC1RucBKZhXovn8rsyeOdNm07BlZgwIyMNZJ5KOMRKdxrNrAHmrYrNcB1Am_Xbvz.png",
                "paste-113cbc442856cfb72ad7388363557cad088852df.jpg",
                "paste-11a0d6246d54de73a964b1a2e753a16541889719.jpg",
                "paste-2bd91593771a4d325c5a24cb932e4ae9f8db4f24.jpg",
                "paste-4f9f988a139771a0d8b0ba8baf06e80d8b64a451.jpg",
                "paste-523ed39b69e358a52faa65288b6f297f053117d0.jpg",
                "paste-6663ec08c6faade8e1051532412f62929d6d0cdb.jpg",
                "paste-701bb3692a349da70b6f55574aaa7fab1e550621.jpg",
                "paste-71579467798eb7bfb798b32d78167ff93e56ee93.jpg",
                "paste-82c23d080fba2d12d56a6e18bd22e26d15ee45cd.jpg",
                "paste-97786296b06f6b979f87092126494d841b019664.jpg",
                "paste-a0a599615d6a06f4f7197663d2bd409d9d42c44b.jpg",
                "paste-a7ecb91c5841f04e1cc2016ef752688f0bfd1c68.jpg",
                "paste-b44bf14901cd3dffeb470dad05763eeffc485dbf.jpg",
                "paste-d371162c712fe7e477940bdba5e78620cb6d861e.jpg",
                "paste-ddefc5ff5008919466c00ff2916ea8c6f86b2c31.jpg",
                "paste-ee703d54d254f9b147dc805fedd81f0f1a0b5632.jpg",
                "paste-f6ab1d5890fbf1cee8d42d21fb1b1643f604ea9e.jpg"
            ],
            "name": "25. Logistic Regression",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Grid search",
                        "<div>Grid search is a method to perform hyper-parameter optimisation, that is, it is a method to find the best combination of hyper-parameters.<br></div><div><br></div><div>Instead of randomly selecting the values of the parameters, a better approach would be to develop an algorithm which automatically finds the best parameters for a particular model. Grid Search is one such algorithm.<br></div><div><br></div>https://stackabuse.com/cross-validation-and-grid-search-for-model-selection-in-python/"
                    ],
                    "flags": 0,
                    "guid": "NlE9E3$_Q:",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Geometric Intuition: Logistic Regression",
                        "<b><img src=\"lOjVKPw--jSzYxx6KeRbIyxs6q6iq1lpfk6I5HtNCU94u49r72O7FXh6PBKOQi1-IED6j_vmsLUEmGcMBKYtvzreNQ_ERiOCYZyFyfi679jZeU14EnldDlO1WZ9pi6L2hVfn.png\"></b><div><div style=\"\">We need to find the hyperplane which makes the data linearly separable.</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "F$Mr9ezT1I",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Equation of line/ hyperplane&nbsp;π&nbsp;",
                        "<i style=\"\">w<sup style=\"\">T</sup>x + b =&nbsp; 0 where </i>w is normal to plane , b - intercept"
                    ],
                    "flags": 0,
                    "guid": "iT1g/66Y-(",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What we are we doing for points for classification",
                        "<b><img src=\"8y7J8XLiwGqqpywc__Hb3aud5lOPMvY1wom884sDBfZpt7Zpoad4whJCXE-K1DMriluQ-ejaCxAehv0zZ_lA4KUb2DCDcdDELpTTtIiRFq11skUd5_kouAly2Me1oTkUNSvJ.png\"></b><div><div style=\"\">We are calculating distance d<sub style=\"\">i</sub> of a point x<sub style=\"\">i</sub> which is d<sub style=\"\">i</sub> = w<sup style=\"\">T</sup>x<sub style=\"\">i</sub>&nbsp;for all the points to classify them.&nbsp;</div><br></div><div>||w|| is removed since it's = 1 or it's a unit-vector</div>"
                    ],
                    "flags": 0,
                    "guid": "J3tD^}7+*@",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How is distance of a point from plane useful for classification?",
                        "<div style=\"\"><b><img src=\"0CPs5nJE8TQ9l28QtjR5RwMEezfjVIKQLViW9Rv38oRG0gY29RGZUKgG7LF-DiLXIJ-SvErbYZfjT30xzoEpnhgpJBeNPZC_h2PtLrnRUJ-x2bw4G2gytks1tcS58KSmoHCW.png\"></b><br></div><div style=\"\">Point x<sub>i</sub> lies in the +ve region and the direction of normal w is also at +ve points direction</div><div style=\"\">So , w<sup>T</sup>x<sub>i</sub>&nbsp;&gt; 0 . Hence we built a classifier saying that if w<sup>T</sup>x<sub>i</sub> &gt; 0 then y<sub>i&nbsp;</sub>= +1 but w<sup>T</sup>x<span style=\"font-size: 16.6667px;\">j</span>&nbsp;&lt; 0 then&nbsp;</div><div style=\"\">y<sub>i&nbsp;</sub>= -1. Thats why we wanted an ideal plane&nbsp;π which is decision surface.</div><br>"
                    ],
                    "flags": 0,
                    "guid": "K%4R!tcd%;",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to validate our classifier for correct classification of&nbsp; +ve points and -ve points?",
                        "<b><img src=\"WTApg_YdgOV-eBK-n2rvi6OLVqSCPO-b6dGYMb8M2N_csM9d-w6ZPIeMJ7ZSfwHcC7DNIwsSfhUoUtVuQerEQnUhpdAdmRCV840ks0sjpIj54rC_Zk1agInJvnaUMQ4PA9iw.png\"></b><div><div style=\"\"><b>Case 1 : I</b>f &nbsp;w<sup>T</sup>x<sub>i&nbsp;</sub>&gt; 0 then the classifier is saying x<sub>i</sub> is a +ve point and if y<sub>i&nbsp;</sub>= +1. So, if</div><div style=\"\">y<sub>i</sub>*w<sup>T</sup>x<sub>i</sub> &gt; 0 and y<sub>i&nbsp;</sub>= +1 then we can say that our normal w is correctly classifying the point</div><br></div><div><div style=\"\"><b>Case 2 : </b>Same as case 1 except that case 2 is for -ve point . Here also y<sub>i&nbsp;</sub>* w<sup>T</sup>x<sub>i</sub>&nbsp;&gt; 0</div><br></div><div><div style=\"\"><div style=\"\"><div>NOTE : Both the cases are of <b>w</b> correctly classifying data points and in both y<sub>i</sub>*w<sup>T</sup>x<sub>i</sub> &gt; 0</div><div>So, we want y<sub>i</sub>* w<sup>T</sup>x<sub>i</sub>&nbsp;&nbsp;greater than 0 to make sure that our classifier is working correctly</div></div><br></div></div><div style=\"\">TL;dr - If&nbsp;y<sub>i</sub>*w<sup>T</sup>x<sub>i</sub>&nbsp;&gt; 0 then our model is correctly classifying the point where y<sub>i</sub>&nbsp;is the actual class value&nbsp;</div>"
                    ],
                    "flags": 0,
                    "guid": "g$z1~>B|#7",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to know if our model is misclassifying data?",
                        "<img src=\"paste-a7ecb91c5841f04e1cc2016ef752688f0bfd1c68.jpg\"><div><b>Case 3 : </b>Here y<sub>i&nbsp;</sub>= +ve point but w<sup>T</sup>x<sub>i</sub> &lt; 0 so y<sub>i&nbsp;</sub>* w<sup>T</sup>x<sub>i</sub> &lt; 0 that means our Logistic Regression is misclassifying the data as LR : -1. Ex: Blue point in the orange points.<br></div><div><br></div><div><b><img src=\"PdMUg5ZxViIvR8HCziqZWLC97G3gWzsnbpmqHoezAxiA0cQ0SSpbAVfzY44rfVrvKlm153gjuCrFXZVjLfCzMIa0P4Rfpgi0ku7rWMOJW66N-AQLi2tjF1RyLJd_Hymxqksu.png\"></b><br></div><div><b>Case 4</b>: Same as case 3 but y = -1 . Here also&nbsp;<span>y</span><sub>i&nbsp;</sub><span>* w</span><sup>T</sup><span>x</span><sub>i</sub><span>&nbsp;</span><span>&lt; 0</span><span>&nbsp;</span><b><br></b></div><div><span><br></span></div><div><b>NOTE : &nbsp;</b>Case 3 and Case 4 both the cases are of <b>w</b> incorrectly classifying data points and in both y<sub>i</sub>*w<sup>T</sup>x<sub>i</sub> &lt; 0. So, we want y<sub>i&nbsp;</sub>* w<sup>T</sup>x<sub>i</sub> &lt; 0 points to be less than 0 to make sure that our classifier is working correctly.<span><br></span></div><div><br></div><div>TL; dr -&nbsp; If&nbsp;<span>&nbsp;</span><span>y</span><sub>i&nbsp;</sub><span>* w</span><sup>T</sup><span>x</span><sub>i</sub><span>&nbsp;</span><span>&lt; 0 where y is ground-truth then our model is misclassifying data</span></div>"
                    ],
                    "flags": 0,
                    "guid": "jHI#BYt2uv",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to know if our Classifier is Good?",
                        "<b><img src=\"g6_dctqfNF-lGoEwFNeO1xEDrjsKyc7PO77q2_cbuJDcfEZgozzPHerbXvkoaxF4nX1l_n3vXY5vYnLzjALzHAEw-QgnlyJ4dGNI7dqDXSwRVcHJzRPBh65PizJWN-hLoJgV.png\"></b><div>Our Classifier is good if we correctly classified maximum points i.e fpr maximum points y<sub>i&nbsp;</sub>* w<sup>T</sup>x<sub>i</sub> &gt; 0&nbsp;</div>"
                    ],
                    "flags": 0,
                    "guid": "bfS93`&]|o",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to get maximum correctly classified points in Logistic Regression",
                        "<b><img src=\"Lp9vNfNa9CPQkUViS___rDhAR7TgqCPyylrwyDJ_sRuG18ui5msrjHHB7S9sX0rH09jOekp7yWkZR6sZm8_XvVe8qWYcZozzSIsM1ao6UNwLpEaIIa0rSZcSdY7cKnnXdydH.png\"></b><div><div style=\"\">In this our x<sub>i</sub> and y<sub>i</sub> are fixed so the only thing that can be varied in y<sub>i</sub>*w<sup>T</sup>x<sub>i</sub> is w<sup>T</sup>.</div><div style=\"\">So, our goal is to find the optimal w =&gt; w*which gives us maximum in summation of y<sub>i</sub>w<sup>T</sup>x<sub>i</sub>&nbsp;as&nbsp; more y<sub>i</sub>* w<sup>T</sup>x<sub>i</sub>&nbsp;&gt;&nbsp;0 means more correctly classified data. Our maximum of summation gives that . So finding this <b>optimal w</b> is the crux of this algorithm called Logistic Regression</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "O{=;gAbB#",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Effect of outlier in Logistic Regression and need of classifiers",
                        "<div>We need to find the optimal w =&gt; w<sup>*</sup></div><div><br></div>If y<sub>i</sub>w<sup>T</sup>x<sub>i</sub>&nbsp;: +ve then our defined by w correctly classifies xi. So we want it to be maxed i.e for all the points we have we want correct classification<div><br></div><div><b><img src=\"2kSmbrq2u_YgV1cKKKwVE11ZP2OlF5HH4fAPErSKHT-MLsaQginzkjqmuIsX7hkmXY6hP1S9EQ4ILTK8Ecd2hjXN-bzcthBrj9CaAzA_Vnrci_QwmyY1eKpeUMZWh9Ty7wwU.png\"></b><br></div><div><div style=\"\">Case 1: Here for +ve points(blue) distance d =&gt; w<sup>T</sup>x = 1 and y = +1 since positive</div><div style=\"\">Here for -ve points(orange) distance d =&gt; w<sup>T</sup>x = -1 since opposite to direction of w and</div><div style=\"\">&nbsp;y = -1 since negative. For the outlier d = +100&nbsp; since in the direction of w and&nbsp; -ve point. Therefore y = -1.&nbsp;<img src=\"paste-b44bf14901cd3dffeb470dad05763eeffc485dbf.jpg\" style=\"background-color: rgb(255, 255, 255);\">&nbsp;= -90</div><div style=\"\"><br></div><div style=\"\"><br></div><b><img src=\"mEt_hLcF8nqs6mYMZ_XcPxDGXvFmqDQor801Ny1rz4O_MG026eLbg5EX7eW_mNAbmIusxC1RucBKZhXovn8rsyeOdNm07BlZgwIyMNZJ5KOMRKdxrNrAHmrYrNcB1Am_Xbvz.png\"></b><br></div><div>Case 2: Here the plane is separating the outlier and points. Here&nbsp;<img src=\"paste-b44bf14901cd3dffeb470dad05763eeffc485dbf.jpg\">&nbsp;= +1.<br></div><div><div style=\"\">We want&nbsp;<img src=\"paste-b44bf14901cd3dffeb470dad05763eeffc485dbf.jpg\" style=\"background-color: rgb(255, 255, 255);\">&nbsp;<span>to be maxed so our model will say Case 2 but we know intuitively that the plane in Case 2 is doing a shit job as it’s misclassifying more data. It’s happening because of our outlier . So our max&nbsp;</span><img src=\"paste-b44bf14901cd3dffeb470dad05763eeffc485dbf.jpg\" style=\"background-color: rgb(255, 255, 255);\">&nbsp;<span>&nbsp;isn’t prone to outliers. With the help of Squashing we can deal with outliers</span></div><div style=\"\"><span><br></span></div><div style=\"\"><span><br></span></div><div style=\"\"><span><br></span></div><div style=\"\"><span>Tl;dr - Logistic Regression is prone to outlier as max&nbsp;&nbsp;</span><img src=\"paste-b44bf14901cd3dffeb470dad05763eeffc485dbf.jpg\" style=\"background-color: rgb(255, 255, 255);\">&nbsp; is affected by presence of outliers . So we need Squashing to deal with Outliers</div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "J(=&qOP%!&",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Squashing",
                        "So to deal with outliers we do squashing. It means if the distance is small use it as it is but if it’s <b>large</b> make it a <b>smaller value</b>"
                    ],
                    "flags": 0,
                    "guid": "fS%.6_lO,J",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How is squashing done in Logistic Regression ?",
                        "<b><img src=\"4qvFbLpwJUeEw71CGCNCfK0u1yB0602En-Yvh6JA5i3UpEZs6ojYRa_4ha2vFPIYqcdojc4n1QlmxdoNK4wK7hn6VA9J90egNxitp6naQsTPhUfzUzeuh0bIyyPoiFI43oKc.png\"></b><div>To get rid of the outlier we need a function such that after a certain distance ,10 here it’ll taper off. That’s max f( ∑ y<sub>i</sub>w<sup>T</sup>x<sub>i&nbsp;</sub>). In the above example our outlier with distance 100 is changing our whole model so it should be tapered down . So we need to find that function 'f'<b><br></b></div><div><br></div><div><b><img src=\"1_wSdP7fDeQtWXli0GV2tpsJut3hZc_snWzYLL4N4QLYT-yGFH-QcAf141UmmBoTn9VcAB82n5-udT-j3nDgKvt4MtVY4jMz7l-luF5or_Gme-U0GeA3vH_v-wYw-uIeJTs5.png\"></b><br></div><div>One such function is sigmoid function. We can see that till a certain value it’s increasing linearly but after that it’s getting tapered down and it’s maxed value is 1 so if our x: signed distance &gt; 3 here it’s y will be 1<b><br></b></div><div><br></div><div><div style=\"\">This sigmoid function has nice probabilistic interpretability properties . For ex: If the point is at plane then w<sup>T</sup>x<sub>i</sub>= 0 therefore P(y<sub>i&nbsp;</sub>= 1) = 0.5 because we don’t know whether it’s +ve or -ve . If the distance w<sup>T</sup>x<sub>i</sub> is large say 12 here our&nbsp; P(y<sub>i&nbsp;</sub>= 1) = 1.</div><div style=\"\"><br></div><div style=\"\"><div style=\"font-weight: bold;\"><img src=\"AgBg52SZXWLHnVwe0_fZL8RWilM9skGECPzhAC5rnHid5Ij2u1NPRtWHqCd_JuNdmBo_2miql5C77e_8YPc39-sytPptjw5S2hxUsKcfoGqJlos97pPud6J9ukfB3ldY8PiY.png\"></div><div style=\"\"><br></div><div style=\"\">We wanted max sum of signed distance but it’d problem of outlier so we are using a function σ(x) which has some nice properties mentioned above. So we want max sum of transformed signed distance with .</div><div style=\"\"><br></div><div style=\"font-weight: bold;\"><img src=\"8Q1qGVmmosmCpInJBi9uN1oKYVuCZ5MUY_uy3ALoyU7nHPhnUTZnQfTIAQnnBbR7DszWSVMWWhkX4XtmioLEgBxyHbngKdV44KdlE6wVTPHOkKEvaF5P5cBWvdQoh1p4VtLq.png\"></div><div style=\"\">So our goal is to find optimal w: &nbsp;<img src=\"paste-97786296b06f6b979f87092126494d841b019664.jpg\" style=\"background-color: rgb(255, 255, 255);\"></div><br></div>Tl;dr : In order to deal with outliers we do squashing i.e&nbsp;<span>we need a function such that after a certain distance ,10 here it’ll taper off/normalizes it.</span></div><div><span><br></span></div><div>For that we apply sigmoid function to our x i.e f(x) = (1 / 1 + e<sup>-x</sup>&nbsp;).&nbsp;</div><div>So our f( y<sub>i&nbsp;</sub><span>w</span><sup>T</sup><span>x</span><sub>i&nbsp;</sub><span>) =&nbsp;</span><img src=\"paste-97786296b06f6b979f87092126494d841b019664.jpg\"></div>"
                    ],
                    "flags": 0,
                    "guid": "uRy_[[)dHH",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Monotonic function",
                        "<b><img src=\"KOd-vtHctJA02c-jtyx6xtmkRLP60eWpzuD5524i7eQTOM990tJCJWs8L6fQlOEheDa-Fz97ufGeXSii5BPVuPLsf2Albu_HuL8gSyP_w7hs9Pkm73yFy-1NzxJdrLlCuIcS.png\"></b><div>A monotonic function is a function which gets increased with x<b><br></b></div>"
                    ],
                    "flags": 0,
                    "guid": "n=6l6p;=PO",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "[$$]||w||_{1}[/$$] (1 norm of a vector)",
                        "We saw this when we learned about minkowski distance<div><br><div>[$$]||w||_{1} =&nbsp; \\displaystyle \\sum_{i=1}^d|wi|[/$$]</div><div><br></div><div>Summation of 1 to d (because w is a d dim vector), absolute value of wi</div></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "K%Qrt&O9Dc",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Derive [$$]L_{2}[/$$]&nbsp; [$$] regularization[/$$]",
                        "<div>Let's simplify our optimisation problem for Logistic regression first -</div><div><br></div><img src=\"paste-f6ab1d5890fbf1cee8d42d21fb1b1643f604ea9e.jpg\"><div><br></div><div>exp(zi) is always positive</div><div><img src=\"paste-701bb3692a349da70b6f55574aaa7fab1e550621.jpg\"><br></div><div>So each of zi and log(1+exp(-zi)) is zero, so the [$$]\\sum_{}[/$$] will also be greater than 0.</div><div><br></div><div><br></div><div><img src=\"paste-d371162c712fe7e477940bdba5e78620cb6d861e.jpg\"><br></div><div><br></div><div>Minimal value of our function is 0.</div><div><br></div><div><img src=\"paste-113cbc442856cfb72ad7388363557cad088852df.jpg\"><br></div><div><br></div><div>As zi tends to [$$]\\infty[/$$] ,exp(-zi) [$$]\\rightarrow[/$$] 0</div><div><br></div><div><img src=\"paste-71579467798eb7bfb798b32d78167ff93e56ee93.jpg\"><br></div><div><br></div><div><img src=\"paste-82c23d080fba2d12d56a6e18bd22e26d15ee45cd.jpg\"><br></div><div>because log(1) [$$]\\rightarrow[/$$] 0</div><div><br></div><div>So we can modify w in such a way that zi [$$]\\rightarrow + \\infty[/$$]</div><div><br></div><div><img src=\"paste-4f9f988a139771a0d8b0ba8baf06e80d8b64a451.jpg\"><br></div><div><br></div><div>To pick best W</div><div><br></div><div><img src=\"paste-6663ec08c6faade8e1051532412f62929d6d0cdb.jpg\"><br></div><div><br></div><div>But if all points are correctly classified in the training data, even outliers then it's overfitting.<br><br>And if [$$]z \\rightarrow + \\infty[/$$] then, it'll either be a very large positive/negative value.</div><div><br></div><div>To get rid of these problems we use L2 regularization</div><div><br></div><div><img src=\"paste-ee703d54d254f9b147dc805fedd81f0f1a0b5632.jpg\"><br></div><div><br></div><div>Tug of war between the optimization function and l2 regularizer, they try to minimise Zi going to [$$]+\\infty[/$$]</div><div><br></div><div>Here [$$]\\lambda[/$$] is the hyperparameter</div>"
                    ],
                    "flags": 0,
                    "guid": "b<k7c}n)Pj",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "[$$]\\lambda[/$$]&nbsp; [$$]W^TW[/$$]",
                        "L2 regularization [$$](&nbsp; \\lambda ||W||_{2}^2&nbsp; )[/$$]<div><br></div><div>[$$]\\lambda[/$$]&nbsp; [$$]W^TW = [/$$] [$$]\\lambda[/$$] [$$]\\textstyle \\sum_{i=1}^d Wj^2[/$$]</div><div><br></div><div><img src=\"paste-a0a599615d6a06f4f7197663d2bd409d9d42c44b.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "yFm!q8of(u",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Logistic loss",
                        "[$$]\\textstyle \\sum_{i=1}^n log(1+exp(-yi w^T xi))[/$$]"
                    ],
                    "flags": 0,
                    "guid": "I+uoV$(W:y",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Optimization problem Logistic resgression",
                        "[$$]w^* = argmin_{w}&nbsp; \\textstyle \\sum_{i=1}^n log(1+exp(-yi w^T xi))[/$$]"
                    ],
                    "flags": 0,
                    "guid": "x~T0zC*zx[",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why to use L1 regularization",
                        "<img src=\"paste-2bd91593771a4d325c5a24cb932e4ae9f8db4f24.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "gKSeRgzn.5",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "elastic-net",
                        "<img src=\"paste-11a0d6246d54de73a964b1a2e753a16541889719.jpg\"><div><br></div><div>Combination of L1 and L2 regularization, best of both worlds</div>"
                    ],
                    "flags": 0,
                    "guid": "M}.eKcBrlr",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Probailistic interpretation v/s Geometric interpretation of Logistic regression",
                        "<img src=\"paste-ddefc5ff5008919466c00ff2916ea8c6f86b2c31.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "Qy5GH:qy.{",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Zero - one loss, Hinge loss, Logistic loss",
                        "<img src=\"kgzmQ.png\">"
                    ],
                    "flags": 0,
                    "guid": "KO92R#NTm3",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "loss-minimizaton interpretation",
                        "<img src=\"paste-523ed39b69e358a52faa65288b6f297f053117d0.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "elu4@+gJ+p",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "135c0c1e-3db5-11ea-973a-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "Capture (1).PNG",
                "latex-94eb392738f8fdf65233dbbbac09bd87a44a9413.png",
                "latex-e0e97bd22503ad2b5e46c6cd5e56a0813c81abf0.png",
                "paste-044f5dd56b907ed469637f04abf1ae8b432a8380.jpg",
                "paste-070c6e764760c0aa17d6d4794aac69fa18883f7c.jpg",
                "paste-09aa792d033869e8301c11f7eec955baa25ea915.jpg",
                "paste-1af7fc700e4d6f3eeb95448e7a3f6f8d4c084068.jpg",
                "paste-2a09c52f884fd65639a91a614e974824646a1028.jpg",
                "paste-2c2ab61ab88a5dd660c1629b80416f1146069639.jpg",
                "paste-32ca06d1259a5f754085ffc04414d8a51cb10a96.jpg",
                "paste-4279cf35c29d7582f6acde4a8b7fbaa8219a3666.jpg",
                "paste-49ae068948c716041aa416c0e38001c5e4ed3405.jpg",
                "paste-4e55ed8c4e4384a9ba58817107f9802cb0c0a684.jpg",
                "paste-51c6d866f78ba242c1a7c064e8b46e90ee8736e7.jpg",
                "paste-675163d45fcb340a48d1fede40dfd339ca584d11.jpg",
                "paste-6f5de743b7bc43b1f63b202c73f1dfde03ebffcb.jpg",
                "paste-7c6da073be22e1cadb8564ce9112f7b7f51a1643.jpg",
                "paste-99171bb176a357a84509cdf5095535ba0190918f.jpg",
                "paste-a2e1bf31141681b22e7bbf27256596f57267ba9e.jpg",
                "paste-a8875ab58377b3150f79ce928c6e867e607fb6f8.jpg",
                "paste-aa8ad2fe0b9161631d068b59c593e1b70d310fec.jpg",
                "paste-c3506ecedda1d4f8eef6721f25c29d835d59d132.jpg",
                "paste-cd0708cad72dc9bb12799670fcb1ec9903f988d3.jpg",
                "paste-db106ba25ee2d757f2482733011da17233140d09.jpg",
                "paste-e2867c72911513de20737b0afff8f4c95d5a9c57.jpg",
                "paste-f01bf2665bf091ff714cd054fcc1c24a3bdfc2b5.jpg",
                "paste-f4f11356b1724cade5270eb8167a48d97ccf84e7.jpg",
                "paste-fb52795408d50e73cfc5a252779785964b5d37d0.jpg",
                "paste-ff9f67cd750198081c505b3d431cf990e2cd8dde.jpg"
            ],
            "name": "24. Naive Bayes",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Mutually exclusive",
                        "<b>mutually exclusive</b>. phrase. If&nbsp;<b>two things are mutually exclusive</b>, they are separate and very different from each other, so that it is impossible for them to exist or happen together.<div><br></div><div><img src=\"paste-db106ba25ee2d757f2482733011da17233140d09.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "m!zzg~E/!&",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Ways of writing A intersection B",
                        "<img src=\"Capture (1).PNG\">"
                    ],
                    "flags": 0,
                    "guid": "z6DPywWIF>",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Naive bayes",
                        "Naive Bayes is a classification algorithm based on the basics of Probability. It is based on Bayes theorem<br>"
                    ],
                    "flags": 0,
                    "guid": "n@I=9*cluS",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Independent Events",
                        "A, B are said to be independent if a probability of event A is not dependent on event B<br>Take the example of 2 dices above thrown.Their outcomes are independent obviously<div><br></div><div><img src=\"paste-f4f11356b1724cade5270eb8167a48d97ccf84e7.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "rt)UlWj}0X",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Bayes theorem derivation",
                        "<div><img src=\"paste-4e55ed8c4e4384a9ba58817107f9802cb0c0a684.jpg\" style=\"background-color: rgb(255, 255, 255);\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "J<p2,;.ihy",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Conditional Probability",
                        "<div>[$$]<span>P(A|B)</span><span>=</span><span>\\frac{</span><span>P(A \\cap B)</span><span>}</span><span>{P(B)}</span><span>[/$$]</span></div>Probability of an event A given B is : Probability of A intersection B by probability of B, given B [$$]\\neq[/$$]&nbsp;&nbsp;0<div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "h1b/ptct;(",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Mathematical definition of independent events",
                        "P(A|B) = P(A)<div>P(B|A) = P(B)</div>"
                    ],
                    "flags": 0,
                    "guid": "Fe)sp^iZjm",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Bayes Theorem",
                        "<img src=\"paste-99171bb176a357a84509cdf5095535ba0190918f.jpg\"><div><br></div><div><img src=\"paste-044f5dd56b907ed469637f04abf1ae8b432a8380.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "w|(u@>LQ}J",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is joint probabililty model ?",
                        "Naive Bayes is also known as a joint probability model"
                    ],
                    "flags": 0,
                    "guid": "Ia3l+WH)(=",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Posterior, Likelihood, Prior and Evidence",
                        "<img src=\"paste-51c6d866f78ba242c1a7c064e8b46e90ee8736e7.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "H#Lqt8TDo8",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What are likelihoods ?",
                        "They are probability of a feature given a class label."
                    ],
                    "flags": 0,
                    "guid": "D.]6-5=ag)",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Time complexity of Naive Bayes model",
                        "Time Complexity : O(ndc) where n- Number of inputs, d- dimensions ,C - No. of classes"
                    ],
                    "flags": 0,
                    "guid": "vf*oGqdw6m",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Space complexity of Naive bayes&nbsp;",
                        "Space Complexity : O(d * c) because for every d- dimensional feature we need to store P(f | c)"
                    ],
                    "flags": 0,
                    "guid": "zO2F4s%yUP",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Space complexity - Naive bayes v/s KNN. How ?",
                        "It’s much better than k-NN which has Space Complexity O(nd)."
                    ],
                    "flags": 0,
                    "guid": "I&8lrtq_K4",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Naive Bayes example (text)",
                        "Text classification - spam filter"
                    ],
                    "flags": 0,
                    "guid": "M?@z@5(DB0",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "In the testing part if there occurs a word w’ which is not present in set of words of Training data then what we should be doing?",
                        "We'll use Laplace smoothing, which is also called as additive smoothing"
                    ],
                    "flags": 0,
                    "guid": "p@wX$oxT^@",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<div><div><div><div><div><div>How is log beneficial to the calculation part in Naive Bayes</div></div></div></div></div></div><br>",
                        "<div>If we do their log then the calculations will get easier as log is a monotonic function<br></div><div><br></div>calculations are getting easier because multiplication get<br>converted to addition. It also converts exponentiation to multiplication as log( a )= b*log a"
                    ],
                    "flags": 0,
                    "guid": "P:IMXW{Rn%",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Bias Variance tradeoff in Naive Bayes",
                        "In Naive Bayes, α in Laplace smoothing determines high bias or high variance.<br><br>We need to find the right α . So, to find it we do Cross Validation as α is a<br>hyper-parameter and hyper-parameters are decided using Cross Validation(CV)<br>"
                    ],
                    "flags": 0,
                    "guid": "BTO;_GT|sD",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Feature importance in Naive Bayes",
                        "We decide the most important features by Sorting w based on their probability scores.<br><br>+ve class:- find words (wi) with highest value of P(wi|y=1)<div><br></div><div>-ve class :- Find words (wi) with highest value of P(wi|y=0)<br><br><img src=\"paste-09aa792d033869e8301c11f7eec955baa25ea915.jpg\"><br></div><div><br></div><div>Unlike in KNN, feature importance in Naive bayes can be obtained directly from the model.<br>Because when we're training our models we're already computing these likelihood ratios, which we can are using to obtain feature importance which wasn't possible in KNN.&nbsp;<br><br>(In KNN we used forward feature selection)</div><br><img src=\"paste-7c6da073be22e1cadb8564ce9112f7b7f51a1643.jpg\"><br>"
                    ],
                    "flags": 0,
                    "guid": "BEQ>Ov]>(K",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Can we directly get most important features from model itself ?",
                        "Probabilities are calculated at the learning stage itself.&nbsp;In k-NN, we used Forward Feature selection to select the most important features."
                    ],
                    "flags": 0,
                    "guid": "np}qmN!4HV",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How can we interpret our Naive bayes model ?&nbsp;",
                        "Suppose if we’ve a positive review. We can conclude it’s y = 1 because xq contains words w3, w5, w8 like terrific, great which has high probabity score and also conclude y =/ 0 since the probabilty scores of negative words w7, w9 like terrible, not-good are small.<br><br><img src=\"paste-1af7fc700e4d6f3eeb95448e7a3f6f8d4c084068.jpg\"><br>"
                    ],
                    "flags": 0,
                    "guid": "y^q<f[A$-}",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Outliers in Naive Bayes",
                        "If a word w’ occurs at test time which wasn’t seen in Train Data then we do Laplace<br>Smoothing to deal with it . It’s an outlier<br><br>If a word w occurs very few times in our Train data then it’s an outlier. How do we deal with<br>it?<br><div><br><img src=\"paste-f01bf2665bf091ff714cd054fcc1c24a3bdfc2b5.jpg\"><br><br><br></div><div>1 ) We can apply a condition/ threshold to just ignore that word as seen above<br>2) We can use Laplace Smoothing in our Train Data as well with the right α<br></div>"
                    ],
                    "flags": 0,
                    "guid": "n1;X^zE8M$",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to handle missing values in Naive bayes",
                        "<div>For text data there is no case of missing values</div><div><br></div>For Categorical features if it doesn’t belong to features {a1,a2,a3} e.g NaN then we also<br>consider NaN as a category.&nbsp;<div><br></div><div>For Numerical data - imputation (mean, median)</div>"
                    ],
                    "flags": 0,
                    "guid": "s?iTYy:x8j",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "In Naive bayes, how do we handle new words in Test set data ?",
                        "Laplace smoothing / Additive smoothing<div><br></div><div>If we drop / ignore the new word, it's just like saying that it's probability is 1. Which is Wrong<br><br><img src=\"paste-a8875ab58377b3150f79ce928c6e867e607fb6f8.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "zg?=p`pplA",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is Additive smoothing",
                        "Additive smoothing also called as Laplace smoothing.<br>It is applied for situation where new words are encountered in Test set which were not part of Train set.<br>But it is <b>also </b>applied to old words also.<br><br><br>1) The problem we face is we can't set it to 0 or 1.<br>Setting it to 0 will decrease the probability of the whole equation to 0.<div><img src=\"paste-070c6e764760c0aa17d6d4794aac69fa18883f7c.jpg\"><br>And setting it as 1 (i.e. dropping it or ignoring it) is also logically incorrect.</div><div><img src=\"paste-32ca06d1259a5f754085ffc04414d8a51cb10a96.jpg\"><br><div><br></div><div>Using laplace smoothing we add an alpha to the numerator and an alpha*k to the denominator.<br>Where k is the no. of distinct values that the word (w') can take.</div></div><div><br></div><div><img src=\"paste-6f5de743b7bc43b1f63b202c73f1dfde03ebffcb.jpg\"><br></div><div><br></div><div>Case 1 - Setting alpha as 1</div><div><img src=\"paste-49ae068948c716041aa416c0e38001c5e4ed3405.jpg\"><br></div><div>Now the situation of that 1/0 problem is resolved.&nbsp;</div><div><br></div><div>Now you'd ask why is case 1 the right way ?<br>We can set alpha to epsilon (a really small no. like 0.0001) so that it's probability is really small<br><br></div><div>Formulae for laplace smoothing -&nbsp;</div><div><img src=\"paste-aa8ad2fe0b9161631d068b59c593e1b70d310fec.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "IH?dDIfp~y",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What happens when alpha in Naive Bayes is large/very large ?",
                        "Then the probability of the new word will be very close to 0.5. Which means overfitting"
                    ],
                    "flags": 0,
                    "guid": "q5W]m6/?`b",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why is laplace smoothing also called as additive smoothing",
                        "Because we're adding the alpha to both the numerator and the denominator.<br><br>Because of which as alpha increases, we're moving the likelihood probabilities to Uniform distribution.<br><br><img src=\"paste-ff9f67cd750198081c505b3d431cf990e2cd8dde.jpg\"><br>"
                    ],
                    "flags": 0,
                    "guid": "I-mh;:;,?U",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "On which points is laplace smoothing is applied ?",
                        "It is applied on both points present in training data as well the new points obtained in test data."
                    ],
                    "flags": 0,
                    "guid": "nb&*|Iu4>4",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is usually selected as alpha in laplace smoothing and why ?",
                        "1 is selected as alpha. It is also called as add-one smoothing"
                    ],
                    "flags": 0,
                    "guid": "lV[f7b(TkD",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is Naive in Naive bayes",
                        "It is the assumption that features are conditionally independent of each other"
                    ],
                    "flags": 0,
                    "guid": "w`B:f>n.aD",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Maximum of a posterior or MAP decision rule",
                        ""
                    ],
                    "flags": 0,
                    "guid": "MT5E/]q&t_",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Which model is used as baseline benchmark for text based classification",
                        "Naive Bayes"
                    ],
                    "flags": 0,
                    "guid": "w(e?d{=@$;",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Numerical stability issue",
                        "When we multiply hundreds of probabilities (which are obviously between 0 and 1), it'll result in a very small number.<br><br>Which can lead to numerical underflow"
                    ],
                    "flags": 0,
                    "guid": "fBF3EuUm9W",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why to use log probabilities",
                        "For numerical stability.<br><br>When multiplying 100's probabilities (which are obviously between 0 and 1) the result can be a really small number. We use log probabilities to get them on the logarithm scale which is much easier to handle.<br><br>Log converts multiplications to addition<br>and exponents to multiplication<div><br></div><div>If we do their log then the calculations will get easier as log is a monotonic function<br><div><br></div></div>"
                    ],
                    "flags": 0,
                    "guid": "z}vRCNEFt?",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "High bias - One word",
                        "Underfitting"
                    ],
                    "flags": 0,
                    "guid": "[J~LfD_V,",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "High variance - One word",
                        "Overfitting<br>As data slightly changes, model changes drastically"
                    ],
                    "flags": 0,
                    "guid": "iiS2GJ9+*+",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "When does underfitting happens in Naive bayes and how is it related to KNN underfitting situation ?",
                        "In KNN, when k (hyperparameter) is large, the majority class overwhelms the minority class and most/all points are classified as majority class.<br><br>Similarly in Naive bayes, when alpha is large, the probability for all classes will be almost equal.&nbsp;<br><br>Now the final score will be dependent on the P(n^ith) value as show below<br><img src=\"paste-2c2ab61ab88a5dd660c1629b80416f1146069639.jpg\"><br><div><br></div><div>This will result in the majority class getting the highest score, and a similar situation to KNN. Hence Underfitting and tatti model</div>"
                    ],
                    "flags": 0,
                    "guid": "l@ScxkZ0a)",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Feature importance in Naive bayes",
                        ""
                    ],
                    "flags": 0,
                    "guid": "sAH&Z=8UBN",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Interpretability in Naive bayes",
                        "Let's say we've got a query point / query text Xq.<br><br>We can declare Yq to be 1 because Xq contains words which have very high probability of being 1.<br>Eg - P(W10 | y=1) is very high<br>And we also have words which have very low probability of being 0.<div><br></div><div><img src=\"paste-c3506ecedda1d4f8eef6721f25c29d835d59d132.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "g)R)J;2KJZ",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Imbalanced data problem in Naive bayes. Explain",
                        "<div><img src=\"paste-cd0708cad72dc9bb12799670fcb1ec9903f988d3.jpg\"><br></div><div><br></div><div>&nbsp;Let's consider out of total n points, 90% of points belong to +ve class n1 and rest 10% belong to -ve class n2.<br><br>Now let's write down the Naive bayes formulaes:-<br><br></div><div><img src=\"paste-675163d45fcb340a48d1fede40dfd339ca584d11.jpg\"><br></div><div><br></div><div><br></div><div><br></div><div>Now in this formulae, let's consider the likelihoods are exactly the same:-<img src=\"paste-fb52795408d50e73cfc5a252779785964b5d37d0.jpg\"></div><div><br></div><div>Now in this situation the +ve class will have an undue advantage as it is 0.9 i.e. 9 times greater than the -ve class.<br><br>So, concluding when you have an imbalanced dataset, because of the class priors, the dominating class has an advantage. This situation should be resolved or avoided.</div>"
                    ],
                    "flags": 0,
                    "guid": "o>1fGgFoQB",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Solutions to imbalanced dataset problem in Naive bayes",
                        "1) Using upsampling or downsampling<br>So, now n1=n2 and p(y=1) = p(y=0) = 1/2<br>(We're making the class priors equal to 1/2)<div><br>2) Dropping p(y=1) &amp; p(y=0)<div>p(y=1) = p(y=0) = 1<br>Here we're making the class priors equal to 1.&nbsp;<br><br>Both these methods are equivalent.<br><br>3) Modifying Naive Bayes for class imbalance, but these are not often used. And are not very popular</div></div><div><br></div><div>Conclusion final - Naive bayes is effected by imbalanced data.&nbsp;</div><div><br></div><div>It impacts the priors, which can be solved by upsampling, downsampling or dropping. It also impacts the likelihood ratio when applying the laplace smoothing using same alpha. Workaround for this is upsampling, downsampling or using different alphas (which is a hack and should be avoided as it is not scientific).<br></div>"
                    ],
                    "flags": 0,
                    "guid": "FXu:}B5o^K",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Another issue with class imbalance with regards to Naive Bayes",
                        "<div><span>The minority class tends to have smaller numerator.</span><br></div><div><br>Now, when we apply laplace smoothing using the same alpha for +ve and -ve classes. The alpha will impact the minority class more than the majority class<br></div><div><br></div><div><img src=\"paste-4279cf35c29d7582f6acde4a8b7fbaa8219a3666.jpg\"><br></div><div><br></div><div>Example:-<br><br><img src=\"paste-a2e1bf31141681b22e7bbf27256596f57267ba9e.jpg\"><br></div><div><br></div><div>Using the same alpha 10, 2% became 10% for the minority class but it increased only to 3.04% for the majority class.<br><br>This can be solved by using a hack - Using different&nbsp; alphas for different classes</div>"
                    ],
                    "flags": 0,
                    "guid": "FlBz~_O/(7",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Handling outliers in Naive bayes",
                        "Outliers in Test data can be taken care by laplace smoothing. It also takes care of multiplication by 0 error.<br>It's an outlier because we haven't seen such a point in the whole training data.<br><br>Outlier in Training data - Word that occurs very few times in both +ve and -ve class (Or all the classes).<br>It can be taken care of by&nbsp;<div><br></div><div>1)Hack :- Deciding a threshold (let's say 10) and removing the word from the training set if it occurs less than that.</div><div>2)Laplace smoothing with reasonable value of alpha. so that it won't impact probabilities with small numerators (which the outliers will obviously have)</div>"
                    ],
                    "flags": 0,
                    "guid": "M8.tk/q)s5",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Laplace smoothing formulae",
                        ""
                    ],
                    "flags": 0,
                    "guid": "vh)oLAU83g",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Assumption of Naive bayes",
                        "Conditional independence, all features are independent of each other"
                    ],
                    "flags": 0,
                    "guid": "H}(1.tv^QB",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Gaussian Naive bayes",
                        "<strong>Gaussian Inputs</strong>: If the input variables are real-valued, a Gaussian distribution is assumed. In which case the algorithm will perform better if the univariate&nbsp;distributions of your data are Gaussian or near-Gaussian.&nbsp;"
                    ],
                    "flags": 0,
                    "guid": "qwF.!Ll@R[",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Can Naive bayes do Multiclass classification ?",
                        "Yes, inherently built for that purpose"
                    ],
                    "flags": 0,
                    "guid": "P00bKIU$VR",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is Distance / Similarity matrix ? Which one can handle&nbsp;it instead of points ,Naive Bayes or KNN ?",
                        "Data point as a feature.<div>Naive bayes can't handle it.&nbsp; Naive bayes is not a distance based method. It is a probabilistic method where actual feature values are need to compute the likelihood of conditional probability</div><div><br></div><div><img src=\"paste-e2867c72911513de20737b0afff8f4c95d5a9c57.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "FWNVOSRR7&",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Can Naive bayes handle high dimensional data ?",
                        "Obviously, Naive bayes is used extensively in text classification with Binary BOW or count BOW. So, text classification in itself is a high dimensional problem to start with.<br><br>Just make sure to use log probability. So that numerical underflow doesn't happen.<div><br></div><div><img src=\"paste-2a09c52f884fd65639a91a614e974824646a1028.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "BUsc3qP|0c",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Best case and worst scenarios for Naive bayes",
                        "1) Conditional independence of features<br>When this assumption is true Naive bayes performs really well. When this assumption is false Naive bayes performance deteriorates.<br><br>But from a practical perspective it still works reasonably well even when some features are dependent.<div><br></div><div>2) Text classification - email spam, review polarity. Works really well with high dim data. So Naive bayes works really well as a baseline model for text classification</div><div><br></div><div>3)Categorical features - Works really well with categorical features but not well with real valued features. Because in internet domain data is usually not gaussian distributed.</div><div><br></div><div>4) It is highly interpretable</div><div>5) Low runtime complexity</div><div>6) Training time complexity is low - O(n)</div><div>7) Run time space complexity is also low as we're only storing class priors and likelihood priors, unlike KNN where whole data is needed to store in runtime.</div><div>8) Extremely simple to understand and implement from scratch. It's very intuitive as it's just like counting.</div><div><br></div><div>Disadvantage - Easy to overfit if we don't do laplace smoothing, and choosing right alpha using CV. Doesn't works well with real valued features.<br><br>When features are conditionally dependent</div>"
                    ],
                    "flags": 0,
                    "guid": "O}A}8;jY-}",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to handle imbalance data in Naive bayes",
                        "You can manually pass class priors in sklearn as an array"
                    ],
                    "flags": 0,
                    "guid": "mt![uPbhTH",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Naive_Bayes"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "135d1d89-3db5-11ea-9017-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "Untitled (1).png",
                "latex-253735f9c606a1525d0395991d089433e88c8897.png",
                "latex-a51c96a6c86210b27093d66166ce3cd906297348.png",
                "paste-1f7cf1cb5e0be23475491528ee8ac940384a278d.jpg",
                "paste-238414302580800aa7c6f4518b31560a37a88fc7.jpg",
                "paste-2796d7f74e5bd7d397c48ef2283f1fbfd8fdcccc.jpg",
                "paste-2fc77608806f1c62e6cf8415ed5369aa516692c4.jpg",
                "paste-35750216b232cbac92c304dd37827518ad221c8a.jpg",
                "paste-365358c7577b4ecd4a296ed7e2dbf8139fada4eb.jpg",
                "paste-3d09561f3ac97f11bf849d9e9e38ef9a89b29a2e.jpg",
                "paste-559bdeba33bdcb7897599dedce3a53cdd6ed2b79.jpg",
                "paste-6de6f2cd9791aa3a34c150883a40a6366589e003.jpg",
                "paste-887d1a12a1c9e371d155766832e970021b1a1b89.jpg",
                "paste-8bfdb715da944efeaa5f94cefeb03bed25275bb5.jpg",
                "paste-9c949cd198b54efeaaf2f74e66979b4edeb3ea9d.jpg",
                "paste-a61296bc6a8ec19cb1ae366b232cea69b0219d1a.jpg",
                "paste-bfe9cec0c690cae681ebe980849c35a06ac39344.jpg",
                "paste-c00b6cedb504040d732a84e15e40052ffc90a02d.jpg",
                "paste-c1282b3ab1895e816b5adab6035f6d2dfaacdef0.jpg",
                "paste-c12db5f27104a6fe13e1178df8b8c7e3323b738b.jpg",
                "paste-c4fba61606ed7e0c1600b7a776a9cdf39e6933d4.jpg",
                "paste-daade1416a0d3f24bbcfae6865cfb1b4b3bb67ab.jpg",
                "paste-e4ac9208003fe26bed63d3a55fa8cab7a8f763c5.jpg",
                "paste-e6a2160024acccd2a525e5f28ec89f17096e65db.jpg",
                "paste-f318b87f662182d1d100bdd62c04a4ce6d538308.jpg",
                "paste-fc85dee8434d1b62b0fe5e6427a8ba6a5326cad4.jpg"
            ],
            "name": "14. Dimensionalty Reduction",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why use Dimensionality reduction ?",
                        "To visualize models efficiently"
                    ],
                    "flags": 0,
                    "guid": "z[Sp**1IN#",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Dimensionality reduction",
                        "<div>We’ve seen that 2-d,3-D data can be analysed using scatterplots. 4-D,5-D can be dealt with pair plots(Iris)</div><div>but if we’ve n-D data then we convert them into 2-D or 3-D using dimensionality reduction techniques</div><div>like PCA &amp; t-SNE.</div>"
                    ],
                    "flags": 0,
                    "guid": "9IfKZPu!6",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "[$$]xi \\in \\rm I\\!R^{d}[/$$]",
                        "xi belongs to D dimensional space of real values<div><img src=\"paste-8bfdb715da944efeaa5f94cefeb03bed25275bb5.jpg\"><br></div><div><br></div><div><img src=\"paste-bfe9cec0c690cae681ebe980849c35a06ac39344.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "J+i=`cpTN5",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Column vector ? mathematical formulation and visualisation",
                        "<img src=\"paste-1f7cf1cb5e0be23475491528ee8ac940384a278d.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "u[yX&xW6QW",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Default convention for a vector ?",
                        "column vector. duh!!"
                    ],
                    "flags": 0,
                    "guid": "MEDVQ5)k%O",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Represent a dataset as a Matrix",
                        "<div>1st way - Read it carefully and summarise it. This one feels more natural as it is similar to table/excel</div><div><br></div><div><img src=\"paste-e4ac9208003fe26bed63d3a55fa8cab7a8f763c5.jpg\"><br></div><div>2nd way :-&nbsp; It is mostly used in research papers</div><div><img src=\"Untitled (1).png\"></div>"
                    ],
                    "flags": 0,
                    "guid": "m`Y#>96g:h",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Essence of data preprocessing",
                        "Preprocessing are the various mathematical operations and transformations that we do on the data before we do other complex data transformations. It is problem dependent and done before data modeling. Before data modeling or running ML algorithms we’ve to make the dataset such that the ML algos perform better there."
                    ],
                    "flags": 0,
                    "guid": "Lbg^/FBq@<",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Column Normalisation",
                        "<div><img src=\"paste-238414302580800aa7c6f4518b31560a37a88fc7.jpg\"><br></div><div><br></div><img src=\"paste-559bdeba33bdcb7897599dedce3a53cdd6ed2b79.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "zT<y5)E5zk",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Normalisation formulae",
                        "[$$]ai' = \\frac{ai - amin}{amax - amin}[/$$]"
                    ],
                    "flags": 0,
                    "guid": "e%}:Fv3~$p",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why Column normalisation ?",
                        "By getting each ai lie between 0 and 1, we're getting rid of scale<div><br></div><div><img src=\"paste-35750216b232cbac92c304dd37827518ad221c8a.jpg\"><br></div><div><br></div><div>Here, the data is in cm and kgs but if it’d been lbs in place of kgs it would’ve nearly been doubled so<br>we get rid of that problem by scaling in the same scale i.e [0,1]<br></div>"
                    ],
                    "flags": 0,
                    "guid": "G/>W6E9c$U",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "State the issue with column normalisation",
                        "The catch is that if you are normalizing a query point(new point), if one of the features values were greater/lesser than any training point feature then you'd end up with a value outside the range of 0,1. For example if in the train data you had the maximum value of one of the features to be 100 and minimum to be 2, and if the query point at that particular feature value had a value lesser than 2 or greater than 100 then the normalized value of the feature lies outside the range of [0,1]."
                    ],
                    "flags": 0,
                    "guid": "b.A60-]T{>",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Geometric intuition of normalisation",
                        "We plotted the data points normally. By column normalization, we scale them and squish into a unit<br>square (1*1) so that they lie in the same scale. We are not changing how is the data aligned or positions of data points just scaling it.<br>Same goes for 3-D or n-D data.<div><br></div><div><img src=\"paste-2fc77608806f1c62e6cf8415ed5369aa516692c4.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "m#2Qs~HIgl",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why does the mean come up so often in all the various concepts:<br><br>-bootstrap<br>-sampling distribution<br>-PDF (distributions - normal, power law, log normal, binomial, poisson, exponential)<br>-hypothesis testing<br>-mean of data matrix<div><br></div><div>Can the median not be used instead for things like:<br>-Bootstrap<br>-Hypothesis testing with re sampling<br>-median of data matrix<br></div>",
                        "Mean is the most used statistic because it’s easy to calculate and easy to update when more information comes in: in fact, you can throw away all your original data once you’ve kept the mean(i.e., the sum of all n measured values equals their mean value times n - i.e. if you replace every individual measurement xi by the mean value of all n measurements, you get the same sum), and simply update that mean as each new piece comes in (which you can NOT do with median and mode).<br>On the other hand median has 50% of the sampled values below and 50% of the sample values above it - which makes a very different interpretations.<br>Hence, because of the different interpretations, none can replace the other!<div><br></div><div>Bottom line: Mean is used where the data is symmetrically distributed. If the data has outliers, median is a robust measure.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "Qw^6-Qn&}@",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Mean Vector",
                        "<img src=\"paste-a61296bc6a8ec19cb1ae366b232cea69b0219d1a.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "yDI<LTX7Iy",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Geometric interpretation of Mean Vector",
                        "<img src=\"paste-2796d7f74e5bd7d397c48ef2283f1fbfd8fdcccc.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "hVJC~_+^Af",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Which is more often used, Column standardisation or normalisation and why ?",
                        "Column standardization is used more than column normalization because it can be used for distributions and much more statistical operations."
                    ],
                    "flags": 0,
                    "guid": "QrsZHkbx.Z",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Column standardisation",
                        "<img src=\"paste-6de6f2cd9791aa3a34c150883a40a6366589e003.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "oRfV3W~Iu8",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How to convert array for standardisation",
                        "<img src=\"paste-daade1416a0d3f24bbcfae6865cfb1b4b3bb67ab.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "AQh}6>|8}5",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Geometric intuition of column standardisation",
                        "<img src=\"paste-887d1a12a1c9e371d155766832e970021b1a1b89.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "w$8:Z}?UB|",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Column standardisation formulae",
                        "<img src=\"paste-e6a2160024acccd2a525e5f28ec89f17096e65db.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "AN,ekQS@s(",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Properties of a Co-variance matrix",
                        "1) It is a square matrix<div>2) It is a symmetric matrix</div><div>3) The diagonal elements are variance</div><div>4) If f1&amp;f2 have been standardised<br>&nbsp; &nbsp; cov(f1,f2) = f1T.f2 / n</div><div><br></div><div>Cov matrix is (1/n)XT.X ,if X has been column standardised</div><div><img src=\"paste-3d09561f3ac97f11bf849d9e9e38ef9a89b29a2e.jpg\"><br></div><div><br></div><div><br></div><div><br></div><div><img src=\"paste-c12db5f27104a6fe13e1178df8b8c7e3323b738b.jpg\"><br></div><div><br></div><div><img src=\"paste-c00b6cedb504040d732a84e15e40052ffc90a02d.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "MBm9RN{;53",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why to find covariance matrix ?",
                        "In the&nbsp;<b>covariance matrix</b>&nbsp;in the output, the off-diagonal elements contain the&nbsp;<b>covariances</b>&nbsp;of each pair of variables. The diagonal elements of the&nbsp;<b>covariance matrix</b>&nbsp;contain the variances of each variable. The variance measures how much the data are scattered about the mean."
                    ],
                    "flags": 0,
                    "guid": "I4fjyO`P0*",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Covariance matrix formulae",
                        "<img src=\"paste-3d09561f3ac97f11bf849d9e9e38ef9a89b29a2e.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "Np`=.<y}1#",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "When is covariance matrix used ?",
                        "It is used extensively while doing dimensionality reduction using PCA"
                    ],
                    "flags": 0,
                    "guid": "odu@z=7Ir5",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Covariance of 2 features (f1 &amp; f2)",
                        "<img src=\"paste-c4fba61606ed7e0c1600b7a776a9cdf39e6933d4.jpg\"><div><br></div><div>But as we know in covariance matrix, the columns are standardised.<br>So, means are 0. Now the formulae becomes:-<br><br><img src=\"paste-c1282b3ab1895e816b5adab6035f6d2dfaacdef0.jpg\"><br></div><div><br></div><div>Which is the average dot product of two columns</div><div><br></div><div><img src=\"paste-f318b87f662182d1d100bdd62c04a4ce6d538308.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "zuzp(?&UW_",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "MNIST dataset",
                        "<img src=\"paste-365358c7577b4ecd4a296ed7e2dbf8139fada4eb.jpg\"><div><br></div><div>Every input is represented by a 28*<span>28 matrix<br></span>Now we need to convert this image matrix into a vector by using something called flattening.<span><br></span></div><div><br></div><div>Let’s say we’ve a 5*5 matrix as shown we flatten it by accessing each row and putting them in a single vector as shown .This is called row flattening which makes it 25 * 1.Our dataset has 28*28 therefore it becomes 784*1 vector and it represents the features of a single data point xi.<br><br><img src=\"paste-9c949cd198b54efeaaf2f74e66979b4edeb3ea9d.jpg\"><br></div><div><br></div><div><img src=\"paste-fc85dee8434d1b62b0fe5e6427a8ba6a5326cad4.jpg\"><br></div><div><br></div><div>Source -&nbsp;http://colah.github.io/posts/2014-10-Visualizing-MNIST/</div>"
                    ],
                    "flags": 0,
                    "guid": "h$,sZdfA;|",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Dimensionality_reduction"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "135db9cc-3db5-11ea-abe4-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "latex-35997cf8a9c92e83a7b97381429d7a1dfa7fdec9.png",
                "latex-5244e46ff4a11428f32b5e46760c897708c73e3d.png",
                "latex-66cb8cd89b9bfffa7aae4fe4296402619389d101.png",
                "latex-b607b77a53b1ce22ff4cbf9468209016f2e94a62.png",
                "latex-edc76661f0488f49f8d870627cc5211d84d1f496.png",
                "latex-f06e84fe84a6c63a9c2c392af11652f6e0d72cf4.png",
                "latex-fff36833837fe0eaf9af0efeca0cd1984d83696f.png",
                "paste-190b8095796f6357ff8b8b646462f7fb737787c7.jpg",
                "paste-2427ff53b7872efff417a37ac34faff4f6586482.jpg",
                "paste-27d682d541b2dd575566f501c03a5cbaf79d79c2.jpg",
                "paste-35f180706afc6bb024541ade647ccb66f8fc7196.jpg",
                "paste-4c110f4e2308a2f17febe00a9c3b69016d1bac7c.jpg",
                "paste-7c15e36889ca98453336da78c2d7ff094255fd96.jpg",
                "paste-8402144c4fc4a734222d6aef4a53f3acb74acb93.jpg",
                "paste-974731aa7c22e8206b74034451bad135931601a2.jpg",
                "paste-9f1ed11a137cf539d8e07ccdc75ea529def5d173.jpg",
                "paste-bb6a6327ad41c8fe039c32c3a4ac9e022b3d5178.jpg",
                "paste-c51fa37e1359749b312d8484ca474ca91fb9385c.jpg",
                "paste-d32bdcf25ee025edd2cd83d09159cf07a0e65dd3.jpg",
                "paste-dfe1a9326a02c72230f59cd1d069bbf522604dc3.jpg",
                "paste-f8ac1ff46d101112a4e737e9b3ee26735a82d301.jpg",
                "paste-fdee294d3deff9283dc6ce5fd72a5be2a3379699.jpg",
                "paste-fe64b4d4db1056d03a8256ff65e6777db29d749e.jpg"
            ],
            "name": "15. PCA",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why PCA ?",
                        "It is used for dimensionality reduction. Can be used to get the most important components with the reduced dimensions. d-dim -&gt; d’-dim where d’ &lt; d"
                    ],
                    "flags": 0,
                    "guid": "cxu{u0I$Ml",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "PCA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "PCA full form",
                        "Principal component analysis"
                    ],
                    "flags": 0,
                    "guid": "m@e_}f+._A",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "PCA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Explain PCA Geometrically (Simple 2D dataset)",
                        "<div>Case 1:- 2D dataset</div><div><br></div><img src=\"paste-c51fa37e1359749b312d8484ca474ca91fb9385c.jpg\"><div><br></div><div><br></div><div>Here we’ve two features f1 and f2 and suppose we are supposed to go from 2d to 1d we’ll check the dataset. It’s clear that spread in f2 is clearly far while f1 has very low spread . So we’ll choose the feature with maximum spread because more spread is more information.<br></div><div><br></div><div><img src=\"paste-190b8095796f6357ff8b8b646462f7fb737787c7.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "tk>vx&}=<R",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "PCA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Explain PCA Geometrically (Standardised 2D dataset)",
                        "We took a 2d dataset but column standardized so the variance for both will be same (1).<br>So, we can’t drop any feature .What will we do now?<br><br><img src=\"paste-bb6a6327ad41c8fe039c32c3a4ac9e022b3d5178.jpg\"><br><div><br></div><div><img src=\"paste-fe64b4d4db1056d03a8256ff65e6777db29d749e.jpg\"><br></div><div><br></div><div>We cant drop f1 or f2 but it is visible that in f1’ the spread is much greater than f2’ and f1’ ⊥f2’. Since f1’ has the max spread which means more information our task is to rotate our axis to find f1’ with maximum - variance/spread and drop f2’ to convert data from 2D - &gt; 1D<br></div>"
                    ],
                    "flags": 0,
                    "guid": "KQietSk>A,",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "PCA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Mathematical Objective function of PCA - 1st method",
                        "<img src=\"paste-fdee294d3deff9283dc6ce5fd72a5be2a3379699.jpg\"><div><br></div><div>Let's understand this formulae step by step:-</div><div><br></div><div>First relate this formulae to the formulae for variance.</div><div><br></div><div>Here, we want to maximise the variance of xi', and want&nbsp; to find u1 (The direction of unit vector) that maximises the variance.<br><br>This is the objective of our optimisation problem.<br><br>We also have a constraint, we can't let U1 be any value. We want U1 to be a unit vector</div><div><br></div><div><img src=\"paste-dfe1a9326a02c72230f59cd1d069bbf522604dc3.jpg\"><br></div><div><br></div><div>If we let U1 be anything, we can make it infinity, then that will make the variance maximal</div>"
                    ],
                    "flags": 0,
                    "guid": "N!q!Ah.,su",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "PCA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<div><span>How to derive the mathematical objective formule (optimisation problem) for PCA - 1st method</span><br></div><div><div><img src=\"paste-35f180706afc6bb024541ade647ccb66f8fc7196.jpg\"><br></div></div>",
                        "<div>TLDR -&nbsp;</div><div>1)</div><div>2)</div><div>3)</div><div><br></div><div>This is the first method for PCA - Variance maximisation</div><div><br></div><img src=\"paste-4c110f4e2308a2f17febe00a9c3b69016d1bac7c.jpg\"><div><br></div><div>We're calling f1' here as u1' because that's the term used in most textbooks.<br><br>Second, we don't care about the whole line. But just the direction. Because then we can represent all the points using just the direction.&nbsp;</div><div><br></div><div>We're assuming a point xi and it's projection on direction u1 as xi'</div><div><br></div><div>We create a new dataset D' with [$]xi^{'}[/$]'s&nbsp; in it.</div><div><img src=\"paste-2427ff53b7872efff417a37ac34faff4f6586482.jpg\"><br></div><div><br></div><div>So, by the formulae above given any point xi we can convert it into [$]xi^{'}[/$]&nbsp;using [$]{u_1}^T[/$].</div><div><br></div><div>Now, [$$]\\bar{x}[/$$]&nbsp;is the mean vector of xi's and upon multiplying that with [$]{u<span>_{1}} ^T[/$] we get [$$]\\bar{x}[/$$], which is the mean of&nbsp;</span>[$]xi^{'}[/$]<span>.</span></div><div><br></div><div>Now let's write down our objective in plain english</div><div><br></div><div><img src=\"paste-27d682d541b2dd575566f501c03a5cbaf79d79c2.jpg\"><br></div><div><br></div><div>Here [$]{u_{1}}^{T}xi[/$] and [$]{u_{1}}^{T}<span>\\bar</span><span>{</span><span>x}[/$] are scalars (as they're dot products), so then can be solved as an optimisation problem.</span></div><div><br></div><div>This is the formulae for variance.<br></div><div><br></div><div>[$]\\bar{x}[/$]&nbsp;is column standardised so the mean vector [$]\\bar{x}[/$] is [0,0,0,......0].</div><div>So, the mean vector is 0.</div><div><br></div><div>Now the formulae becomes<br><img src=\"paste-7c15e36889ca98453336da78c2d7ff094255fd96.jpg\"><br></div><div><br></div><div>And our final objective is to maximise this variance</div><div><img src=\"paste-d32bdcf25ee025edd2cd83d09159cf07a0e65dd3.jpg\"><br></div><div><br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "u=WacyTXiz",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "PCA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "PCA - 2nd method",
                        "<img src=\"paste-8402144c4fc4a734222d6aef4a53f3acb74acb93.jpg\"><div><br></div><div>Distance minimisation<br>1) calculate distance of every point of distance between points and unit vector u1</div><div>2) sum them up</div><div>3) optimisation problem to minimise distance</div><div><br></div><div><img src=\"paste-974731aa7c22e8206b74034451bad135931601a2.jpg\"><br></div><div><br></div><div><img src=\"paste-9f1ed11a137cf539d8e07ccdc75ea529def5d173.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "C1Zp?kgi;V",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "PCA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Ways to solve PCA",
                        "1) Variance maximisation<div>2) Distance minimisation</div>"
                    ],
                    "flags": 0,
                    "guid": "qp-@P>t;BG",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "PCA"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Limitations of PCA",
                        "<img src=\"paste-f8ac1ff46d101112a4e737e9b3ee26735a82d301.jpg\"><img src=\"paste-f8ac1ff46d101112a4e737e9b3ee26735a82d301.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "KW?).{pg)^",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "PCA"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "135de0da-3db5-11ea-ad4a-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "latex-5193d09c683f8176f6b7712126774f2811a7c539.png",
                "latex-a167027d886479c4f051385c53ee7d980f4d5d23.png",
                "latex-ba7c5848a59daa61727fc1e73467ad6ae4537dcf.png",
                "latex-ecfb65dd90273aee56bd9eeb5136571debe854e1.png",
                "paste-025066d9f885a06383ce8665f2f649e71a17ed66.jpg",
                "paste-0be5f8424959d0e59cb3adefe1ac7b85c10dc42b.jpg",
                "paste-27c3f612d93052d5228a40d905646f6a862d3cd5.jpg",
                "paste-4593f2faa3888361ca0d608ccd01a0fb3f1bf8fa.jpg",
                "paste-4800ee65876e9bfe0860c86d8a918d6831e8051a.jpg",
                "paste-49dd48ffb748c968e2201039fcc7feec88385044.jpg",
                "paste-4acebe9bebb03da5ad7c98d7a6a6497a5b866d98.jpg",
                "paste-4b16a48512928de8c00958bc612542b9515f18e3.jpg",
                "paste-56e0eaba18dc8d89b858a84b784c52f5df89a420.jpg",
                "paste-6a4e0ea67f44d2aa36d9c2b9d925c9523671b790.jpg",
                "paste-764da25e28747c32ca6e21b486d0be42ddede5ba.jpg",
                "paste-795831c5c0a73d1ad45df03b46c871b81ed08502.jpg",
                "paste-806d5465695f6ffb52fb8e580c6be3eaa06f1110.jpg",
                "paste-9360ae5f28d22bf04c2aa772d0adb87b6222d728.jpg",
                "paste-d19c1e272a45ee743d65d498055bfc4f6edeeca5.jpg",
                "paste-e5c28b965e83b78a3b215d8d3cb9b29809a529e6.jpg",
                "paste-fdaf41120b306672e7c5536812411d4754d2b6e2.jpg"
            ],
            "name": "16. T-SNE",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Properties Eigen vectors ?",
                        "Interesting properties of Eigen values ([$$]\\lambda[/$$]) and Eigen vectors ([$$]v_{i}[/$$]) :-<div><br></div><div>Every pair of Eigen vectors are perpendicular to each other.</div><div><img src=\"paste-806d5465695f6ffb52fb8e580c6be3eaa06f1110.jpg\"><br></div><div><br></div><div>What's so interesting about Eigen vectors and values is that we can proove that:-</div><div><br></div><div><img src=\"paste-fdaf41120b306672e7c5536812411d4754d2b6e2.jpg\"><br></div><div><br></div><div>where S is the covariance square matrix.</div><div>So, we can find the direction of maximum variance using these.</div>"
                    ],
                    "flags": 0,
                    "guid": "e#M){<[V[q",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Steps for PCA",
                        "<img src=\"paste-795831c5c0a73d1ad45df03b46c871b81ed08502.jpg\"><div><br></div><div>S is square covariance matrix</div><div>U1 is the direction of maximum variance</div>"
                    ],
                    "flags": 0,
                    "guid": "nLLl;z9vdZ",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Geometric interpretation of Eigen vectors, values (for PCA)",
                        "No. of dimensions or columns equal to the no. of eigen vectors and values.<div><br></div><div>Given any pair of eigen vectors, they're perpendicular to each other</div><div><br></div><div><img src=\"paste-27c3f612d93052d5228a40d905646f6a862d3cd5.jpg\"><br></div><div>What we're doing is rotating the axis s.t. the direction with most variance will be V1 and direction with second most variance will be v2 and so on.</div><div><br></div><div><br></div><div>Now imagine a scenario with 10 dimensions:-<br></div><div><br></div><div>This was the geometric interpretations of vi's i.e. Eigen Vectors.</div><div><br></div><div>So, Vectors vi's tell you the direction where variance is maximal.</div><div><br></div><div><img src=\"paste-0be5f8424959d0e59cb3adefe1ac7b85c10dc42b.jpg\"><br></div><div><br></div><div>[$$]\\lambda[/$$] explains what % of variance is explained (or information preserved)</div><div><br></div><div><img src=\"paste-9360ae5f28d22bf04c2aa772d0adb87b6222d728.jpg\"><br></div><div><br></div>"
                    ],
                    "flags": 0,
                    "guid": "rM_{yKlKUb",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Definition of Eigen values and vectors",
                        "<img src=\"paste-d19c1e272a45ee743d65d498055bfc4f6edeeca5.jpg\"><div><br></div><div>Given a covariance square matrix,</div><div>If this condition is satisfied then [$$]\\lambda_{1}[/$$] is the eigen value and [$$]v_{1}[/$$] is the eigen vector.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "r<H9blq/bE",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "PCA for dim red and normalisation",
                        "<img src=\"paste-764da25e28747c32ca6e21b486d0be42ddede5ba.jpg\"><div><img src=\"paste-4b16a48512928de8c00958bc612542b9515f18e3.jpg\"><br></div>"
                    ],
                    "flags": 0,
                    "guid": "B:e+/{,#t~",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "How PCA works on MNIST",
                        "<img src=\"paste-e5c28b965e83b78a3b215d8d3cb9b29809a529e6.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "pdoe59Li<L",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Difference between TSNE and PCA",
                        "<img src=\"paste-025066d9f885a06383ce8665f2f649e71a17ed66.jpg\">"
                    ],
                    "flags": 0,
                    "guid": "A&(.q&jknM",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Global / Local&nbsp; &nbsp;Shape / Structure of data",
                        "<img src=\"paste-4593f2faa3888361ca0d608ccd01a0fb3f1bf8fa.jpg\"><div><br></div><div>The two structures in the low variance region are the local structures.<br><br>When using PCA they're merged and the structure is lost.<br><br>But in T-SNE it is preserved.</div><div><br></div><div>P.S In T-SNE the global structure can also be preserved using just a change in parameter</div>"
                    ],
                    "flags": 0,
                    "guid": "IO_+%Co]F%",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "What is t-SNE ?",
                        "t-distributed stochastic neighbourhood embedding"
                    ],
                    "flags": 0,
                    "guid": "EIj117K0uK",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Neighbourhood of a point and Embedding in T-SNE",
                        "Neighbourhood - points geometrically close<div><br></div><div><img src=\"paste-4800ee65876e9bfe0860c86d8a918d6831e8051a.jpg\"><br></div><div><br></div><div>Embedding - For every point in high dimension space, we find a corresponding point in low dimension space. Such a thing is called embedding.</div><div><br></div><div><img src=\"paste-56e0eaba18dc8d89b858a84b784c52f5df89a420.jpg\"><br></div><div><br></div><div>In the context of neural networks, embeddings&nbsp;are&nbsp;<em>low-dimensional,</em>&nbsp;<em>learned</em>&nbsp;continuous vector representations of discrete variables. Neural network embeddings are useful because they can&nbsp;<em>reduce the dimensionality</em>&nbsp;of categorical variables and&nbsp;<em>meaningfully represent</em>categories in the transformed space.<br></div>"
                    ],
                    "flags": 0,
                    "guid": "zqi?[dI~Ae",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Geometric intuition",
                        "<img src=\"paste-6a4e0ea67f44d2aa36d9c2b9d925c9523671b790.jpg\"><div><br></div><div>T-SNE tries to preserve the distances of points in the neighbourhood, but does not guarantees for points outside the neighbourhood</div><div><br></div><div>T-SNE is a Neighbourhood preserving embedding</div>"
                    ],
                    "flags": 0,
                    "guid": "DRP5A~:E_e",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Mathematical formulation of T-SNE",
                        "It's fairly advanced as it is a technique invented in 2008.<br>Optimisation, Gaussian kernels, T-distributed kernels are required which is very intense."
                    ],
                    "flags": 0,
                    "guid": "d^}*wGirRG",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Crowding problem in T-SNE",
                        "<div><img src=\"paste-4acebe9bebb03da5ad7c98d7a6a6497a5b866d98.jpg\"><br></div><div><br></div><div><img src=\"paste-49dd48ffb748c968e2201039fcc7feec88385044.jpg\"><br></div><div><br></div>Sometimes it is impossible to preserve distance in all the neighbourhood .It is called crowding problem which is solved by t-disb<div><br></div><div>Extra -&nbsp;https://www.youtube.com/watch?v=ylx_1bsnv2c</div>"
                    ],
                    "flags": 0,
                    "guid": "AVI@K;iG36",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "T-SNE"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "243deea1-49c0-11ea-b284-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "paste-4c01fe7d9772828f208724c2b55bcd243ac917ae.jpg"
            ],
            "name": "Pending",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Generalisation accuracy",
                        "<div>18.13</div>20:42"
                    ],
                    "flags": 0,
                    "guid": "c]Y}b7<o|t",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Explain signed distance in Logistic regression",
                        ""
                    ],
                    "flags": 0,
                    "guid": "rmP~ZhlKsI",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Logistic_Regression"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Why does L1 reg create sparsity in W as compared to L2 reg ?",
                        ""
                    ],
                    "flags": 0,
                    "guid": "h+>h{,?}c1",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Optimization"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "<p>Content-based filtering for recommendation systems</p>",
                        "<p><img src=\"paste-4c01fe7d9772828f208724c2b55bcd243ac917ae.jpg\"></p><p>Traditionally, two approaches were used to give recommendations: content-based filtering\nand collaborative filtering.&nbsp;</p><p>Content-based filtering consists of learning what users like based on the description of the\ncontent they consume. For example, if the user of a news site often reads news articles on\nscience and technology, then we would suggest to this user more documents on science and\ntechnology. More generally, we could create one training set per user and add news articles\nto this dataset as a feature vector x and whether the user recently read this news article as a\nlabel y. Then we build the model of each user and can regularly examine each new piece of\ncontent to determine whether a specific user would read it or not.&nbsp;</p><p>The content-based approach has many limitations. For example, the user can be trapped in\nthe so-called filter bubble: the system will always suggest to that user the information that\nlooks very similar to what user already consumed. That could result in complete isolation of\nthe user from information that disagrees with their viewpoints or expands them. On a more\npractical side, the users might just stop following recommendations, which is undesirable.<br></p>"
                    ],
                    "flags": 0,
                    "guid": "z2iN6^Lw<[",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Recommender_systems"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Clustering as Matrix Factorization",
                        "https://medium.com/analytics-vidhya/clustering-as-matrix-factorization-65b871cf2ab9<div><br></div><div>https://medium.com/@1993jayant/k-means-clustering-as-a-matrix-factorization-problem-ce1de56c6ec6<br></div>"
                    ],
                    "flags": 0,
                    "guid": "u,9qU<zR]d",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Recommender_systems"
                    ]
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Matrix Factorizaton for Collaborative filtering",
                        "https://medium.com/sfu-big-data/recommendation-systems-collaborative-filtering-using-matrix-factorization-simplified-2118f4ef2cd3"
                    ],
                    "flags": 0,
                    "guid": "lGsJONyy6E",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": []
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "243e15b7-49c0-11ea-b0ff-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [],
            "name": "Extras",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Difference between Factorization machines and Matrix Factorization?<br>",
                        "https://stats.stackexchange.com/questions/108901/difference-between-factorization-machines-and-matrix-factorization"
                    ],
                    "flags": 0,
                    "guid": "k>Ra13Sl(V",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": [
                        "Rcommender",
                        "systems"
                    ]
                }
            ]
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "243e15be-49c0-11ea-9da0-74867a6fa926",
            "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "desc": "",
            "dyn": 0,
            "extendNew": 10,
            "extendRev": 50,
            "media_files": [
                "paste-451c282edacef3b920580f8b318aa11b2404a080.jpg",
                "paste-483d3cee80dc1360d633e00905baf9912f83f3fb.jpg",
                "paste-9fc1fb7192cd520d5ce9062a0e915850f983458f.jpg",
                "paste-a9d7a981a00f98e414ca96dfcdd523a0a0fc4f65.jpg",
                "paste-c22580e8933e11e25fe052dbf2450c333268ecc4.jpg"
            ],
            "name": "34. Feature engineering",
            "notes": [
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Moving Windows",
                        ""
                    ],
                    "flags": 0,
                    "guid": "o*rJ,w$%}X",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Fourier decomposition",
                        "Frequency<br>Amplitude<br>phase<div><br></div><div><img src=\"paste-9fc1fb7192cd520d5ce9062a0e915850f983458f.jpg\"><br></div><div><br></div><div>Understand frequency - it is very important<br>(oscillations per time period)</div><div><br></div><div>phase of a wave</div><div><img src=\"paste-483d3cee80dc1360d633e00905baf9912f83f3fb.jpg\"><br></div><div><br></div><div>compositve wave (repeating wave)</div><div>decomposite it into sum of sin waves</div><div><br></div><div>Converting time waveform(or domain) to frequency domain</div><div><img src=\"paste-a9d7a981a00f98e414ca96dfcdd523a0a0fc4f65.jpg\"><br></div><div><img src=\"paste-c22580e8933e11e25fe052dbf2450c333268ecc4.jpg\"><br></div><div><br></div><div>Frequency waveform is very useful for repeating patterns like heart wave(ECG) or e-commerce sale</div><div><br></div><div><br></div><div>To featurize this time data we create a feature vector with freqency and amplitude. This is called fourier representation of data.</div>"
                    ],
                    "flags": 0,
                    "guid": "Gv&T3&g/]M",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Deep Learning on featurization",
                        "People spent years on signal processing even 30-50 years generating special features and understanding data.<div><br></div><div><img src=\"paste-451c282edacef3b920580f8b318aa11b2404a080.jpg\"><br></div><div><br></div><div>Over last 50 years researchers specialized in one topics like heartbeat data, speech data or even ecommerce data.<br><br>But things changed after 2013 DL</div><div><br></div><div>Automatically learns best featurizations for</div><div>- Time series data</div><div>- Text data</div><div>- Image data</div><div><br></div><div><br></div><div>Only if we have lots of data, DL has best features today for all these</div>"
                    ],
                    "flags": 0,
                    "guid": "je--!GD;Z_",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "data": "",
                    "fields": [
                        "Image histograms",
                        "faces, objects, scans, autonomous cars<div><br></div><div>DL CNN</div><div><br></div><div><br></div><div>Color histograms(RGB), edge-histogram</div>"
                    ],
                    "flags": 0,
                    "guid": "mP_GS0<QZ@",
                    "note_model_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
                    "tags": []
                }
            ]
        }
    ],
    "crowdanki_uuid": "1355ca98-3db5-11ea-92ff-74867a6fa926",
    "deck_config_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
    "deck_configurations": [
        {
            "__type__": "DeckConfig",
            "autoAgain": 0,
            "autoAlert": 0,
            "autoAnswer": 0,
            "autoplay": true,
            "crowdanki_uuid": "1355ca99-3db5-11ea-8cde-74867a6fa926",
            "dyn": false,
            "lapse": {
                "delays": [
                    10
                ],
                "leechAction": 0,
                "leechFails": 8,
                "minInt": 1,
                "mult": 0.0
            },
            "maxTaken": 60,
            "name": "Default",
            "new": {
                "bury": true,
                "delays": [
                    1,
                    10
                ],
                "initialFactor": 2500,
                "ints": [
                    1,
                    4,
                    7
                ],
                "order": 1,
                "perDay": 40,
                "separate": true
            },
            "replayq": true,
            "rev": {
                "bury": true,
                "ease4": 1.3,
                "fuzz": 0.05,
                "hardFactor": 1.2,
                "ivlFct": 1.0,
                "maxIvl": 36500,
                "minSpace": 1,
                "perDay": 100
            },
            "timer": 0
        }
    ],
    "desc": "",
    "dyn": 0,
    "extendNew": 10,
    "extendRev": 50,
    "media_files": [],
    "name": "Machine Learning Anki",
    "note_models": [
        {
            "__type__": "NoteModel",
            "crowdanki_uuid": "135618f6-3db5-11ea-94a4-74867a6fa926",
            "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n",
            "flds": [
                {
                    "font": "Arial",
                    "media": [],
                    "name": "Front",
                    "ord": 0,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                },
                {
                    "font": "Arial",
                    "media": [],
                    "name": "Back",
                    "ord": 1,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                }
            ],
            "latexPost": "\\end{document}",
            "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
            "name": "Basic-575d6",
            "req": [
                [
                    0,
                    "all",
                    [
                        0
                    ]
                ]
            ],
            "sortf": 0,
            "tags": [],
            "tmpls": [
                {
                    "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n{{Back}}",
                    "bafmt": "",
                    "bqfmt": "",
                    "did": null,
                    "name": "Card 1",
                    "ord": 0,
                    "qfmt": "{{Front}}"
                }
            ],
            "type": 0,
            "vers": []
        }
    ],
    "notes": []
}